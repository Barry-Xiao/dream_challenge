{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1124b28",
   "metadata": {},
   "source": [
    "# Preterm Birth Prediction Microbiome Model Framework (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a2a71",
   "metadata": {},
   "source": [
    "Challenge website:\n",
    "https://www.synapse.org/#!Synapse:syn26133770/wiki/618018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabb89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from collections import Counter,defaultdict, OrderedDict\n",
    "from itertools import islice\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dataset_splitID(meta_data, prop, myseed):\n",
    "    \n",
    "    subjects = list(np.unique(meta_data[\"participant_id\"]))\n",
    "    numsubjects = len(subjects)\n",
    "    \n",
    "    if myseed != None:\n",
    "        random.seed(myseed)\n",
    "\n",
    "    subjects_shuffle = random.sample(subjects, numsubjects)\n",
    "    \n",
    "    train_subjects = subjects_shuffle[0:(int(numsubjects*prop[0])+1)] \n",
    "    valid_subjects = subjects_shuffle[(int(numsubjects*prop[0])+2):(int(numsubjects*(prop[0]+prop[1]))+1)]\n",
    "    test_subjects = subjects_shuffle[(int(numsubjects*(prop[0]+prop[1]))+2):numsubjects]\n",
    "    \n",
    "    splitID_train = meta_data['participant_id'].isin(train_subjects)\n",
    "    splitID_valid = meta_data['participant_id'].isin(valid_subjects)\n",
    "    splitID_test = meta_data['participant_id'].isin(test_subjects)\n",
    "    \n",
    "    return splitID_train, splitID_valid, splitID_test\n",
    "\n",
    "\n",
    "# Possible, but not used here\n",
    "def dataset_pjt_splitID(meta_data, prop, myseed):\n",
    "    \n",
    "    projects = meta_data['project']\n",
    "\n",
    "    splitID_train = []\n",
    "    splitID_valid = []\n",
    "    splitID_test  = []\n",
    "    \n",
    "    for pjt in np.unique(projects):\n",
    "        \n",
    "        submeta = meta_data[projects == pjt]\n",
    "        subsubjects = list(np.unique(submeta[\"participant_id\"]))\n",
    "        numsub = len(subsubjects)\n",
    "        \n",
    "        subsubjects_shuffle = random.sample(subsubjects, numsub)\n",
    "        \n",
    "        train_subsubjects = subsubjects_shuffle[0:(int(numsub*prop[0])+1)] \n",
    "        valid_subsubjects = subsubjects_shuffle[(int(numsub*prop[0])+2):(int(numsub*(prop[0]+prop[1]))+1)]\n",
    "        test_subsubjects  = subsubjects_shuffle[(int(numsub*(prop[0]+prop[1]))+2):numsub]\n",
    "        \n",
    "        splitID_train.extend(submeta['participant_id'].isin(train_subsubjects))\n",
    "        splitID_valid.extend(submeta['participant_id'].isin(valid_subsubjects))\n",
    "        splitID_test.extend(submeta['participant_id'].isin(test_subsubjects))\n",
    "        \n",
    "    return splitID_train, splitID_valid, splitID_test\n",
    "\n",
    "\n",
    "def Data_Reshaper_Input(data, seq_length):\n",
    "    \n",
    "    numsubjects = len(np.unique(data['participant_id']))\n",
    "    myvary = list(data.columns.values)[2:data.shape[1]]\n",
    "    num_covariates = len(myvary)\n",
    "    \n",
    "    myinput = np.zeros((numsubjects, seq_length, num_covariates), dtype=np.float32)\n",
    "    for i in range(num_covariates):\n",
    "        data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=myvary[i])\n",
    "        data_wide = data_wide.sort_index(axis=1)\n",
    "        data_wide = data_wide.fillna(0)\n",
    "        tmpindex = data_wide._get_numeric_data().columns.values - 1\n",
    "        tmpindex = tmpindex.astype(int)\n",
    "        # time varying variables need to impute all and no records are denoted as 0\n",
    "        for j in range(numsubjects):\n",
    "                myinput[j,tmpindex,i] = data_wide.iloc[[j]]\n",
    "    return myinput\n",
    "\n",
    "\n",
    "\n",
    "def Data_Reshaper_Output_ManytoMany_0(data, seq_length, classlabel):\n",
    "\n",
    "    num_samples = len(np.unique(data['participant_id']))\n",
    "    \n",
    "    data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=classlabel)\n",
    "    data_wide = data_wide.sort_index(axis=1)\n",
    "    \n",
    "    myoutput = np.zeros((num_samples, seq_length, 2), dtype=np.float32)\n",
    "    for i in range(num_samples):\n",
    "        tmp = data_wide.iloc[i,:]\n",
    "        \n",
    "        if np.nanmax(tmp) == 1:\n",
    "            # label linear smoonthing from 0.5 to 1\n",
    "            # fill all position 1 to have final labels equal to 1\n",
    "            myoutput[i,:,0].fill(1)\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=1, num=seq_length)\n",
    "        else:\n",
    "            # label linear smoonthing from 0.5 to 0\n",
    "            # fill all position 0 to have final labels equal to 0 \n",
    "            #     but array alrady initialize as 0\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=0, num=seq_length)\n",
    "            \n",
    "        myoutput[i,:,1] = 1 - myoutput[i,:,0]\n",
    "    return myoutput\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, device, myinput, myoutput, finalperiod, cutoff=0.5):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # predicted labels\n",
    "    myinput  = torch.from_numpy(myinput).float().to(device)\n",
    "    myoutput_nn, hidden = model(myinput, device)\n",
    "    myoutput_nn = myoutput_nn.reshape((myoutput.shape))\n",
    "    output_prob = nn.functional.softmax(myoutput_nn, dim=2)\n",
    "    mypredprob = output_prob[:,finalperiod-1,:].cpu().detach().numpy()\n",
    "    mypred = 1*(mypredprob[:,0] > cutoff)\n",
    "    # observed labels\n",
    "    myobs  = myoutput[:,finalperiod-1,0]\n",
    "    \n",
    "    return myobs, mypred, mypredprob\n",
    "\n",
    "\n",
    "\n",
    "def metadata_loader(meta_dir, alpha_dir, cst_dir, task, finalperiod):\n",
    "    \n",
    "    meta_data = pd.DataFrame(pd.read_csv(meta_dir, delimiter=','))\n",
    "    meta_data.replace('Unknown', np.nan, inplace=True)\n",
    "    meta_data = meta_data[['participant_id', 'project', 'delivery_wk', 'collect_wk', 'age', 'race']]\n",
    "    \n",
    "    alpha_data = pd.DataFrame(pd.read_csv(alpha_dir, delimiter=','))\n",
    "    cst_data = pd.DataFrame(pd.read_csv(cst_dir, delimiter=','))\n",
    "    \n",
    "    meta_data = pd.concat([meta_data, alpha_data['shannon'], alpha_data['inv_simpson'], alpha_data['rooted_pd'], cst_data['CST']], axis=1)\n",
    "\n",
    "    for i in range(1,meta_data.shape[1]):\n",
    "        if meta_data.iloc[:,i].dtypes == object:\n",
    "            meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('category').cat.codes + 1\n",
    "            meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('float64')\n",
    "            \n",
    "    # create new variable 'collect_period'\n",
    "    meta_data['collect_period'] = 1\n",
    "    meta_data.loc[(meta_data['collect_wk']>=8)  & (meta_data['collect_wk']<=14),'collect_period'] = 2\n",
    "    meta_data.loc[(meta_data['collect_wk']>=15) & (meta_data['collect_wk']<=21),'collect_period'] = 3\n",
    "    meta_data.loc[(meta_data['collect_wk']>=22) & (meta_data['collect_wk']<=28),'collect_period'] = 4\n",
    "    meta_data.loc[(meta_data['collect_wk']>=29) & (meta_data['collect_wk']<=32),'collect_period'] = 5\n",
    "    meta_data.loc[(meta_data['collect_wk']>=33), 'collect_period']                                = 6\n",
    "    \n",
    "    # print(meta_data['collect_period'].value_counts())\n",
    "    \n",
    "    # create task class label\n",
    "    if task == \"was_preterm\":\n",
    "        meta_data[task] = 1*(meta_data['delivery_wk'] < 37)\n",
    "    elif task == \"was_early_preterm\":\n",
    "        meta_data[task] = 1*(meta_data['delivery_wk'] < 32)\n",
    "        \n",
    "    # Filtered out observations with \"collect_wk<=32\" OR \"collect_period<=5\" \n",
    "    # Filtered out observations with \"collect_wk<=28\" OR \"collect_period<=4\" \n",
    "    meta_data = meta_data[meta_data['collect_period']<=finalperiod]\n",
    "    # Average within each collection period\n",
    "    meta_data = meta_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "\n",
    "    return meta_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac44e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputLoader(data_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod):\n",
    "    \n",
    "    participant_id = meta_data['participant_id']\n",
    "    collect_period = meta_data['collect_period']\n",
    "   \n",
    "    Input_data = pd.DataFrame(pd.read_csv(data_dir, delimiter=','))\n",
    "    Input_data = pd.concat([participant_id, collect_period, Input_data], axis=1)\n",
    "    \n",
    "    #---- Filter 1 on columns ----#\n",
    "    # columns/taxons that were observed in fewer than 10 samples\n",
    "    Input_reads = Input_data.iloc[:,3:Input_data.shape[1]]\n",
    "    # pt = np.where((1*(Input_reads != 0)).sum(axis = 0)  > 10)[0]+3\n",
    "    pt = np.where((1*(Input_reads != 0)).sum(axis = 0)  > Input_data.shape[0]*0.01)[0]+3\n",
    "    pt = np.concatenate(([0, 1, 2], pt), axis=None)\n",
    "    Input_data = Input_data.iloc[:,pt]\n",
    "    \n",
    "    #---- Filter 2 on rows    ----#\n",
    "    # Filtered out observations with \"collect_wk<=32\" OR \"collect_period<=6\" \n",
    "    # Filtered out observations with \"collect_wk<=28\" OR \"collect_period<=5\" \n",
    "    Input_data = Input_data[Input_data['collect_period']<=finalperiod]\n",
    "    \n",
    "    # Average within each collection period\n",
    "    Input_data = Input_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "    \n",
    "    Input_data_train = Input_data[trainID]\n",
    "    Input_data_valid = Input_data[validID]\n",
    "    Input_data_test  = Input_data[testID]\n",
    "    \n",
    "    print(\"## Input: train/valid/test (before reshape)\")\n",
    "    print(Input_data_train.shape)\n",
    "    print(Input_data_valid.shape)\n",
    "    print(Input_data_test.shape)\n",
    "    \n",
    "    #---- Input features reshaper ----#\n",
    "    mytrain_input = Data_Reshaper_Input(data=Input_data_train, seq_length=finalperiod)\n",
    "    myvalid_input = Data_Reshaper_Input(data=Input_data_valid, seq_length=finalperiod)\n",
    "    mytest_input  = Data_Reshaper_Input(data=Input_data_test, seq_length=finalperiod)\n",
    "    \n",
    "    print(\"## Input: train/valid/test (after reshape)\")\n",
    "    print(mytrain_input.shape)\n",
    "    print(myvalid_input.shape)\n",
    "    print(mytest_input.shape)\n",
    "    \n",
    "    return mytrain_input, myvalid_input, mytest_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190af89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputLoader(meta_data, trainID, validID, testID, task, finalperiod):\n",
    "    \n",
    "    meta_data_train = meta_data[trainID]\n",
    "    meta_data_valid = meta_data[validID]\n",
    "    meta_data_test  = meta_data[testID]\n",
    "    \n",
    "    print(\"################ Output: train/valid/test (before reshape)\")\n",
    "    print(meta_data_train.shape)\n",
    "    print(meta_data_valid.shape)\n",
    "    print(meta_data_test.shape)\n",
    "    \n",
    "    #---- Output label reshaper ----#\n",
    "    mytrain_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_train, seq_length=finalperiod, classlabel=task)\n",
    "    myvalid_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_valid, seq_length=finalperiod, classlabel=task)\n",
    "    mytest_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_test, seq_length=finalperiod, classlabel=task)\n",
    "    \n",
    "    print(\"################ Output: train/valid/test (after reshape)\")\n",
    "    print(mytrain_output.shape)\n",
    "    print(myvalid_output.shape)\n",
    "    print(mytest_output.shape)\n",
    "    \n",
    "    return mytrain_output, myvalid_output, mytest_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3947df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputLoaderMtd(meta_data, trainID, validID, testID, task, finalperiod):\n",
    "    \n",
    "    meta_data_train = meta_data[trainID]\n",
    "    meta_data_valid = meta_data[validID]\n",
    "    meta_data_test  = meta_data[testID]\n",
    "    \n",
    "    #---- Input features reshaper ----#\n",
    "    mytrain_input_mtd = meta_data_train.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    myvalid_input_mtd = meta_data_valid.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    mytest_input_mtd  = meta_data_test.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    \n",
    "    # scale the input features in this data set\n",
    "    columns = ['collect_wk', 'age', 'race', 'shannon', 'inv_simpson', 'rooted_pd', 'CST']\n",
    "    for col in columns:\n",
    "        mytrain_input_mtd[col] = MinMaxScaler().fit_transform(np.array(mytrain_input_mtd[col]).reshape(-1,1))\n",
    "        myvalid_input_mtd[col] = MinMaxScaler().fit_transform(np.array(myvalid_input_mtd[col]).reshape(-1,1))\n",
    "        mytest_input_mtd[col]  = MinMaxScaler().fit_transform(np.array(mytest_input_mtd[col]).reshape(-1,1))\n",
    "    \n",
    "    print(\"## Input: train/valid/test (before reshape)\")\n",
    "    print(mytrain_input_mtd.shape)\n",
    "    print(myvalid_input_mtd.shape)\n",
    "    print(mytest_input_mtd.shape)\n",
    "    \n",
    "    mytrain_input_mtd = Data_Reshaper_Input(data=mytrain_input_mtd, seq_length=finalperiod)\n",
    "    myvalid_input_mtd = Data_Reshaper_Input(data=myvalid_input_mtd, seq_length=finalperiod)\n",
    "    mytest_input_mtd  = Data_Reshaper_Input(data=mytest_input_mtd,  seq_length=finalperiod) \n",
    "    \n",
    "    print(\"## Input: train/valid/test (after reshape)\")\n",
    "    print(mytrain_input_mtd.shape)\n",
    "    print(myvalid_input_mtd.shape)\n",
    "    print(mytest_input_mtd.shape)\n",
    "    \n",
    "    return mytrain_input_mtd, myvalid_input_mtd, mytest_input_mtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c901b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMtrain(model, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, finalperiod, patience, earlystop='loss', verbose=True):\n",
    "    \n",
    "    # training and validation set class proportion\n",
    "    trainprior = sum(mytrain_output[:,finalperiod-1,0])/mytrain_output.shape[0]\n",
    "    class1ID_train = mytrain_output[:,finalperiod-1,0] == 1\n",
    "    class2ID_train = mytrain_output[:,finalperiod-1,0] == 0\n",
    "    \n",
    "    validprior = sum(myvalid_output[:,finalperiod-1,0])/myvalid_output.shape[0]\n",
    "    class1ID_valid = myvalid_output[:,finalperiod-1,0] == 1\n",
    "    class2ID_valid = myvalid_output[:,finalperiod-1,0] == 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Track the value of the loss function and model accuracy across epochs\n",
    "    history_train_valid = {'TrainLoss': [], 'TrainAcc': [], 'TrainAUC': [],\n",
    "                           'ValidLoss': [], 'ValidAcc': [], 'ValidAUC': []}\n",
    "    \n",
    "    # Same reshaped Validation set for each epoch    \n",
    "    myvalid_input  = torch.from_numpy(myvalid_input).float().to(device)\n",
    "    myvalid_output = torch.from_numpy(myvalid_output).float().to(device)\n",
    "        \n",
    "    valid_loss_min = np.inf\n",
    "    valid_losses = []\n",
    "    \n",
    "    valid_auc_max = np.NINF\n",
    "    valid_auces = []\n",
    "    \n",
    "    last_valid_loss = 100\n",
    "    last_valid_auc  = 100\n",
    "    \n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        #----  shuffle the training set to avoid the batch(project) effects ----#\n",
    "        shuffleindex = list(range(mytrain_output.shape[0]))\n",
    "        random.shuffle(shuffleindex)\n",
    "        mytrain_output = mytrain_output[shuffleindex]\n",
    "        mytrain_input = mytrain_input[shuffleindex]\n",
    "        \n",
    "        #-------------- Batch-wise training model --------------#\n",
    "        model.train()\n",
    "        # train_loss = 0.0\n",
    "        train_num_correct = 0\n",
    "        train_prob = []\n",
    "        for batch_idx in range(0, mytrain_input.shape[0], batch_size):\n",
    "            \n",
    "            # subset a batch of sequences and class labels\n",
    "            tmpindex = list(range(batch_idx, min(batch_idx+batch_size, mytrain_input.shape[0])))\n",
    "            mytrain_input_batch  = mytrain_input[tmpindex,:]\n",
    "            mytrain_output_batch = mytrain_output[tmpindex,:]\n",
    "            \n",
    "            batchprior = sum(mytrain_output_batch[:,finalperiod-1,0])/mytrain_output_batch.shape[0]\n",
    "            class1ID_batch = mytrain_output_batch[:,finalperiod-1,0] == 1\n",
    "            class2ID_batch = mytrain_output_batch[:,finalperiod-1,0] == 0\n",
    "            \n",
    "            mytrain_input_batch  = torch.from_numpy(mytrain_input_batch).float().to(device)\n",
    "            mytrain_output_batch = torch.from_numpy(mytrain_output_batch).float().to(device)\n",
    "            \n",
    "            # forward pass of RNN model\n",
    "            output, hidden = model(mytrain_input_batch, device)\n",
    "            output = output.reshape((mytrain_output_batch.shape))\n",
    "            output_prob = nn.functional.softmax(output, dim=2)\n",
    "            # weighted MSE\n",
    "            loss = batchprior*criterion(output_prob[class1ID_batch,:,0], mytrain_output_batch[class1ID_batch,:,0]) + (1-batchprior)*criterion(output_prob[class2ID_batch,:,1], mytrain_output_batch[class2ID_batch,:,1])\n",
    "            # loss = trainprior*criterion(output_prob[class1ID_batch,:,0], mytrain_output_batch[class1ID_batch,:,0]) + (1-trainprior)*criterion(output_prob[class2ID_batch,:,1], mytrain_output_batch[class2ID_batch,:,1])\n",
    "            # loss = criterion(output_prob, mytrain_output_batch)\n",
    "            # Clear existing gradients from previous epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Does backpropagation and calculates gradients\n",
    "            loss.backward()\n",
    "            # Updates the weights accordingly\n",
    "            optimizer.step()\n",
    "            # Number correct prediction on trainning set collection\n",
    "            tmppred = 1*(output_prob[:,finalperiod-1,0] > 0.5)\n",
    "            train_num_correct += sum(1*(tmppred == mytrain_output_batch[:,finalperiod-1,0]))\n",
    "            # Training function loss collection\n",
    "            # train_loss += loss.item()\n",
    "            train_prob = np.concatenate((train_prob, output_prob[:,finalperiod-1,0].cpu().detach().numpy()), axis=None)\n",
    "            \n",
    "        train_acc = (float(train_num_correct) / len(mytrain_output))*100\n",
    "        train_auc = metrics.roc_auc_score(mytrain_output[:,finalperiod-1,0], train_prob)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Training loss calculation\n",
    "        tmpmytrain_input  = torch.from_numpy(mytrain_input).float().to(device)\n",
    "        tmpmytrain_output = torch.from_numpy(mytrain_output).float().to(device)\n",
    "        tmpoutputtrain, tmphidden = model(tmpmytrain_input, device)\n",
    "        tmpoutputtrain = tmpoutputtrain.reshape((tmpmytrain_output.shape))\n",
    "        tmpoutputtrain_prob = nn.functional.softmax(tmpoutputtrain, dim=2)\n",
    "        # train_loss = criterion(tmpoutputtrain_prob, tmpmytrain_output)\n",
    "        train_loss = trainprior*criterion(tmpoutputtrain_prob[class1ID_train,:,0], tmpmytrain_output[class1ID_train,:,0]) + (1-trainprior)*criterion(tmpoutputtrain_prob[class2ID_train,:,1], tmpmytrain_output[class2ID_train,:,1])\n",
    "        history_train_valid['TrainLoss'].append(train_loss.item())\n",
    "        history_train_valid['TrainAcc'].append(train_acc)\n",
    "        history_train_valid['TrainAUC'].append(train_auc)\n",
    "        \n",
    "\n",
    "        #--------------       Validate model      --------------#\n",
    "        outputvalid, hidden = model(myvalid_input, device)\n",
    "        outputvalid = outputvalid.reshape((myvalid_output.shape))\n",
    "        outputvalid_prob = nn.functional.softmax(outputvalid, dim=2)\n",
    "        # validation loss\n",
    "        # valid_loss = criterion(outputvalid_prob, myvalid_output)\n",
    "        valid_loss = validprior*criterion(outputvalid_prob[class1ID_valid,:,0], myvalid_output[class1ID_valid,:,0]) + (1-validprior)*criterion(outputvalid_prob[class2ID_valid,:,1], myvalid_output[class2ID_valid,:,1])\n",
    "        # Number correct prediction on trainning set collection\n",
    "        tmppredprob = outputvalid_prob[:,finalperiod-1,0].cpu().detach().numpy()\n",
    "        tmppred = 1*(tmppredprob > 0.5)\n",
    "        tmpobs = myvalid_output[:,finalperiod-1,0].cpu().detach().numpy()\n",
    "        valid_num_correct = sum(1*(tmppred == tmpobs))\n",
    "        valid_acc = (float(valid_num_correct) / len(myvalid_output))*100\n",
    "        valid_auc = metrics.roc_auc_score(tmpobs, tmppredprob)\n",
    "        \n",
    "        history_train_valid['ValidLoss'].append(valid_loss.item())\n",
    "        history_train_valid['ValidAcc'].append(valid_acc)\n",
    "        history_train_valid['ValidAUC'].append(valid_auc)\n",
    "        \n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[E {epoch + 1}/{max_epochs}]'\n",
    "                  f\" T.Loss: {history_train_valid['TrainLoss'][-1]:.4f}, T.Acc: {history_train_valid['TrainAcc'][-1]:2.2f}, T.AUC: {history_train_valid['TrainAUC'][-1]:.4f}\"\n",
    "                  f\" V.Loss: {history_train_valid['ValidLoss'][-1]:.4f}, V.Acc: {history_train_valid['ValidAcc'][-1]:2.2f}, V.AUC: {history_train_valid['ValidAUC'][-1]:.4f};\")\n",
    "        \n",
    "        valid_auces.append(valid_auc.item())\n",
    "        valid_losses.append(valid_loss.item())\n",
    "        \n",
    "        if earlystop == \"auc\":\n",
    "            current_valid_auc = valid_auc\n",
    "            if current_valid_auc < last_valid_auc:\n",
    "                trigger_times += 1\n",
    "                print('AUC Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping by AUC!.')\n",
    "                    break\n",
    "            else:\n",
    "                print('trigger times: 0')\n",
    "                trigger_times = 0\n",
    "            last_valid_auc = np.mean(valid_auces[-10:])\n",
    "            # last_valid_auc = current_valid_auc\n",
    "        elif earlystop == \"loss\":\n",
    "            current_valid_loss = valid_loss\n",
    "            if current_valid_loss > last_valid_loss:\n",
    "                trigger_times += 1\n",
    "                print('Loss Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping by LOSS!.')\n",
    "                    break\n",
    "            else:\n",
    "                print('Trigger times >= patience: 0')\n",
    "                trigger_times = 0\n",
    "            last_valid_loss = np.mean(valid_losses[-10:])\n",
    "            # last_valid_loss = current_valid_loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if earlystop == \"auc\":\n",
    "        #     # start to considering early-stop after 20 epoch\n",
    "        #     if epoch > 20:\n",
    "        #        if np.mean(valid_auces) < valid_auc_max:\n",
    "        #            print(\"Stopped here by AUC!\")\n",
    "        #            break\n",
    "        #        valid_auc_max = np.mean(valid_auces)\n",
    "        # elif earlystop == \"loss\":\n",
    "        #    # start to considering early-stop after 20 epoch\n",
    "        #    if epoch > 20:\n",
    "        #        if np.mean(valid_losses) > valid_loss_min:\n",
    "        #            print(\"Stopped here by LOSS!\")\n",
    "        #            break\n",
    "        #        # valid_loss_min = np.mean(valid_losses[-20:])\n",
    "        #        valid_loss_min = np.mean(valid_losses)\n",
    "        \n",
    "    return history_train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53778d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Mtd(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_Mtd, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=output_size, bias=False)\n",
    "\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "            \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "\n",
    "        outp = self.fc_1(outp)\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daed6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_Mtd(mytrain_input_mtd, mytrain_output, myvalid_input_mtd, myvalid_output, mytest_input_mtd, mytest_output, finalperiod):\n",
    "    \n",
    "    # 7 -> lstm -> 16 -> 8\n",
    "    \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_mtd.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 8\n",
    "    fc_size     = [16]\n",
    "    n_layers    = 1\n",
    "    \n",
    "    dropoutrate = 0.5\n",
    "    lr          = 0.001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 50\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_Mtd = Model_Mtd(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, \n",
    "                          n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_Mtd.parameters(), lr=lr) \n",
    "    \n",
    "    print(\"################ Mtd LSTM training...\")\n",
    "    Mtd_hist = LSTMtrain(model_Mtd, device, criterion, optimizer, mytrain_input_mtd, mytrain_output, \n",
    "                         myvalid_input_mtd, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    Mtd_obs, Mtd_pred, Mtd_prob = evaluate(model_Mtd, device, mytest_input_mtd, mytest_output, finalperiod, cutoff=0.5)\n",
    "    Mtdtest_auc = metrics.roc_auc_score(Mtd_obs, Mtd_prob[:,0])\n",
    "    Mtdtest_acc = metrics.accuracy_score(Mtd_obs, Mtd_pred)\n",
    "    Mtdtest_conf = metrics.confusion_matrix(Mtd_obs, Mtd_pred)\n",
    "\n",
    "    return model_Mtd, Mtd_hist, Mtd_obs, Mtd_pred, Mtd_prob, Mtdtest_auc, Mtdtest_acc, Mtdtest_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5816b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_pty(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_pty, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=output_size, bias=False)\n",
    "        # self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        \n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910645cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_pty(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod):\n",
    "   \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_pty.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 128\n",
    "    n_layers    = 1\n",
    "    fc_size     = [128, 64]\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.0001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 200\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_pty = Model_pty(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, \n",
    "                          n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_pty.parameters(), lr=lr) \n",
    "    \n",
    "    #---- training lstm ----#\n",
    "    print(\"################ pty LSTM training...\")\n",
    "    pty_hist = LSTMtrain(model_pty, device, criterion, optimizer, mytrain_input_pty, mytrain_output, \n",
    "                         myvalid_input_pty, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    pty_obs, pty_pred, pty_prob = evaluate(model_pty, device, mytest_input_pty, mytest_output, finalperiod, cutoff=0.5)\n",
    "    ptytest_auc = metrics.roc_auc_score(pty_obs, pty_prob[:,0])\n",
    "    ptytest_acc = metrics.accuracy_score(pty_obs, pty_pred)\n",
    "    ptytest_conf = metrics.confusion_matrix(pty_obs, pty_pred)\n",
    "\n",
    "    return model_pty, pty_hist, pty_obs, pty_pred, pty_prob, ptytest_auc, ptytest_acc, ptytest_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73926e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_txy(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_txy, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=fc_size[2], bias=False)\n",
    "        self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_4(outp)   # 4th Ouuput\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "426a2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_txy(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod):\n",
    "    \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_txy.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 256\n",
    "    n_layers    = 1\n",
    "    fc_size     = [256, 128, 64]\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.0001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 200\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_txy = Model_txy(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, \n",
    "                          seq_len=seq_len, n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_txy.parameters(), lr=lr) \n",
    "    \n",
    "    print(\"################ txy LSTM training...\")\n",
    "    txy_hist = LSTMtrain(model_txy, device, criterion, optimizer, mytrain_input_txy, mytrain_output, \n",
    "                         myvalid_input_txy, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    txy_obs, txy_pred, txy_prob = evaluate(model_txy, device, mytest_input_txy, mytest_output, finalperiod, cutoff=0.5)\n",
    "    txytest_auc = metrics.roc_auc_score(txy_obs, txy_prob[:,0])\n",
    "    txytest_acc = metrics.accuracy_score(txy_obs, txy_pred)\n",
    "    txytest_conf = metrics.confusion_matrix(txy_obs, txy_pred)\n",
    "\n",
    "    return model_txy, txy_hist, txy_obs, txy_pred, txy_prob, txytest_auc, txytest_acc, txytest_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0661a",
   "metadata": {},
   "source": [
    "# Main script start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15b65d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Output: train/valid/test (before reshape)\n",
      "(998, 12)\n",
      "(488, 12)\n",
      "(162, 12)\n",
      "################ Output: train/valid/test (after reshape)\n",
      "(729, 5, 2)\n",
      "(363, 5, 2)\n",
      "(120, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "# data directory\n",
    "# meta_dir      = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/metadata/metadata.csv'\n",
    "meta_dir      = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/metadata_imputed1.csv'\n",
    "alpha_dir     = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/alpha_diversity/alpha_diversity.csv'\n",
    "cst_dir       = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/community_state_types/cst_valencia.csv'\n",
    "\n",
    "txy_dir_fam = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.family.csv'\n",
    "txy_dir_gen = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.genus.csv'\n",
    "txy_dir_spe = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.species.csv'\n",
    "\n",
    "pty_dir_1dot = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.1e0.csv'\n",
    "pty_dir_dot5 = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.5e_1.csv'\n",
    "pty_dir_dot1 = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.1e_1.csv'\n",
    "\n",
    "# krdwide_dir   = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/pairwise_distance/krd_distance_wide.csv'\n",
    "\n",
    "\n",
    "txy_dir = txy_dir_gen\n",
    "pty_dir = pty_dir_1dot\n",
    "\n",
    "task = \"was_preterm\"\n",
    "finalperiod = 5\n",
    "# task = \"was_early_preterm\"\n",
    "# finalperiod = 4\n",
    "\n",
    "myprop = [0.6, 0.3, 0.1]\n",
    "myseed = 0\n",
    "\n",
    "\n",
    "#-------------------------------------------#\n",
    "#---- Data Preparation                  ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "meta_data = metadata_loader(meta_dir, alpha_dir, cst_dir, task, finalperiod)\n",
    "\n",
    "#---- data set splitter ----#\n",
    "trainID, validID, testID = dataset_splitID(meta_data=meta_data, prop=myprop, myseed=myseed)\n",
    "\n",
    "#---- output loader ----#\n",
    "mytrain_output, myvalid_output, mytest_output = OutputLoader(meta_data, trainID, validID, testID, task, finalperiod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17876d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ meta:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(998, 9)\n",
      "(488, 9)\n",
      "(162, 9)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(729, 5, 7)\n",
      "(363, 5, 7)\n",
      "(120, 5, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ meta:\")\n",
    "mytrain_input_mtd, myvalid_input_mtd, mytest_input_mtd = InputLoaderMtd(meta_data, trainID, validID, testID, task, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803ac03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ pty:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(998, 165)\n",
      "(488, 165)\n",
      "(162, 165)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(729, 5, 163)\n",
      "(363, 5, 163)\n",
      "(120, 5, 163)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ pty:\")\n",
    "mytrain_input_pty, myvalid_input_pty, mytest_input_pty = InputLoader(pty_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "815b14aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ txy:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(998, 250)\n",
      "(488, 250)\n",
      "(162, 250)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(729, 5, 248)\n",
      "(363, 5, 248)\n",
      "(120, 5, 248)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ txy:\")\n",
    "mytrain_input_txy, myvalid_input_txy, mytest_input_txy = InputLoader(txy_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06738bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Mtd LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0852, T.Acc: 66.39, T.AUC: 0.5498 V.Loss: 0.0869, V.Acc: 65.84, V.AUC: 0.6538;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0811, T.Acc: 69.14, T.AUC: 0.5085 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6588;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0798, T.Acc: 69.00, T.AUC: 0.4840 V.Loss: 0.0836, V.Acc: 65.84, V.AUC: 0.6693;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0794, T.Acc: 69.14, T.AUC: 0.5274 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6755;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0793, T.Acc: 69.14, T.AUC: 0.5439 V.Loss: 0.0832, V.Acc: 65.84, V.AUC: 0.6787;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.5413 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6797;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.5445 V.Loss: 0.0826, V.Acc: 65.84, V.AUC: 0.6791;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.5546 V.Loss: 0.0823, V.Acc: 65.84, V.AUC: 0.6783;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0784, T.Acc: 69.27, T.AUC: 0.5261 V.Loss: 0.0819, V.Acc: 65.84, V.AUC: 0.6823;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.5575 V.Loss: 0.0816, V.Acc: 65.84, V.AUC: 0.6841;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0780, T.Acc: 69.14, T.AUC: 0.5379 V.Loss: 0.0811, V.Acc: 65.84, V.AUC: 0.6839;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0777, T.Acc: 69.27, T.AUC: 0.5731 V.Loss: 0.0810, V.Acc: 65.84, V.AUC: 0.6852;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0775, T.Acc: 69.27, T.AUC: 0.5782 V.Loss: 0.0806, V.Acc: 65.84, V.AUC: 0.6860;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0773, T.Acc: 69.14, T.AUC: 0.5801 V.Loss: 0.0801, V.Acc: 65.84, V.AUC: 0.6859;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0770, T.Acc: 69.00, T.AUC: 0.5828 V.Loss: 0.0799, V.Acc: 65.84, V.AUC: 0.6863;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0769, T.Acc: 69.27, T.AUC: 0.5831 V.Loss: 0.0794, V.Acc: 65.84, V.AUC: 0.6868;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0766, T.Acc: 69.00, T.AUC: 0.6044 V.Loss: 0.0790, V.Acc: 65.84, V.AUC: 0.6888;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0765, T.Acc: 69.00, T.AUC: 0.5933 V.Loss: 0.0788, V.Acc: 65.84, V.AUC: 0.6877;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0761, T.Acc: 69.14, T.AUC: 0.6121 V.Loss: 0.0786, V.Acc: 65.84, V.AUC: 0.6880;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0759, T.Acc: 69.00, T.AUC: 0.6203 V.Loss: 0.0789, V.Acc: 65.84, V.AUC: 0.6899;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0758, T.Acc: 69.14, T.AUC: 0.6125 V.Loss: 0.0779, V.Acc: 66.94, V.AUC: 0.6898;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0755, T.Acc: 69.68, T.AUC: 0.6286 V.Loss: 0.0774, V.Acc: 67.49, V.AUC: 0.6909;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0752, T.Acc: 69.68, T.AUC: 0.6292 V.Loss: 0.0771, V.Acc: 68.32, V.AUC: 0.6914;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0749, T.Acc: 69.82, T.AUC: 0.6311 V.Loss: 0.0769, V.Acc: 68.04, V.AUC: 0.6922;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0747, T.Acc: 69.82, T.AUC: 0.6172 V.Loss: 0.0766, V.Acc: 68.04, V.AUC: 0.6935;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0745, T.Acc: 70.92, T.AUC: 0.6534 V.Loss: 0.0768, V.Acc: 68.04, V.AUC: 0.6905;\n",
      "Trigger times >= patience: 0\n",
      "[E 27/2000] T.Loss: 0.0742, T.Acc: 70.78, T.AUC: 0.6443 V.Loss: 0.0762, V.Acc: 69.70, V.AUC: 0.6889;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0743, T.Acc: 70.64, T.AUC: 0.6281 V.Loss: 0.0757, V.Acc: 69.15, V.AUC: 0.6894;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0738, T.Acc: 70.51, T.AUC: 0.6472 V.Loss: 0.0757, V.Acc: 69.70, V.AUC: 0.6901;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0736, T.Acc: 70.37, T.AUC: 0.6475 V.Loss: 0.0754, V.Acc: 69.15, V.AUC: 0.6912;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0734, T.Acc: 70.37, T.AUC: 0.6411 V.Loss: 0.0755, V.Acc: 69.70, V.AUC: 0.6897;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0732, T.Acc: 71.19, T.AUC: 0.6405 V.Loss: 0.0752, V.Acc: 69.70, V.AUC: 0.6904;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0730, T.Acc: 70.92, T.AUC: 0.6577 V.Loss: 0.0751, V.Acc: 69.97, V.AUC: 0.6902;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0728, T.Acc: 71.88, T.AUC: 0.6495 V.Loss: 0.0749, V.Acc: 70.25, V.AUC: 0.6906;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0726, T.Acc: 70.64, T.AUC: 0.6529 V.Loss: 0.0748, V.Acc: 70.25, V.AUC: 0.6925;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0725, T.Acc: 71.19, T.AUC: 0.6492 V.Loss: 0.0748, V.Acc: 70.25, V.AUC: 0.6922;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0724, T.Acc: 71.47, T.AUC: 0.6759 V.Loss: 0.0748, V.Acc: 70.25, V.AUC: 0.6930;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0722, T.Acc: 72.29, T.AUC: 0.6504 V.Loss: 0.0746, V.Acc: 70.52, V.AUC: 0.6926;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0721, T.Acc: 71.47, T.AUC: 0.6633 V.Loss: 0.0746, V.Acc: 70.52, V.AUC: 0.6914;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0719, T.Acc: 72.15, T.AUC: 0.6653 V.Loss: 0.0746, V.Acc: 70.52, V.AUC: 0.6915;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0718, T.Acc: 72.15, T.AUC: 0.6764 V.Loss: 0.0747, V.Acc: 70.80, V.AUC: 0.6923;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0717, T.Acc: 72.29, T.AUC: 0.6496 V.Loss: 0.0743, V.Acc: 71.07, V.AUC: 0.6918;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0715, T.Acc: 72.15, T.AUC: 0.6580 V.Loss: 0.0746, V.Acc: 70.80, V.AUC: 0.6919;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0715, T.Acc: 72.02, T.AUC: 0.6634 V.Loss: 0.0743, V.Acc: 71.07, V.AUC: 0.6923;\n",
      "Trigger times >= patience: 0\n",
      "[E 45/2000] T.Loss: 0.0714, T.Acc: 72.98, T.AUC: 0.6626 V.Loss: 0.0743, V.Acc: 71.07, V.AUC: 0.6920;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0712, T.Acc: 72.02, T.AUC: 0.6805 V.Loss: 0.0744, V.Acc: 71.07, V.AUC: 0.6923;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0711, T.Acc: 72.43, T.AUC: 0.6825 V.Loss: 0.0743, V.Acc: 71.35, V.AUC: 0.6933;\n",
      "Trigger times >= patience: 0\n",
      "[E 48/2000] T.Loss: 0.0710, T.Acc: 72.98, T.AUC: 0.6688 V.Loss: 0.0744, V.Acc: 71.35, V.AUC: 0.6933;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0710, T.Acc: 72.98, T.AUC: 0.6567 V.Loss: 0.0746, V.Acc: 71.07, V.AUC: 0.6910;\n",
      "Loss Trigger Times: 1\n",
      "[E 50/2000] T.Loss: 0.0709, T.Acc: 73.25, T.AUC: 0.6730 V.Loss: 0.0746, V.Acc: 71.35, V.AUC: 0.6910;\n",
      "Loss Trigger Times: 2\n",
      "[E 51/2000] T.Loss: 0.0708, T.Acc: 72.29, T.AUC: 0.6641 V.Loss: 0.0748, V.Acc: 71.07, V.AUC: 0.6898;\n",
      "Loss Trigger Times: 3\n",
      "[E 52/2000] T.Loss: 0.0707, T.Acc: 73.25, T.AUC: 0.6639 V.Loss: 0.0745, V.Acc: 71.63, V.AUC: 0.6900;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.7583333333333333\n",
      "0.7733868477925285\n",
      "[[75  4]\n",
      " [25 16]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: Metadata             ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_Mtd, Mtd_hist, Mtdtest_obs, Mtdtest_pred, Mtdtest_prob, Mtdtest_auc, Mtdtest_acc, Mtdtest_conf = FirstStage_Mtd(mytrain_input_mtd, mytrain_output, myvalid_input_mtd, myvalid_output, mytest_input_mtd, mytest_output, finalperiod)\n",
    "print(Mtdtest_acc)\n",
    "print(Mtdtest_auc)\n",
    "print(Mtdtest_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d9b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ pty LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0930, T.Acc: 62.41, T.AUC: 0.4917 V.Loss: 0.0932, V.Acc: 65.84, V.AUC: 0.4968;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0918, T.Acc: 69.27, T.AUC: 0.5252 V.Loss: 0.0921, V.Acc: 65.84, V.AUC: 0.5158;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0905, T.Acc: 69.27, T.AUC: 0.5229 V.Loss: 0.0911, V.Acc: 65.84, V.AUC: 0.5263;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0892, T.Acc: 69.27, T.AUC: 0.5278 V.Loss: 0.0901, V.Acc: 65.84, V.AUC: 0.5295;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0878, T.Acc: 69.27, T.AUC: 0.5271 V.Loss: 0.0890, V.Acc: 65.84, V.AUC: 0.5379;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0863, T.Acc: 69.27, T.AUC: 0.5155 V.Loss: 0.0879, V.Acc: 65.84, V.AUC: 0.5449;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0848, T.Acc: 69.27, T.AUC: 0.4631 V.Loss: 0.0868, V.Acc: 65.84, V.AUC: 0.5507;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0834, T.Acc: 69.27, T.AUC: 0.5273 V.Loss: 0.0858, V.Acc: 65.84, V.AUC: 0.5552;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0820, T.Acc: 69.27, T.AUC: 0.4813 V.Loss: 0.0850, V.Acc: 65.84, V.AUC: 0.5607;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0810, T.Acc: 69.27, T.AUC: 0.5103 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.5685;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0802, T.Acc: 69.27, T.AUC: 0.5558 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.5757;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0798, T.Acc: 69.27, T.AUC: 0.5365 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.5827;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5175 V.Loss: 0.0847, V.Acc: 65.84, V.AUC: 0.5947;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5215 V.Loss: 0.0849, V.Acc: 65.84, V.AUC: 0.6056;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5464 V.Loss: 0.0851, V.Acc: 65.84, V.AUC: 0.6192;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5569 V.Loss: 0.0850, V.Acc: 65.84, V.AUC: 0.6248;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5422 V.Loss: 0.0849, V.Acc: 65.84, V.AUC: 0.6332;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.4867 V.Loss: 0.0848, V.Acc: 65.84, V.AUC: 0.6453;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5242 V.Loss: 0.0847, V.Acc: 65.84, V.AUC: 0.6478;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5241 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.6528;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5337 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6571;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5772 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6601;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.4906 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6613;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5444 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6636;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5596 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6665;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5414 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6702;\n",
      "Trigger times >= patience: 0\n",
      "[E 27/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5449 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6705;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5356 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6712;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5202 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6724;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5223 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6748;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5635 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6765;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5622 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6768;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5767 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6766;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5703 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6767;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.5806 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6771;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.5915 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.5731 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.5756 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6790;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.5765 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6784;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.5640 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.5612 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6777;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.6072 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6783;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.5930 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6788;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.6083 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6795;\n",
      "Trigger times >= patience: 0\n",
      "[E 45/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.5855 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6798;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.6002 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6800;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0787, T.Acc: 69.27, T.AUC: 0.6375 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6801;\n",
      "Trigger times >= patience: 0\n",
      "[E 48/2000] T.Loss: 0.0787, T.Acc: 69.27, T.AUC: 0.6210 V.Loss: 0.0838, V.Acc: 65.84, V.AUC: 0.6804;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.6025 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6801;\n",
      "Trigger times >= patience: 0\n",
      "[E 50/2000] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.5893 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6799;\n",
      "Trigger times >= patience: 0\n",
      "[E 51/2000] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.6268 V.Loss: 0.0836, V.Acc: 65.84, V.AUC: 0.6795;\n",
      "Trigger times >= patience: 0\n",
      "[E 52/2000] T.Loss: 0.0785, T.Acc: 69.27, T.AUC: 0.6102 V.Loss: 0.0835, V.Acc: 65.84, V.AUC: 0.6784;\n",
      "Trigger times >= patience: 0\n",
      "[E 53/2000] T.Loss: 0.0785, T.Acc: 69.27, T.AUC: 0.6463 V.Loss: 0.0835, V.Acc: 65.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 54/2000] T.Loss: 0.0784, T.Acc: 69.27, T.AUC: 0.6302 V.Loss: 0.0835, V.Acc: 65.84, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 55/2000] T.Loss: 0.0784, T.Acc: 69.27, T.AUC: 0.6107 V.Loss: 0.0836, V.Acc: 65.84, V.AUC: 0.6784;\n",
      "Trigger times >= patience: 0\n",
      "[E 56/2000] T.Loss: 0.0783, T.Acc: 69.27, T.AUC: 0.6301 V.Loss: 0.0836, V.Acc: 65.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 57/2000] T.Loss: 0.0783, T.Acc: 69.27, T.AUC: 0.6422 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6777;\n",
      "Trigger times >= patience: 0\n",
      "[E 58/2000] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.6378 V.Loss: 0.0833, V.Acc: 65.84, V.AUC: 0.6774;\n",
      "Trigger times >= patience: 0\n",
      "[E 59/2000] T.Loss: 0.0781, T.Acc: 69.27, T.AUC: 0.6431 V.Loss: 0.0832, V.Acc: 65.84, V.AUC: 0.6773;\n",
      "Trigger times >= patience: 0\n",
      "[E 60/2000] T.Loss: 0.0781, T.Acc: 69.27, T.AUC: 0.6329 V.Loss: 0.0832, V.Acc: 65.84, V.AUC: 0.6773;\n",
      "Trigger times >= patience: 0\n",
      "[E 61/2000] T.Loss: 0.0780, T.Acc: 69.27, T.AUC: 0.6386 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6771;\n",
      "Trigger times >= patience: 0\n",
      "[E 62/2000] T.Loss: 0.0779, T.Acc: 69.27, T.AUC: 0.6550 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6770;\n",
      "Trigger times >= patience: 0\n",
      "[E 63/2000] T.Loss: 0.0779, T.Acc: 69.27, T.AUC: 0.6594 V.Loss: 0.0830, V.Acc: 65.84, V.AUC: 0.6767;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 64/2000] T.Loss: 0.0778, T.Acc: 69.27, T.AUC: 0.6398 V.Loss: 0.0829, V.Acc: 65.84, V.AUC: 0.6767;\n",
      "Trigger times >= patience: 0\n",
      "[E 65/2000] T.Loss: 0.0777, T.Acc: 69.27, T.AUC: 0.6260 V.Loss: 0.0829, V.Acc: 65.84, V.AUC: 0.6769;\n",
      "Trigger times >= patience: 0\n",
      "[E 66/2000] T.Loss: 0.0776, T.Acc: 69.27, T.AUC: 0.6438 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6770;\n",
      "Trigger times >= patience: 0\n",
      "[E 67/2000] T.Loss: 0.0776, T.Acc: 69.27, T.AUC: 0.6746 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6770;\n",
      "Trigger times >= patience: 0\n",
      "[E 68/2000] T.Loss: 0.0775, T.Acc: 69.27, T.AUC: 0.6557 V.Loss: 0.0827, V.Acc: 65.84, V.AUC: 0.6766;\n",
      "Trigger times >= patience: 0\n",
      "[E 69/2000] T.Loss: 0.0774, T.Acc: 69.27, T.AUC: 0.6668 V.Loss: 0.0827, V.Acc: 65.84, V.AUC: 0.6761;\n",
      "Trigger times >= patience: 0\n",
      "[E 70/2000] T.Loss: 0.0773, T.Acc: 69.27, T.AUC: 0.6481 V.Loss: 0.0826, V.Acc: 65.84, V.AUC: 0.6763;\n",
      "Trigger times >= patience: 0\n",
      "[E 71/2000] T.Loss: 0.0772, T.Acc: 69.27, T.AUC: 0.6593 V.Loss: 0.0824, V.Acc: 65.84, V.AUC: 0.6764;\n",
      "Trigger times >= patience: 0\n",
      "[E 72/2000] T.Loss: 0.0771, T.Acc: 69.27, T.AUC: 0.6673 V.Loss: 0.0823, V.Acc: 65.84, V.AUC: 0.6764;\n",
      "Trigger times >= patience: 0\n",
      "[E 73/2000] T.Loss: 0.0770, T.Acc: 69.27, T.AUC: 0.6514 V.Loss: 0.0822, V.Acc: 65.84, V.AUC: 0.6759;\n",
      "Trigger times >= patience: 0\n",
      "[E 74/2000] T.Loss: 0.0769, T.Acc: 69.27, T.AUC: 0.6785 V.Loss: 0.0823, V.Acc: 65.84, V.AUC: 0.6760;\n",
      "Trigger times >= patience: 0\n",
      "[E 75/2000] T.Loss: 0.0768, T.Acc: 69.27, T.AUC: 0.6675 V.Loss: 0.0822, V.Acc: 65.84, V.AUC: 0.6760;\n",
      "Trigger times >= patience: 0\n",
      "[E 76/2000] T.Loss: 0.0767, T.Acc: 69.27, T.AUC: 0.6569 V.Loss: 0.0822, V.Acc: 65.84, V.AUC: 0.6762;\n",
      "Trigger times >= patience: 0\n",
      "[E 77/2000] T.Loss: 0.0765, T.Acc: 69.27, T.AUC: 0.6833 V.Loss: 0.0819, V.Acc: 65.84, V.AUC: 0.6759;\n",
      "Trigger times >= patience: 0\n",
      "[E 78/2000] T.Loss: 0.0764, T.Acc: 69.27, T.AUC: 0.6612 V.Loss: 0.0817, V.Acc: 65.84, V.AUC: 0.6760;\n",
      "Trigger times >= patience: 0\n",
      "[E 79/2000] T.Loss: 0.0763, T.Acc: 69.27, T.AUC: 0.6672 V.Loss: 0.0817, V.Acc: 65.84, V.AUC: 0.6762;\n",
      "Trigger times >= patience: 0\n",
      "[E 80/2000] T.Loss: 0.0762, T.Acc: 69.27, T.AUC: 0.6723 V.Loss: 0.0816, V.Acc: 65.84, V.AUC: 0.6767;\n",
      "Trigger times >= patience: 0\n",
      "[E 81/2000] T.Loss: 0.0760, T.Acc: 69.27, T.AUC: 0.6629 V.Loss: 0.0817, V.Acc: 65.84, V.AUC: 0.6771;\n",
      "Trigger times >= patience: 0\n",
      "[E 82/2000] T.Loss: 0.0759, T.Acc: 69.27, T.AUC: 0.6911 V.Loss: 0.0816, V.Acc: 65.84, V.AUC: 0.6774;\n",
      "Trigger times >= patience: 0\n",
      "[E 83/2000] T.Loss: 0.0757, T.Acc: 69.27, T.AUC: 0.6620 V.Loss: 0.0813, V.Acc: 65.84, V.AUC: 0.6776;\n",
      "Trigger times >= patience: 0\n",
      "[E 84/2000] T.Loss: 0.0756, T.Acc: 69.27, T.AUC: 0.6757 V.Loss: 0.0811, V.Acc: 65.84, V.AUC: 0.6778;\n",
      "Trigger times >= patience: 0\n",
      "[E 85/2000] T.Loss: 0.0754, T.Acc: 69.27, T.AUC: 0.6715 V.Loss: 0.0810, V.Acc: 65.84, V.AUC: 0.6779;\n",
      "Trigger times >= patience: 0\n",
      "[E 86/2000] T.Loss: 0.0753, T.Acc: 69.27, T.AUC: 0.6682 V.Loss: 0.0810, V.Acc: 65.56, V.AUC: 0.6774;\n",
      "Trigger times >= patience: 0\n",
      "[E 87/2000] T.Loss: 0.0751, T.Acc: 69.27, T.AUC: 0.6847 V.Loss: 0.0808, V.Acc: 65.56, V.AUC: 0.6774;\n",
      "Trigger times >= patience: 0\n",
      "[E 88/2000] T.Loss: 0.0750, T.Acc: 69.27, T.AUC: 0.6772 V.Loss: 0.0806, V.Acc: 65.56, V.AUC: 0.6776;\n",
      "Trigger times >= patience: 0\n",
      "[E 89/2000] T.Loss: 0.0748, T.Acc: 69.00, T.AUC: 0.6712 V.Loss: 0.0805, V.Acc: 65.29, V.AUC: 0.6776;\n",
      "Trigger times >= patience: 0\n",
      "[E 90/2000] T.Loss: 0.0746, T.Acc: 69.27, T.AUC: 0.6712 V.Loss: 0.0805, V.Acc: 65.01, V.AUC: 0.6777;\n",
      "Trigger times >= patience: 0\n",
      "[E 91/2000] T.Loss: 0.0745, T.Acc: 69.27, T.AUC: 0.6782 V.Loss: 0.0804, V.Acc: 65.01, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 92/2000] T.Loss: 0.0743, T.Acc: 69.14, T.AUC: 0.6866 V.Loss: 0.0802, V.Acc: 64.74, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 93/2000] T.Loss: 0.0741, T.Acc: 69.00, T.AUC: 0.6788 V.Loss: 0.0801, V.Acc: 64.74, V.AUC: 0.6790;\n",
      "Trigger times >= patience: 0\n",
      "[E 94/2000] T.Loss: 0.0740, T.Acc: 68.72, T.AUC: 0.6803 V.Loss: 0.0797, V.Acc: 64.74, V.AUC: 0.6791;\n",
      "Trigger times >= patience: 0\n",
      "[E 95/2000] T.Loss: 0.0738, T.Acc: 69.00, T.AUC: 0.6902 V.Loss: 0.0798, V.Acc: 64.74, V.AUC: 0.6792;\n",
      "Trigger times >= patience: 0\n",
      "[E 96/2000] T.Loss: 0.0736, T.Acc: 68.59, T.AUC: 0.6928 V.Loss: 0.0800, V.Acc: 64.74, V.AUC: 0.6791;\n",
      "Trigger times >= patience: 0\n",
      "[E 97/2000] T.Loss: 0.0734, T.Acc: 68.45, T.AUC: 0.6814 V.Loss: 0.0797, V.Acc: 64.74, V.AUC: 0.6792;\n",
      "Trigger times >= patience: 0\n",
      "[E 98/2000] T.Loss: 0.0732, T.Acc: 68.86, T.AUC: 0.6908 V.Loss: 0.0794, V.Acc: 65.01, V.AUC: 0.6791;\n",
      "Trigger times >= patience: 0\n",
      "[E 99/2000] T.Loss: 0.0731, T.Acc: 69.41, T.AUC: 0.6923 V.Loss: 0.0792, V.Acc: 65.01, V.AUC: 0.6789;\n",
      "Trigger times >= patience: 0\n",
      "[E 100/2000] T.Loss: 0.0729, T.Acc: 69.41, T.AUC: 0.7030 V.Loss: 0.0792, V.Acc: 65.01, V.AUC: 0.6787;\n",
      "Trigger times >= patience: 0\n",
      "[E 101/2000] T.Loss: 0.0727, T.Acc: 69.27, T.AUC: 0.6878 V.Loss: 0.0794, V.Acc: 64.74, V.AUC: 0.6790;\n",
      "Trigger times >= patience: 0\n",
      "[E 102/2000] T.Loss: 0.0725, T.Acc: 68.45, T.AUC: 0.6991 V.Loss: 0.0794, V.Acc: 64.74, V.AUC: 0.6789;\n",
      "Trigger times >= patience: 0\n",
      "[E 103/2000] T.Loss: 0.0724, T.Acc: 69.41, T.AUC: 0.6953 V.Loss: 0.0793, V.Acc: 64.74, V.AUC: 0.6791;\n",
      "Trigger times >= patience: 0\n",
      "[E 104/2000] T.Loss: 0.0722, T.Acc: 68.59, T.AUC: 0.6977 V.Loss: 0.0789, V.Acc: 67.49, V.AUC: 0.6788;\n",
      "Trigger times >= patience: 0\n",
      "[E 105/2000] T.Loss: 0.0720, T.Acc: 69.68, T.AUC: 0.6879 V.Loss: 0.0787, V.Acc: 67.77, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 106/2000] T.Loss: 0.0719, T.Acc: 69.68, T.AUC: 0.6965 V.Loss: 0.0786, V.Acc: 68.32, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 107/2000] T.Loss: 0.0717, T.Acc: 69.68, T.AUC: 0.6940 V.Loss: 0.0791, V.Acc: 66.94, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 108/2000] T.Loss: 0.0716, T.Acc: 70.23, T.AUC: 0.7113 V.Loss: 0.0791, V.Acc: 66.94, V.AUC: 0.6789;\n",
      "Loss Trigger Times: 1\n",
      "[E 109/2000] T.Loss: 0.0714, T.Acc: 69.82, T.AUC: 0.7078 V.Loss: 0.0785, V.Acc: 68.32, V.AUC: 0.6791;\n",
      "Trigger times >= patience: 0\n",
      "[E 110/2000] T.Loss: 0.0713, T.Acc: 70.78, T.AUC: 0.7014 V.Loss: 0.0782, V.Acc: 69.15, V.AUC: 0.6797;\n",
      "Trigger times >= patience: 0\n",
      "[E 111/2000] T.Loss: 0.0711, T.Acc: 70.23, T.AUC: 0.6968 V.Loss: 0.0783, V.Acc: 68.87, V.AUC: 0.6796;\n",
      "Trigger times >= patience: 0\n",
      "[E 112/2000] T.Loss: 0.0709, T.Acc: 70.37, T.AUC: 0.7021 V.Loss: 0.0784, V.Acc: 68.32, V.AUC: 0.6798;\n",
      "Trigger times >= patience: 0\n",
      "[E 113/2000] T.Loss: 0.0708, T.Acc: 69.96, T.AUC: 0.7037 V.Loss: 0.0787, V.Acc: 68.04, V.AUC: 0.6802;\n",
      "Trigger times >= patience: 0\n",
      "[E 114/2000] T.Loss: 0.0706, T.Acc: 70.51, T.AUC: 0.7140 V.Loss: 0.0783, V.Acc: 68.60, V.AUC: 0.6797;\n",
      "Trigger times >= patience: 0\n",
      "[E 115/2000] T.Loss: 0.0705, T.Acc: 70.37, T.AUC: 0.7035 V.Loss: 0.0783, V.Acc: 68.87, V.AUC: 0.6800;\n",
      "Trigger times >= patience: 0\n",
      "[E 116/2000] T.Loss: 0.0704, T.Acc: 70.37, T.AUC: 0.7135 V.Loss: 0.0782, V.Acc: 68.60, V.AUC: 0.6804;\n",
      "Trigger times >= patience: 0\n",
      "[E 117/2000] T.Loss: 0.0702, T.Acc: 70.10, T.AUC: 0.7032 V.Loss: 0.0782, V.Acc: 68.87, V.AUC: 0.6804;\n",
      "Trigger times >= patience: 0\n",
      "[E 118/2000] T.Loss: 0.0701, T.Acc: 70.37, T.AUC: 0.7013 V.Loss: 0.0781, V.Acc: 68.32, V.AUC: 0.6802;\n",
      "Trigger times >= patience: 0\n",
      "[E 119/2000] T.Loss: 0.0700, T.Acc: 70.51, T.AUC: 0.7172 V.Loss: 0.0783, V.Acc: 69.15, V.AUC: 0.6792;\n",
      "Trigger times >= patience: 0\n",
      "[E 120/2000] T.Loss: 0.0698, T.Acc: 70.37, T.AUC: 0.7088 V.Loss: 0.0782, V.Acc: 69.15, V.AUC: 0.6788;\n",
      "Trigger times >= patience: 0\n",
      "[E 121/2000] T.Loss: 0.0698, T.Acc: 71.47, T.AUC: 0.7096 V.Loss: 0.0779, V.Acc: 67.77, V.AUC: 0.6783;\n",
      "Trigger times >= patience: 0\n",
      "[E 122/2000] T.Loss: 0.0696, T.Acc: 71.19, T.AUC: 0.7088 V.Loss: 0.0780, V.Acc: 68.04, V.AUC: 0.6778;\n",
      "Trigger times >= patience: 0\n",
      "[E 123/2000] T.Loss: 0.0695, T.Acc: 71.74, T.AUC: 0.7189 V.Loss: 0.0782, V.Acc: 68.60, V.AUC: 0.6775;\n",
      "Loss Trigger Times: 1\n",
      "[E 124/2000] T.Loss: 0.0694, T.Acc: 71.06, T.AUC: 0.7194 V.Loss: 0.0786, V.Acc: 68.87, V.AUC: 0.6775;\n",
      "Loss Trigger Times: 2\n",
      "[E 125/2000] T.Loss: 0.0692, T.Acc: 70.78, T.AUC: 0.7175 V.Loss: 0.0782, V.Acc: 67.77, V.AUC: 0.6772;\n",
      "Loss Trigger Times: 3\n",
      "[E 126/2000] T.Loss: 0.0692, T.Acc: 71.88, T.AUC: 0.7230 V.Loss: 0.0777, V.Acc: 68.04, V.AUC: 0.6770;\n",
      "Trigger times >= patience: 0\n",
      "[E 127/2000] T.Loss: 0.0691, T.Acc: 72.29, T.AUC: 0.7273 V.Loss: 0.0777, V.Acc: 67.77, V.AUC: 0.6773;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 128/2000] T.Loss: 0.0689, T.Acc: 72.29, T.AUC: 0.7197 V.Loss: 0.0778, V.Acc: 67.77, V.AUC: 0.6769;\n",
      "Trigger times >= patience: 0\n",
      "[E 129/2000] T.Loss: 0.0688, T.Acc: 72.02, T.AUC: 0.7296 V.Loss: 0.0779, V.Acc: 68.04, V.AUC: 0.6768;\n",
      "Trigger times >= patience: 0\n",
      "[E 130/2000] T.Loss: 0.0687, T.Acc: 72.57, T.AUC: 0.7210 V.Loss: 0.0785, V.Acc: 68.04, V.AUC: 0.6757;\n",
      "Loss Trigger Times: 1\n",
      "[E 131/2000] T.Loss: 0.0685, T.Acc: 72.43, T.AUC: 0.7255 V.Loss: 0.0783, V.Acc: 68.32, V.AUC: 0.6758;\n",
      "Loss Trigger Times: 2\n",
      "[E 132/2000] T.Loss: 0.0684, T.Acc: 72.57, T.AUC: 0.7260 V.Loss: 0.0780, V.Acc: 68.04, V.AUC: 0.6751;\n",
      "Trigger times >= patience: 0\n",
      "[E 133/2000] T.Loss: 0.0684, T.Acc: 72.29, T.AUC: 0.7176 V.Loss: 0.0777, V.Acc: 67.49, V.AUC: 0.6749;\n",
      "Trigger times >= patience: 0\n",
      "[E 134/2000] T.Loss: 0.0682, T.Acc: 73.39, T.AUC: 0.7263 V.Loss: 0.0784, V.Acc: 68.60, V.AUC: 0.6743;\n",
      "Loss Trigger Times: 1\n",
      "[E 135/2000] T.Loss: 0.0682, T.Acc: 71.74, T.AUC: 0.7234 V.Loss: 0.0786, V.Acc: 68.04, V.AUC: 0.6746;\n",
      "Loss Trigger Times: 2\n",
      "[E 136/2000] T.Loss: 0.0680, T.Acc: 72.29, T.AUC: 0.7268 V.Loss: 0.0779, V.Acc: 67.77, V.AUC: 0.6744;\n",
      "Trigger times >= patience: 0\n",
      "[E 137/2000] T.Loss: 0.0679, T.Acc: 73.11, T.AUC: 0.7338 V.Loss: 0.0779, V.Acc: 68.04, V.AUC: 0.6744;\n",
      "Trigger times >= patience: 0\n",
      "[E 138/2000] T.Loss: 0.0678, T.Acc: 73.25, T.AUC: 0.7278 V.Loss: 0.0784, V.Acc: 68.32, V.AUC: 0.6743;\n",
      "Loss Trigger Times: 1\n",
      "[E 139/2000] T.Loss: 0.0677, T.Acc: 73.11, T.AUC: 0.7299 V.Loss: 0.0784, V.Acc: 68.60, V.AUC: 0.6742;\n",
      "Loss Trigger Times: 2\n",
      "[E 140/2000] T.Loss: 0.0676, T.Acc: 72.84, T.AUC: 0.7308 V.Loss: 0.0779, V.Acc: 68.04, V.AUC: 0.6740;\n",
      "Trigger times >= patience: 0\n",
      "[E 141/2000] T.Loss: 0.0675, T.Acc: 73.80, T.AUC: 0.7302 V.Loss: 0.0779, V.Acc: 68.04, V.AUC: 0.6738;\n",
      "Trigger times >= patience: 0\n",
      "[E 142/2000] T.Loss: 0.0674, T.Acc: 73.80, T.AUC: 0.7360 V.Loss: 0.0782, V.Acc: 67.77, V.AUC: 0.6729;\n",
      "Loss Trigger Times: 1\n",
      "[E 143/2000] T.Loss: 0.0672, T.Acc: 73.39, T.AUC: 0.7345 V.Loss: 0.0782, V.Acc: 68.04, V.AUC: 0.6726;\n",
      "Loss Trigger Times: 2\n",
      "[E 144/2000] T.Loss: 0.0672, T.Acc: 73.53, T.AUC: 0.7331 V.Loss: 0.0785, V.Acc: 68.04, V.AUC: 0.6726;\n",
      "Loss Trigger Times: 3\n",
      "[E 145/2000] T.Loss: 0.0671, T.Acc: 73.66, T.AUC: 0.7337 V.Loss: 0.0782, V.Acc: 68.04, V.AUC: 0.6726;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.75\n",
      "0.7057733868477924\n",
      "[[73  6]\n",
      " [24 17]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: phylotype data       ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_pty, pty_hist, ptytest_obs, ptytest_pred, ptytest_prob, ptytest_auc, ptytest_acc, ptytest_conf = FirstStage_pty(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod)\n",
    "print(ptytest_acc)\n",
    "print(ptytest_auc)\n",
    "print(ptytest_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b67692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ txy LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0925, T.Acc: 62.69, T.AUC: 0.4933 V.Loss: 0.0928, V.Acc: 65.84, V.AUC: 0.5102;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0912, T.Acc: 69.27, T.AUC: 0.5094 V.Loss: 0.0917, V.Acc: 65.84, V.AUC: 0.5203;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0897, T.Acc: 69.27, T.AUC: 0.5216 V.Loss: 0.0905, V.Acc: 65.84, V.AUC: 0.5235;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0878, T.Acc: 69.27, T.AUC: 0.4831 V.Loss: 0.0890, V.Acc: 65.84, V.AUC: 0.5257;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0856, T.Acc: 69.27, T.AUC: 0.4855 V.Loss: 0.0875, V.Acc: 65.84, V.AUC: 0.5264;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0835, T.Acc: 69.27, T.AUC: 0.5035 V.Loss: 0.0860, V.Acc: 65.84, V.AUC: 0.5277;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0816, T.Acc: 69.27, T.AUC: 0.4753 V.Loss: 0.0850, V.Acc: 65.84, V.AUC: 0.5269;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0804, T.Acc: 69.27, T.AUC: 0.5355 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.5276;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0799, T.Acc: 69.27, T.AUC: 0.4945 V.Loss: 0.0848, V.Acc: 65.84, V.AUC: 0.5301;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0798, T.Acc: 69.27, T.AUC: 0.4812 V.Loss: 0.0852, V.Acc: 65.84, V.AUC: 0.5366;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0799, T.Acc: 69.27, T.AUC: 0.4810 V.Loss: 0.0854, V.Acc: 65.84, V.AUC: 0.5496;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0799, T.Acc: 69.27, T.AUC: 0.5035 V.Loss: 0.0854, V.Acc: 65.84, V.AUC: 0.5617;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0798, T.Acc: 69.27, T.AUC: 0.5022 V.Loss: 0.0852, V.Acc: 65.84, V.AUC: 0.5698;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5074 V.Loss: 0.0849, V.Acc: 65.84, V.AUC: 0.5736;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5611 V.Loss: 0.0847, V.Acc: 65.84, V.AUC: 0.5825;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.4860 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.5918;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5355 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6023;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5326 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6083;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5304 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6128;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5429 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6170;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5234 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6238;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5797 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6325;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5868 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6427;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5489 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6511;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.4922 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6582;\n",
      "Loss Trigger Times: 1\n",
      "[E 26/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5920 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6628;\n",
      "Loss Trigger Times: 2\n",
      "[E 27/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5519 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6662;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5557 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6703;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5573 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6737;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5642 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6758;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5921 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.6242 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6783;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.5661 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6814;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.5634 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6838;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.6011 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6840;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.5971 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6856;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.5916 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6857;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.6238 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6856;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.6273 V.Loss: 0.0838, V.Acc: 65.84, V.AUC: 0.6855;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0787, T.Acc: 69.27, T.AUC: 0.6519 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6849;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0787, T.Acc: 69.27, T.AUC: 0.6073 V.Loss: 0.0836, V.Acc: 65.84, V.AUC: 0.6849;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.6260 V.Loss: 0.0835, V.Acc: 65.84, V.AUC: 0.6846;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0785, T.Acc: 69.27, T.AUC: 0.6169 V.Loss: 0.0835, V.Acc: 65.84, V.AUC: 0.6844;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0784, T.Acc: 69.27, T.AUC: 0.6502 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6836;\n",
      "Trigger times >= patience: 0\n",
      "[E 45/2000] T.Loss: 0.0783, T.Acc: 69.27, T.AUC: 0.6489 V.Loss: 0.0833, V.Acc: 65.84, V.AUC: 0.6829;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.6462 V.Loss: 0.0832, V.Acc: 65.84, V.AUC: 0.6823;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0781, T.Acc: 69.27, T.AUC: 0.6495 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6817;\n",
      "Trigger times >= patience: 0\n",
      "[E 48/2000] T.Loss: 0.0780, T.Acc: 69.27, T.AUC: 0.6608 V.Loss: 0.0830, V.Acc: 65.84, V.AUC: 0.6815;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0779, T.Acc: 69.27, T.AUC: 0.6751 V.Loss: 0.0830, V.Acc: 65.84, V.AUC: 0.6807;\n",
      "Trigger times >= patience: 0\n",
      "[E 50/2000] T.Loss: 0.0777, T.Acc: 69.27, T.AUC: 0.6512 V.Loss: 0.0830, V.Acc: 65.84, V.AUC: 0.6805;\n",
      "Trigger times >= patience: 0\n",
      "[E 51/2000] T.Loss: 0.0776, T.Acc: 69.27, T.AUC: 0.6463 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6807;\n",
      "Trigger times >= patience: 0\n",
      "[E 52/2000] T.Loss: 0.0774, T.Acc: 69.27, T.AUC: 0.6595 V.Loss: 0.0825, V.Acc: 65.84, V.AUC: 0.6803;\n",
      "Trigger times >= patience: 0\n",
      "[E 53/2000] T.Loss: 0.0773, T.Acc: 69.27, T.AUC: 0.6512 V.Loss: 0.0822, V.Acc: 65.84, V.AUC: 0.6798;\n",
      "Trigger times >= patience: 0\n",
      "[E 54/2000] T.Loss: 0.0771, T.Acc: 69.27, T.AUC: 0.6578 V.Loss: 0.0820, V.Acc: 65.84, V.AUC: 0.6805;\n",
      "Trigger times >= patience: 0\n",
      "[E 55/2000] T.Loss: 0.0768, T.Acc: 69.27, T.AUC: 0.6754 V.Loss: 0.0820, V.Acc: 65.84, V.AUC: 0.6799;\n",
      "Trigger times >= patience: 0\n",
      "[E 56/2000] T.Loss: 0.0766, T.Acc: 69.27, T.AUC: 0.6798 V.Loss: 0.0820, V.Acc: 65.84, V.AUC: 0.6804;\n",
      "Trigger times >= patience: 0\n",
      "[E 57/2000] T.Loss: 0.0764, T.Acc: 69.27, T.AUC: 0.6731 V.Loss: 0.0817, V.Acc: 65.84, V.AUC: 0.6798;\n",
      "Trigger times >= patience: 0\n",
      "[E 58/2000] T.Loss: 0.0761, T.Acc: 69.27, T.AUC: 0.6734 V.Loss: 0.0813, V.Acc: 65.84, V.AUC: 0.6796;\n",
      "Trigger times >= patience: 0\n",
      "[E 59/2000] T.Loss: 0.0758, T.Acc: 69.27, T.AUC: 0.6694 V.Loss: 0.0810, V.Acc: 65.56, V.AUC: 0.6788;\n",
      "Trigger times >= patience: 0\n",
      "[E 60/2000] T.Loss: 0.0755, T.Acc: 69.14, T.AUC: 0.6889 V.Loss: 0.0809, V.Acc: 65.01, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 61/2000] T.Loss: 0.0752, T.Acc: 68.86, T.AUC: 0.6700 V.Loss: 0.0806, V.Acc: 64.74, V.AUC: 0.6779;\n",
      "Trigger times >= patience: 0\n",
      "[E 62/2000] T.Loss: 0.0748, T.Acc: 68.72, T.AUC: 0.6860 V.Loss: 0.0803, V.Acc: 65.01, V.AUC: 0.6777;\n",
      "Trigger times >= patience: 0\n",
      "[E 63/2000] T.Loss: 0.0745, T.Acc: 68.86, T.AUC: 0.6856 V.Loss: 0.0801, V.Acc: 65.01, V.AUC: 0.6776;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 64/2000] T.Loss: 0.0741, T.Acc: 69.27, T.AUC: 0.6797 V.Loss: 0.0797, V.Acc: 67.22, V.AUC: 0.6777;\n",
      "Trigger times >= patience: 0\n",
      "[E 65/2000] T.Loss: 0.0737, T.Acc: 68.45, T.AUC: 0.6796 V.Loss: 0.0798, V.Acc: 65.29, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 66/2000] T.Loss: 0.0734, T.Acc: 69.68, T.AUC: 0.6790 V.Loss: 0.0795, V.Acc: 67.22, V.AUC: 0.6787;\n",
      "Trigger times >= patience: 0\n",
      "[E 67/2000] T.Loss: 0.0731, T.Acc: 68.86, T.AUC: 0.6876 V.Loss: 0.0795, V.Acc: 67.77, V.AUC: 0.6784;\n",
      "Trigger times >= patience: 0\n",
      "[E 68/2000] T.Loss: 0.0727, T.Acc: 70.10, T.AUC: 0.6935 V.Loss: 0.0792, V.Acc: 67.77, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 69/2000] T.Loss: 0.0725, T.Acc: 71.06, T.AUC: 0.6900 V.Loss: 0.0791, V.Acc: 67.77, V.AUC: 0.6781;\n",
      "Trigger times >= patience: 0\n",
      "[E 70/2000] T.Loss: 0.0722, T.Acc: 70.78, T.AUC: 0.6960 V.Loss: 0.0791, V.Acc: 68.32, V.AUC: 0.6770;\n",
      "Trigger times >= patience: 0\n",
      "[E 71/2000] T.Loss: 0.0719, T.Acc: 71.06, T.AUC: 0.6891 V.Loss: 0.0791, V.Acc: 68.60, V.AUC: 0.6747;\n",
      "Trigger times >= patience: 0\n",
      "[E 72/2000] T.Loss: 0.0716, T.Acc: 71.47, T.AUC: 0.7020 V.Loss: 0.0791, V.Acc: 68.60, V.AUC: 0.6735;\n",
      "Trigger times >= patience: 0\n",
      "[E 73/2000] T.Loss: 0.0714, T.Acc: 70.64, T.AUC: 0.7072 V.Loss: 0.0792, V.Acc: 68.32, V.AUC: 0.6731;\n",
      "Trigger times >= patience: 0\n",
      "[E 74/2000] T.Loss: 0.0711, T.Acc: 71.19, T.AUC: 0.6938 V.Loss: 0.0790, V.Acc: 68.60, V.AUC: 0.6724;\n",
      "Trigger times >= patience: 0\n",
      "[E 75/2000] T.Loss: 0.0709, T.Acc: 71.47, T.AUC: 0.7091 V.Loss: 0.0793, V.Acc: 68.32, V.AUC: 0.6703;\n",
      "Loss Trigger Times: 1\n",
      "[E 76/2000] T.Loss: 0.0706, T.Acc: 72.29, T.AUC: 0.7086 V.Loss: 0.0791, V.Acc: 67.77, V.AUC: 0.6693;\n",
      "Trigger times >= patience: 0\n",
      "[E 77/2000] T.Loss: 0.0704, T.Acc: 72.57, T.AUC: 0.7025 V.Loss: 0.0792, V.Acc: 68.04, V.AUC: 0.6687;\n",
      "Loss Trigger Times: 1\n",
      "[E 78/2000] T.Loss: 0.0701, T.Acc: 72.70, T.AUC: 0.7090 V.Loss: 0.0793, V.Acc: 68.32, V.AUC: 0.6678;\n",
      "Loss Trigger Times: 2\n",
      "[E 79/2000] T.Loss: 0.0700, T.Acc: 72.29, T.AUC: 0.7100 V.Loss: 0.0795, V.Acc: 68.32, V.AUC: 0.6641;\n",
      "Loss Trigger Times: 3\n",
      "[E 80/2000] T.Loss: 0.0700, T.Acc: 72.57, T.AUC: 0.7137 V.Loss: 0.0792, V.Acc: 67.77, V.AUC: 0.6642;\n",
      "Trigger times >= patience: 0\n",
      "[E 81/2000] T.Loss: 0.0696, T.Acc: 73.11, T.AUC: 0.7083 V.Loss: 0.0796, V.Acc: 68.04, V.AUC: 0.6606;\n",
      "Loss Trigger Times: 1\n",
      "[E 82/2000] T.Loss: 0.0693, T.Acc: 72.84, T.AUC: 0.7242 V.Loss: 0.0794, V.Acc: 68.32, V.AUC: 0.6610;\n",
      "Loss Trigger Times: 2\n",
      "[E 83/2000] T.Loss: 0.0692, T.Acc: 73.25, T.AUC: 0.7201 V.Loss: 0.0793, V.Acc: 67.49, V.AUC: 0.6612;\n",
      "Loss Trigger Times: 3\n",
      "[E 84/2000] T.Loss: 0.0690, T.Acc: 73.66, T.AUC: 0.7091 V.Loss: 0.0795, V.Acc: 68.60, V.AUC: 0.6604;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.75\n",
      "0.7205927755480087\n",
      "[[74  5]\n",
      " [25 16]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: taxonomy data        ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_txy, txy_hist, txytest_obs, txytest_pred, txytest_prob, txytest_auc, txytest_acc, txytest_conf = FirstStage_txy(mytrain_input_txy, mytrain_output, myvalid_input_txy, myvalid_output, mytest_input_txy, mytest_output, finalperiod)\n",
    "print(txytest_acc)\n",
    "print(txytest_auc)\n",
    "print(txytest_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b6192b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3416666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mytest_output[:,finalperiod-1,0])/mytest_output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d415957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3415977961432507\n",
      "0.7583333333333333\n",
      "0.7665946279715962\n",
      "[[74  5]\n",
      " [24 17]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use validation set only without class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "Mtdvalid_obs, Mtdvalid_pred, Mtdvalid_prob = evaluate(model_Mtd, device, myvalid_input_mtd, myvalid_output, finalperiod, cutoff=0.5)\n",
    "ptyvalid_obs, ptyvalid_pred, ptyvalid_prob = evaluate(model_pty, device, myvalid_input_pty, myvalid_output, finalperiod, cutoff=0.5)\n",
    "txyvalid_obs, txyvalid_pred, txyvalid_prob = evaluate(model_txy, device, myvalid_input_txy, myvalid_output, finalperiod, cutoff=0.5)\n",
    "\n",
    "x_valid = np.array(np.column_stack([Mtdvalid_prob, ptyvalid_prob, txyvalid_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(Mtdvalid_obs)/len(Mtdvalid_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag')\n",
    "L2Logistic_model.fit(x_valid, Mtdvalid_obs)\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ebac487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3415977961432507\n",
      "0.7333333333333333\n",
      "0.7616548317381907\n",
      "[[61 18]\n",
      " [14 27]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use validation set only with class weights (Best)\n",
    "#-------------------------------------------#\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "Mtdvalid_obs, Mtdvalid_pred, Mtdvalid_prob = evaluate(model_Mtd, device, myvalid_input_mtd, myvalid_output, finalperiod, cutoff=0.5)\n",
    "ptyvalid_obs, ptyvalid_pred, ptyvalid_prob = evaluate(model_pty, device, myvalid_input_pty, myvalid_output, finalperiod, cutoff=0.5)\n",
    "txyvalid_obs, txyvalid_pred, txyvalid_prob = evaluate(model_txy, device, myvalid_input_txy, myvalid_output, finalperiod, cutoff=0.5)\n",
    "\n",
    "x_valid = np.array(np.column_stack([Mtdvalid_prob, ptyvalid_prob, txyvalid_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(Mtdvalid_obs)/len(Mtdvalid_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag', class_weight=wt)\n",
    "L2Logistic_model.fit(x_valid, Mtdvalid_obs)\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd20ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31868131868131866\n",
      "0.7666666666666667\n",
      "0.7542451373880827\n",
      "[[72  7]\n",
      " [21 20]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use trianing+validation set without class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "MtdS2train_input = np.concatenate((mytrain_input_mtd, myvalid_input_mtd), axis=0)\n",
    "ptyS2train_input = np.concatenate((mytrain_input_pty, myvalid_input_pty), axis=0)\n",
    "txyS2train_input = np.concatenate((mytrain_input_txy, myvalid_input_txy), axis=0)\n",
    "\n",
    "S2train_output = np.concatenate((mytrain_output, myvalid_output), axis=0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "\n",
    "MtdS2_obs, MtdS2_pred, MtdS2_prob = evaluate(model_Mtd, device, MtdS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "ptyS2_obs, ptyS2_pred, ptyS2_prob = evaluate(model_pty, device, ptyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "txyS2_obs, txyS2_pred, txyS2_prob = evaluate(model_txy, device, txyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "x_valid = np.array(np.column_stack([MtdS2_prob, ptyS2_prob, txyS2_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(MtdS2_obs)/len(MtdS2_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag')\n",
    "L2Logistic_model.fit(x_valid, MtdS2_obs)\n",
    "\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "# x_test = np.array(np.transpose([Mtdtest_prob, ptytest_prob, txytest_prob, krdtest_prob])).reshape(-1, 3*2)\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55178019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31868131868131866\n",
      "0.7166666666666667\n",
      "0.7548626119172585\n",
      "[[60 19]\n",
      " [15 26]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use trianing + validation set with class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "MtdS2train_input = np.concatenate((mytrain_input_mtd, myvalid_input_mtd), axis=0)\n",
    "ptyS2train_input = np.concatenate((mytrain_input_pty, myvalid_input_pty), axis=0)\n",
    "txyS2train_input = np.concatenate((mytrain_input_txy, myvalid_input_txy), axis=0)\n",
    "\n",
    "S2train_output = np.concatenate((mytrain_output, myvalid_output), axis=0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "\n",
    "MtdS2_obs, MtdS2_pred, MtdS2_prob = evaluate(model_Mtd, device, MtdS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "ptyS2_obs, ptyS2_pred, ptyS2_prob = evaluate(model_pty, device, ptyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "txyS2_obs, txyS2_pred, txyS2_prob = evaluate(model_txy, device, txyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "x_valid = np.array(np.column_stack([MtdS2_prob, ptyS2_prob, txyS2_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(MtdS2_obs)/len(MtdS2_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag', class_weight=wt)\n",
    "L2Logistic_model.fit(x_valid, MtdS2_obs)\n",
    "\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "# x_test = np.array(np.transpose([Mtdtest_prob, ptytest_prob, txytest_prob, krdtest_prob])).reshape(-1, 3*2)\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f5fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870a092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
