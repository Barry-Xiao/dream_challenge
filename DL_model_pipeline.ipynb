{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f0fb9d",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8b25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a08086",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 8022022 # or any of your favorite number \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b461620",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e4a851",
   "metadata": {},
   "source": [
    "* Make sure your input dataset included `participant_id`,`specimen`,`collect_wk`,`was_preterm`,`was_early_preterm`. Other variables should be co-variates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7299f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2461, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specimen</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>collect_wk</th>\n",
       "      <th>project</th>\n",
       "      <th>was_preterm</th>\n",
       "      <th>was_early_preterm</th>\n",
       "      <th>age_imp</th>\n",
       "      <th>shannon</th>\n",
       "      <th>bwpd</th>\n",
       "      <th>CST</th>\n",
       "      <th>Lactobacillus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00001-05</td>\n",
       "      <td>A00001</td>\n",
       "      <td>33</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>III</td>\n",
       "      <td>0.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00002-01</td>\n",
       "      <td>A00002</td>\n",
       "      <td>38</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>1.96362</td>\n",
       "      <td>2.62894</td>\n",
       "      <td>III</td>\n",
       "      <td>0.805641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A00003-02</td>\n",
       "      <td>A00003</td>\n",
       "      <td>30</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>II</td>\n",
       "      <td>0.963299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A00004-08</td>\n",
       "      <td>A00004</td>\n",
       "      <td>27</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>III</td>\n",
       "      <td>0.927544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A00004-12</td>\n",
       "      <td>A00004</td>\n",
       "      <td>29</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>6.94884</td>\n",
       "      <td>2.78896</td>\n",
       "      <td>III</td>\n",
       "      <td>0.806593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    specimen participant_id  collect_wk project  was_preterm  \\\n",
       "0  A00001-05         A00001          33       A        False   \n",
       "1  A00002-01         A00002          38       A        False   \n",
       "2  A00003-02         A00003          30       A        False   \n",
       "3  A00004-08         A00004          27       A        False   \n",
       "4  A00004-12         A00004          29       A        False   \n",
       "\n",
       "   was_early_preterm  age_imp  shannon     bwpd  CST  Lactobacillus  \n",
       "0              False       27  1.00000  0.00000  III       0.797900  \n",
       "1              False       24  1.96362  2.62894  III       0.805641  \n",
       "2              False       32  1.00000  0.00000   II       0.963299  \n",
       "3              False       25  1.00000  0.00000  III       0.927544  \n",
       "4              False       25  6.94884  2.78896  III       0.806593  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace 'test/metadata_imputed.csv' with the path to your input file\n",
    "\n",
    "mydata = pd.read_csv('data/combo_clean_data.csv', delimiter=',')\n",
    "mydata = pd.DataFrame(mydata)\n",
    "print(mydata.shape)\n",
    "mydata.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e16f71",
   "metadata": {},
   "source": [
    "## Data subset/type conversion if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ce0fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "specimen               object\n",
       "participant_id         object\n",
       "collect_wk              int64\n",
       "project              category\n",
       "was_preterm              int8\n",
       "was_early_preterm        int8\n",
       "age_imp                 int64\n",
       "shannon               float64\n",
       "bwpd                  float64\n",
       "CST                  category\n",
       "Lactobacillus         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata[\"project\"] = mydata[\"project\"].astype('category')\n",
    "mydata[\"CST\"] = mydata[\"CST\"].astype('category')\n",
    "mydata['was_preterm'] = mydata['was_preterm'].astype('int8')\n",
    "mydata['was_early_preterm'] = mydata['was_early_preterm'].astype('int8')\n",
    "mydata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd99cc9",
   "metadata": {},
   "source": [
    "## Subsetting dataset for different outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b3d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2077, 11)\n",
      "(1765, 11)\n",
      "category\n"
     ]
    }
   ],
   "source": [
    "# only keep the rows with collect_wk < 32 for Preterm task\n",
    "mydata_preterm = mydata.loc[mydata['collect_wk']<=32,]\n",
    "print(mydata_preterm.shape)\n",
    "# only keep the rows with collect_wk < 28 for Early preterm task\n",
    "mydata_epreterm = mydata.loc[mydata['collect_wk']<=28,]\n",
    "print(mydata_epreterm.shape)\n",
    "print(mydata_preterm['CST'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa51c19",
   "metadata": {},
   "source": [
    "## Define functions for pytorch input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a967a",
   "metadata": {},
   "source": [
    "### feature transforming function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8510614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform(data, var_name, id_list, out_var = \"was_preterm\"):\n",
    "    \n",
    "    ##check argument validity\n",
    "    if out_var != \"was_preterm\" and out_var != \"was_early_preterm\":\n",
    "        raise ValueError(\"out_var must be was_preterm or was_early_preterm\")\n",
    "    if var_name not in list(data.columns):\n",
    "        raise ValueError(\"var_name must be in column names of data\")\n",
    "        \n",
    "    ##get data type\n",
    "    var_type = data[var_name].dtypes\n",
    "    \n",
    "    if var_type == \"category\":\n",
    "        data[var_name] = data[var_name].cat.codes + 1\n",
    "    \n",
    "    ##get pivot table of features\n",
    "    temp_data = data.pivot_table(index = ['participant_id'], columns = 'collect_wk', values = var_name).sort_index(axis = 0)\n",
    "    temp_data = temp_data.sort_index(axis=1)\n",
    "    \n",
    "    ##if categorical impute with mode of each outcome group, continuous with mean\n",
    "    if var_type == \"category\":\n",
    "        temp_data = temp_data.groupby(data.groupby('participant_id').first().sort_index(axis = 0)[out_var]). \\\n",
    "        transform(lambda x: x.fillna(x.agg(pd.Series.mode).iloc[0,])). \\\n",
    "        apply(lambda x: x.fillna(x.mode().iloc[0,]), axis = 0)\n",
    "    else:\n",
    "        temp_data = temp_data.groupby(data.groupby('participant_id').first().sort_index(axis = 0)[out_var]). \\\n",
    "        transform(lambda x: x.fillna(x.mean())). \\\n",
    "        apply(lambda x: x.fillna(x.mode().iloc[0,]), axis = 0)\n",
    "    \n",
    "    return temp_data.loc[id_list]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180b7e6",
   "metadata": {},
   "source": [
    "### outcome transforming function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1226eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_transform(data, id_list, multi_outcome = True, out_var = \"was_preterm\",label_smooth = True):\n",
    "    outcome_data = data.groupby('participant_id').first().sort_index(axis = 0)[out_var]\n",
    "    max_week_per = data.groupby('participant_id')['collect_wk'].max().sort_index()\n",
    "    max_week = max_week_per.max()\n",
    "    id_all = list(max_week_per.index)\n",
    "    \n",
    "    \n",
    "    if multi_outcome:\n",
    "        if label_smooth:\n",
    "            label_list = [np.concatenate((np.linspace(0.5,0,max_week_per[id]),np.repeat(0,max_week-max_week_per[id]))) \\\n",
    "                          if outcome_data[id] == 0 else \\\n",
    "                         np.concatenate((np.linspace(0.5,1,max_week_per[id]),np.repeat(1,max_week-max_week_per[id]))) \\\n",
    "                         for id in id_all]\n",
    "            temp_y = pd.DataFrame(label_list,columns = np.arange(1,max_week+1,1), index = max_week_per.index)\n",
    "            return temp_y.loc[id_list]\n",
    "        else:\n",
    "            temp_y = data.pivot_table(index=['participant_id'], columns='collect_wk', values= out_var).sort_index(axis = 0)\n",
    "            # sort by collect_wk\n",
    "            temp_y = temp_y.sort_index(axis=1)\n",
    "            temp_y = temp_y.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "            return temp_y.loc[id_list]\n",
    "    else:\n",
    "        return data.groupby('participant_id').first().sort_index(axis = 0)[out_var][id_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055e98a",
   "metadata": {},
   "source": [
    "### ternsor generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6585b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_generator(data, id_list, features, out_var = \"was_preterm\", label_smooth = True, multi_outcome = True):\n",
    "    X_matrix = [feature_transform(data, var_name,id_list, out_var).to_numpy() for var_name in features]\n",
    "    y_matrix = outcome_transform(data, id_list,multi_outcome,out_var,label_smooth).to_numpy()\n",
    "    \n",
    "    input_X = torch.from_numpy(np.dstack(X_matrix).astype('float32'))\n",
    "    \n",
    "    if multi_outcome:\n",
    "        input_y = torch.from_numpy(np.dstack((y_matrix,1-y_matrix)).astype('float32'))\n",
    "    else:\n",
    "        input_y = torch.from_numpy(np.vstack((y_matrix,1-y_matrix)).T.astype('float32'))\n",
    "    \n",
    "    return input_X, input_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f7dd8",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502fef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers = 1,drop_prob=0.2):\n",
    "        \n",
    "        \"\"\"\n",
    "            parameters:\n",
    "                input_dim: dimensions of input data (# features)\n",
    "                hidden_dim: dimensions of hidden layer\n",
    "                output_dim: dimensions of output layer (should be two in our analysis)\n",
    "                n_layers: number of layers for GRU structure, default is 1, 2 means stacked GRU\n",
    "                drop_prob: dropout probability\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #inherit from super class\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        #define parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #define layers\n",
    "        \n",
    "        ##GRU layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ##fully connected layer(use one linear layer first, later can customize)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        #pass out to fully connected layer\n",
    "        out = self.fc(out.reshape(-1,out.shape[-1]))\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros((self.n_layers,batch_size, self.hidden_dim), device = device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4b5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers = 1,drop_prob=0.2):\n",
    "        \n",
    "        \"\"\"\n",
    "            parameters:\n",
    "                input_dim: dimensions of input data (# features)\n",
    "                hidden_dim: dimensions of hidden layer\n",
    "                output_dim: dimensions of output layer (should be two in our analysis)\n",
    "                n_layers: number of layers for GRU structure, default is 1, 2 means stacked GRU\n",
    "                drop_prob: dropout probability\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #inherit from super class\n",
    "        super(GRUModel, self).__init__()\n",
    "        \n",
    "        #define parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #define layers\n",
    "        \n",
    "        ##GRU layers\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ##fully connected layer(use one linear layer first, later can customize)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        #pass out to fully connected layer\n",
    "        out = self.fc(out.reshape(-1,out.shape[-1]))\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros((self.n_layers,batch_size, self.hidden_dim), device = device)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bb8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "        parameters:\n",
    "            input_dim: dimensions of input data (# features)\n",
    "            hidden_dim: dimensions of hidden layer\n",
    "            output_dim: dimensions of output layer (should be two in our analysis)\n",
    "            n_layers: number of layers for GRU structure, default is 1, 2 means stacked GRU\n",
    "            drop_prob: dropout probability\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers = 1, drop_prob=0.2):\n",
    "        \n",
    "        #inherit from super class\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        #define parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #define layers\n",
    "        \n",
    "        ##LSTM layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ##fully connected layer(use one linear layer first, later can customize)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        #Initializing hidden state\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        #pass out to fully connected layer\n",
    "        out = self.fc(out.reshape(-1,out.shape[-1]))\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros((self.n_layers,batch_size, self.hidden_dim),device = device),\n",
    "                torch.zeros((self.n_layers,batch_size, self.hidden_dim),device = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80906d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc61a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train function\n",
    "\n",
    "def dataloader(X, y, batch_size = 100):\n",
    "    data = TensorDataset(X, y)\n",
    "    \n",
    "    if batch_size > X.shape[0]:\n",
    "        batch_size = X.shape[0]\n",
    "    \n",
    "    loader = DataLoader(data, shuffle = True, batch_size = batch_size, drop_last = True)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, validation_loader, learn_rate, \\\n",
    "                hidden_dim, n_layers, drop_prob, \\\n",
    "                device = device, EPOCHS = 100, output_dim = 2, method = \"RNN\"):\n",
    "    \n",
    "    #input_dim\n",
    "    \n",
    "    input_dim = next(iter(train_loader))[0].shape[2]\n",
    "    \n",
    "    #instantiating the models\n",
    "    if method == \"RNN\":\n",
    "        model = RNNModel(input_dim, hidden_dim, output_dim,n_layers = n_layers,drop_prob=drop_prob)\n",
    "    if method == \"GRU\":\n",
    "        model = GRUModel(input_dim, hidden_dim, output_dim,n_layers = n_layers,drop_prob=drop_prob)\n",
    "    elif method == \"LSTM\":\n",
    "        model = LSTMModel(input_dim, hidden_dim, output_dim,n_layers = n_layers,drop_prob=drop_prob)\n",
    "        \n",
    "    model.to(device)\n",
    "    \n",
    "    # loss criterion and optimizer\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learn_rate)\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    # train model\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print('Starting training of {} model'.format(method))\n",
    "    \n",
    "    #Start training loop\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        \n",
    "        batch_train_losses = []\n",
    "        batch_val_losses = []\n",
    "        \n",
    "        for x, label in train_loader:\n",
    "            \n",
    "            x, label = x.to(device), label.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            label = label.reshape(-1,label.shape[-1])\n",
    "        \n",
    "            predictions = model(x)[0]\n",
    "        \n",
    "            predictions = predictions.to(device)\n",
    "        \n",
    "            loss = criterion(predictions, label)\n",
    "            \n",
    "            batch_train_losses.append(loss.detach().numpy())\n",
    "            # backpropagation\n",
    "            loss.backward() \n",
    "            # Updates the weights accordingly\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss.append(np.mean(batch_train_losses))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for x, label in validation_loader:\n",
    "                \n",
    "                x, label = x.to(device), label.to(device)\n",
    "                \n",
    "                label = label.reshape(-1,label.shape[-1])\n",
    "                \n",
    "                out = model(x)[0]\n",
    "                \n",
    "                out = out.to(device)\n",
    "                \n",
    "                loss_val = criterion(out, label)\n",
    "                \n",
    "                batch_val_losses.append(loss_val.numpy())\n",
    "        \n",
    "        val_loss.append(np.mean(batch_val_losses))\n",
    "        \n",
    "        if epoch%10 == 0:\n",
    "            print('Epoch: {}/{}.............'.format(epoch, EPOCHS), end=' ')\n",
    "            print(\"Train Loss: {:.4f}\".format(train_loss[epoch-1]))\n",
    "            print(\"Validation Loss: {:.4f}\".format(val_loss[epoch-1]))\n",
    "            \n",
    "        ##stopping rule?\n",
    "        \n",
    "    return model,train_loss,val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5df6a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metrics(model, data, id_list, features, out_var = \"was_preterm\", multi_outcome = True):\n",
    "    \n",
    "    test_X = tensor_generator(data, id_list,features, out_var, label_smooth = False, multi_outcome = multi_outcome)[0]\n",
    "    \n",
    "    test_y = outcome_transform(data ,id_list, multi_outcome = False, out_var = \"was_preterm\",label_smooth = False).to_numpy()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    if multi_outcome:\n",
    "        out = model(test_X)[0]\n",
    "        predicted_props = nn.functional.softmax(out.reshape(test_X.shape[0],test_X.shape[1],2),dim = 2)[:,-1,0].detach().numpy()\n",
    "        predicted_labels = 1*(predicted_props >0.5)\n",
    "    else:\n",
    "        out = model(test_X)\n",
    "        predicted_props = nn.functional.softmax(out, dim = 1)[:,0].detach().numpy()\n",
    "        predicted_labels = 1*(predicted_props >0.5)\n",
    "    \n",
    "    \n",
    "    result_tab = pd.DataFrame(data = [predicted_props,predicted_labels,test_y],\n",
    "                             columns = id_list,\n",
    "                             index= ['predicted_prop','predicted_y','y']).T\n",
    "    \n",
    "    acc = metrics.accuracy_score(test_y, predicted_labels, normalize=False) / float(test_y.size)\n",
    "    confusion = metrics.confusion_matrix(test_y, predicted_labels)\n",
    "    \n",
    "\n",
    "    TP = confusion[1, 1]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    \n",
    "    specificity = TN / (TN + FP)\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    precision = TP/(TP + FP)\n",
    "    \n",
    "    auc = metrics.roc_auc_score(test_y, predicted_props)\n",
    "    \n",
    "    \n",
    "    return acc,sensitivity,specificity,auc,precision,result_tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54830e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_cross_val(features,train_ratio = 0.7, learn_rate = 0.01,hidden_dim = 18, n_layers = 1, device = device, \\\n",
    "                      batch_size = 1000,drop_prob = 0.2, EPOCHS = 1000, output_dim = 2, method = \"RNN\", \\\n",
    "                      out_var = \"was_preterm\",label_smooth= True, multi_outcome= True):\n",
    "    \n",
    "    ##check argument validity\n",
    "    if out_var != \"was_preterm\" and out_var != \"was_early_preterm\":\n",
    "        raise ValueError(\"outcome must be was_preterm or was_early_preterm\")\n",
    "    \n",
    "    ##obtain data by goal\n",
    "    if out_var == \"was_preterm\":\n",
    "        data = mydata_preterm\n",
    "    else:\n",
    "        data = mydata_epreterm\n",
    "        \n",
    "    project_list = data['project'].unique()\n",
    "     ##accuracy, sen, spe, auc score\n",
    "    train_loss = {}\n",
    "    val_loss = {}\n",
    "    acc = {}\n",
    "    sen = {}\n",
    "    spe = {}\n",
    "    auc = {}\n",
    "    precision = {}\n",
    "    result_tabs = {}\n",
    "    \n",
    "    \n",
    "    for project in project_list:\n",
    "        train_list = data[data['project']!= project].groupby('participant_id').first().sort_index(axis = 0)\n",
    "        train_id = list(train_list.groupby(out_var).sample(frac = train_ratio, random_state = 100).sort_index(axis = 0).index)\n",
    "        val_id = np.setdiff1d(list(train_list.index),train_id)\n",
    "        test_id = list(data[data['project']== project].groupby('participant_id').first().sort_index(axis = 0).index)\n",
    "        train_x, train_y = tensor_generator(data,train_id,features = features, \\\n",
    "                                            out_var= \"was_preterm\", \\\n",
    "                                            label_smooth= label_smooth, multi_outcome= multi_outcome)\n",
    "        val_x, val_y = tensor_generator(data,val_id,features = features, \\\n",
    "                                            out_var= \"was_preterm\", \\\n",
    "                                            label_smooth= label_smooth, multi_outcome= multi_outcome)\n",
    "        \n",
    "        train_dataloader = dataloader(train_x, train_y,batch_size =  batch_size)\n",
    "        val_dataloader = dataloader(val_x, val_y,batch_size =  batch_size)\n",
    "        model, train_loss[project],val_loss[project] = train_epoch(train_dataloader,val_dataloader, learn_rate = learn_rate, \\\n",
    "                hidden_dim = hidden_dim, n_layers = n_layers, drop_prob = drop_prob, \\\n",
    "                device = device, EPOCHS = EPOCHS, output_dim = output_dim, method = method)\n",
    "        \n",
    "        acc[project],sen[project],spe[project],auc[project],precision[project],result_tabs[project] = \\\n",
    "        test_metrics(model, data,test_id,features, out_var = out_var, multi_outcome = multi_outcome)\n",
    "    \n",
    "    return train_loss,val_loss, result_tabs ,pd.DataFrame([acc,sen,spe,precision,auc], \\\n",
    "                                                 index = ['accuracy','sensitivity','specificity','precision','AUC'])\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db06b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1654\n",
      "Validation Loss: 0.1660\n",
      "Epoch: 20/120............. Train Loss: 0.1341\n",
      "Validation Loss: 0.1348\n",
      "Epoch: 30/120............. Train Loss: 0.1264\n",
      "Validation Loss: 0.1291\n",
      "Epoch: 40/120............. Train Loss: 0.1245\n",
      "Validation Loss: 0.1273\n",
      "Epoch: 50/120............. Train Loss: 0.1236\n",
      "Validation Loss: 0.1264\n",
      "Epoch: 60/120............. Train Loss: 0.1222\n",
      "Validation Loss: 0.1248\n",
      "Epoch: 70/120............. Train Loss: 0.1197\n",
      "Validation Loss: 0.1223\n",
      "Epoch: 80/120............. Train Loss: 0.1130\n",
      "Validation Loss: 0.1147\n",
      "Epoch: 90/120............. Train Loss: 0.0905\n",
      "Validation Loss: 0.0902\n",
      "Epoch: 100/120............. Train Loss: 0.0444\n",
      "Validation Loss: 0.0375\n",
      "Epoch: 110/120............. Train Loss: 0.0248\n",
      "Validation Loss: 0.0214\n",
      "Epoch: 120/120............. Train Loss: 0.0142\n",
      "Validation Loss: 0.0113\n",
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1271\n",
      "Validation Loss: 0.1214\n",
      "Epoch: 20/120............. Train Loss: 0.1232\n",
      "Validation Loss: 0.1190\n",
      "Epoch: 30/120............. Train Loss: 0.1209\n",
      "Validation Loss: 0.1166\n",
      "Epoch: 40/120............. Train Loss: 0.1174\n",
      "Validation Loss: 0.1128\n",
      "Epoch: 50/120............. Train Loss: 0.1082\n",
      "Validation Loss: 0.1029\n",
      "Epoch: 60/120............. Train Loss: 0.0694\n",
      "Validation Loss: 0.0994\n",
      "Epoch: 70/120............. Train Loss: 0.0341\n",
      "Validation Loss: 0.0187\n",
      "Epoch: 80/120............. Train Loss: 0.0134\n",
      "Validation Loss: 0.0132\n",
      "Epoch: 90/120............. Train Loss: 0.0072\n",
      "Validation Loss: 0.0097\n",
      "Epoch: 100/120............. Train Loss: 0.0063\n",
      "Validation Loss: 0.0079\n",
      "Epoch: 110/120............. Train Loss: 0.0057\n",
      "Validation Loss: 0.0070\n",
      "Epoch: 120/120............. Train Loss: 0.0055\n",
      "Validation Loss: 0.0067\n",
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1495\n",
      "Validation Loss: 0.1521\n",
      "Epoch: 20/120............. Train Loss: 0.1278\n",
      "Validation Loss: 0.1303\n",
      "Epoch: 30/120............. Train Loss: 0.1231\n",
      "Validation Loss: 0.1262\n",
      "Epoch: 40/120............. Train Loss: 0.1200\n",
      "Validation Loss: 0.1226\n",
      "Epoch: 50/120............. Train Loss: 0.1172\n",
      "Validation Loss: 0.1203\n",
      "Epoch: 60/120............. Train Loss: 0.1134\n",
      "Validation Loss: 0.1161\n",
      "Epoch: 70/120............. Train Loss: 0.1080\n",
      "Validation Loss: 0.1100\n",
      "Epoch: 80/120............. Train Loss: 0.0959\n",
      "Validation Loss: 0.0955\n",
      "Epoch: 90/120............. Train Loss: 0.0844\n",
      "Validation Loss: 0.0721\n",
      "Epoch: 100/120............. Train Loss: 0.0480\n",
      "Validation Loss: 0.0387\n",
      "Epoch: 110/120............. Train Loss: 0.0152\n",
      "Validation Loss: 0.0191\n",
      "Epoch: 120/120............. Train Loss: 0.0082\n",
      "Validation Loss: 0.0120\n",
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1220\n",
      "Validation Loss: 0.1272\n",
      "Epoch: 20/120............. Train Loss: 0.1171\n",
      "Validation Loss: 0.1161\n",
      "Epoch: 30/120............. Train Loss: 0.1047\n",
      "Validation Loss: 0.1039\n",
      "Epoch: 40/120............. Train Loss: 0.0873\n",
      "Validation Loss: 0.0756\n",
      "Epoch: 50/120............. Train Loss: 0.0437\n",
      "Validation Loss: 0.0537\n",
      "Epoch: 60/120............. Train Loss: 0.0191\n",
      "Validation Loss: 0.0245\n",
      "Epoch: 70/120............. Train Loss: 0.0119\n",
      "Validation Loss: 0.0133\n",
      "Epoch: 80/120............. Train Loss: 0.0078\n",
      "Validation Loss: 0.0099\n",
      "Epoch: 90/120............. Train Loss: 0.0068\n",
      "Validation Loss: 0.0091\n",
      "Epoch: 100/120............. Train Loss: 0.0059\n",
      "Validation Loss: 0.0087\n",
      "Epoch: 110/120............. Train Loss: 0.0055\n",
      "Validation Loss: 0.0084\n",
      "Epoch: 120/120............. Train Loss: 0.0053\n",
      "Validation Loss: 0.0082\n",
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1338\n",
      "Validation Loss: 0.1319\n",
      "Epoch: 20/120............. Train Loss: 0.1238\n",
      "Validation Loss: 0.1252\n",
      "Epoch: 30/120............. Train Loss: 0.1181\n",
      "Validation Loss: 0.1200\n",
      "Epoch: 40/120............. Train Loss: 0.1105\n",
      "Validation Loss: 0.1112\n",
      "Epoch: 50/120............. Train Loss: 0.0835\n",
      "Validation Loss: 0.0776\n",
      "Epoch: 60/120............. Train Loss: 0.0254\n",
      "Validation Loss: 0.0407\n",
      "Epoch: 70/120............. Train Loss: 0.0175\n",
      "Validation Loss: 0.0139\n",
      "Epoch: 80/120............. Train Loss: 0.0090\n",
      "Validation Loss: 0.0091\n",
      "Epoch: 90/120............. Train Loss: 0.0056\n",
      "Validation Loss: 0.0074\n",
      "Epoch: 100/120............. Train Loss: 0.0048\n",
      "Validation Loss: 0.0068\n",
      "Epoch: 110/120............. Train Loss: 0.0045\n",
      "Validation Loss: 0.0065\n",
      "Epoch: 120/120............. Train Loss: 0.0043\n",
      "Validation Loss: 0.0064\n",
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1272\n",
      "Validation Loss: 0.1252\n",
      "Epoch: 20/120............. Train Loss: 0.1182\n",
      "Validation Loss: 0.1151\n",
      "Epoch: 30/120............. Train Loss: 0.0833\n",
      "Validation Loss: 0.0796\n",
      "Epoch: 40/120............. Train Loss: 0.0356\n",
      "Validation Loss: 0.0459\n",
      "Epoch: 50/120............. Train Loss: 0.0182\n",
      "Validation Loss: 0.0178\n",
      "Epoch: 60/120............. Train Loss: 0.0098\n",
      "Validation Loss: 0.0101\n",
      "Epoch: 70/120............. Train Loss: 0.0076\n",
      "Validation Loss: 0.0088\n",
      "Epoch: 80/120............. Train Loss: 0.0061\n",
      "Validation Loss: 0.0064\n",
      "Epoch: 90/120............. Train Loss: 0.0056\n",
      "Validation Loss: 0.0058\n",
      "Epoch: 100/120............. Train Loss: 0.0053\n",
      "Validation Loss: 0.0055\n",
      "Epoch: 110/120............. Train Loss: 0.0052\n",
      "Validation Loss: 0.0054\n",
      "Epoch: 120/120............. Train Loss: 0.0050\n",
      "Validation Loss: 0.0054\n",
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1437\n",
      "Validation Loss: 0.1336\n",
      "Epoch: 20/120............. Train Loss: 0.1322\n",
      "Validation Loss: 0.1278\n",
      "Epoch: 30/120............. Train Loss: 0.1291\n",
      "Validation Loss: 0.1254\n",
      "Epoch: 40/120............. Train Loss: 0.1271\n",
      "Validation Loss: 0.1244\n",
      "Epoch: 50/120............. Train Loss: 0.1254\n",
      "Validation Loss: 0.1226\n",
      "Epoch: 60/120............. Train Loss: 0.1237\n",
      "Validation Loss: 0.1210\n",
      "Epoch: 70/120............. Train Loss: 0.1214\n",
      "Validation Loss: 0.1186\n",
      "Epoch: 80/120............. Train Loss: 0.1166\n",
      "Validation Loss: 0.1133\n",
      "Epoch: 90/120............. Train Loss: 0.1007\n",
      "Validation Loss: 0.0953\n",
      "Epoch: 100/120............. Train Loss: 0.0564\n",
      "Validation Loss: 0.0356\n",
      "Epoch: 110/120............. Train Loss: 0.0140\n",
      "Validation Loss: 0.0100\n",
      "Epoch: 120/120............. Train Loss: 0.0109\n",
      "Validation Loss: 0.0099\n",
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1305\n",
      "Validation Loss: 0.1241\n",
      "Epoch: 20/120............. Train Loss: 0.1143\n",
      "Validation Loss: 0.1120\n",
      "Epoch: 30/120............. Train Loss: 0.0970\n",
      "Validation Loss: 0.0933\n",
      "Epoch: 40/120............. Train Loss: 0.0454\n",
      "Validation Loss: 0.0674\n",
      "Epoch: 50/120............. Train Loss: 0.0262\n",
      "Validation Loss: 0.0225\n",
      "Epoch: 60/120............. Train Loss: 0.0101\n",
      "Validation Loss: 0.0185\n",
      "Epoch: 70/120............. Train Loss: 0.0078\n",
      "Validation Loss: 0.0123\n",
      "Epoch: 80/120............. Train Loss: 0.0059\n",
      "Validation Loss: 0.0093\n",
      "Epoch: 90/120............. Train Loss: 0.0056\n",
      "Validation Loss: 0.0084\n",
      "Epoch: 100/120............. Train Loss: 0.0052\n",
      "Validation Loss: 0.0075\n",
      "Epoch: 110/120............. Train Loss: 0.0049\n",
      "Validation Loss: 0.0071\n",
      "Epoch: 120/120............. Train Loss: 0.0048\n",
      "Validation Loss: 0.0069\n",
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1491\n",
      "Validation Loss: 0.1439\n",
      "Epoch: 20/120............. Train Loss: 0.1321\n",
      "Validation Loss: 0.1321\n",
      "Epoch: 30/120............. Train Loss: 0.1259\n",
      "Validation Loss: 0.1257\n",
      "Epoch: 40/120............. Train Loss: 0.1234\n",
      "Validation Loss: 0.1241\n",
      "Epoch: 50/120............. Train Loss: 0.1217\n",
      "Validation Loss: 0.1221\n",
      "Epoch: 60/120............. Train Loss: 0.1194\n",
      "Validation Loss: 0.1201\n",
      "Epoch: 70/120............. Train Loss: 0.1148\n",
      "Validation Loss: 0.1154\n",
      "Epoch: 80/120............. Train Loss: 0.1032\n",
      "Validation Loss: 0.1035\n",
      "Epoch: 90/120............. Train Loss: 0.0512\n",
      "Validation Loss: 0.0943\n",
      "Epoch: 100/120............. Train Loss: 0.0198\n",
      "Validation Loss: 0.0189\n",
      "Epoch: 110/120............. Train Loss: 0.0113\n",
      "Validation Loss: 0.0122\n",
      "Epoch: 120/120............. Train Loss: 0.0092\n",
      "Validation Loss: 0.0099\n",
      "Starting training of GRU model\n",
      "Epoch: 10/120............. Train Loss: 0.1388\n",
      "Validation Loss: 0.1282\n",
      "Epoch: 20/120............. Train Loss: 0.1240\n",
      "Validation Loss: 0.1235\n",
      "Epoch: 30/120............. Train Loss: 0.1219\n",
      "Validation Loss: 0.1212\n",
      "Epoch: 40/120............. Train Loss: 0.1193\n",
      "Validation Loss: 0.1183\n",
      "Epoch: 50/120............. Train Loss: 0.1141\n",
      "Validation Loss: 0.1125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60/120............. Train Loss: 0.1008\n",
      "Validation Loss: 0.0978\n",
      "Epoch: 70/120............. Train Loss: 0.0695\n",
      "Validation Loss: 0.0837\n",
      "Epoch: 80/120............. Train Loss: 0.0375\n",
      "Validation Loss: 0.0296\n",
      "Epoch: 90/120............. Train Loss: 0.0149\n",
      "Validation Loss: 0.0164\n",
      "Epoch: 100/120............. Train Loss: 0.0099\n",
      "Validation Loss: 0.0086\n",
      "Epoch: 110/120............. Train Loss: 0.0072\n",
      "Validation Loss: 0.0073\n",
      "Epoch: 120/120............. Train Loss: 0.0065\n",
      "Validation Loss: 0.0071\n"
     ]
    }
   ],
   "source": [
    "train_loss_GRU,val_loss_GRU,result_tabs_GRU,metrics_GRU = project_cross_val(['shannon','bwpd','CST','Lactobacillus'], \\\n",
    "                                                    train_ratio = 0.7,learn_rate = 0.01,hidden_dim = 10, n_layers = 2, device = device, \\\n",
    "                                                    batch_size = 1000, drop_prob = 0,EPOCHS = 120, output_dim = 2, method = \"GRU\", \\\n",
    "                                                    out_var = \"was_preterm\",label_smooth= True, multi_outcome= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67a295a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               A    B    C         D    E    F         G         H    I    J\n",
      "accuracy     1.0  1.0  1.0  0.971014  1.0  1.0  0.977273  0.979167  1.0  1.0\n",
      "sensitivity  1.0  1.0  1.0  0.963636  1.0  1.0  0.938776  1.000000  1.0  1.0\n",
      "specificity  1.0  1.0  1.0  1.000000  1.0  1.0  1.000000  0.964912  1.0  1.0\n",
      "precision    1.0  1.0  1.0  1.000000  1.0  1.0  1.000000  0.951220  1.0  1.0\n",
      "AUC          1.0  1.0  1.0  1.000000  1.0  1.0  0.983280  1.000000  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "print(metrics_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f05a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_LSTM_t2,val_loss_LSTM_t2,result_tabs_LSTM_t2,metrics_LSTM_t2 = project_cross_val(['shannon','bwpd','CST','Lactobacillus'], \\\n",
    "#                                                     train_ratio = 0.7,learn_rate = 0.01,hidden_dim = 15, n_layers = 2, device = device, \\\n",
    "#                                                     batch_size = 1000, drop_prob = 0,EPOCHS = 100, output_dim = 2, method = \"LSTM\", \\\n",
    "#                                                     out_var = \"was_early_preterm\",label_smooth= True, multi_outcome= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7839d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics_LSTM_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59581560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dream_dl] *",
   "language": "python",
   "name": "conda-env-dream_dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
