{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1124b28",
   "metadata": {},
   "source": [
    "# Preterm Birth Prediction Microbiome Model Framework (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a2a71",
   "metadata": {},
   "source": [
    "Challenge website:\n",
    "https://www.synapse.org/#!Synapse:syn26133770/wiki/618018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabb89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from collections import Counter,defaultdict, OrderedDict\n",
    "from itertools import islice\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dataset_splitID(meta_data, prop, myseed):\n",
    "    \n",
    "    subjects = list(np.unique(meta_data[\"participant_id\"]))\n",
    "    numsubjects = len(subjects)\n",
    "    \n",
    "    if myseed != None:\n",
    "        random.seed(myseed)\n",
    "\n",
    "    subjects_shuffle = random.sample(subjects, numsubjects)\n",
    "    \n",
    "    train_subjects = subjects_shuffle[0:(int(numsubjects*prop[0])+1)] \n",
    "    valid_subjects = subjects_shuffle[(int(numsubjects*prop[0])+2):(int(numsubjects*(prop[0]+prop[1]))+1)]\n",
    "    test_subjects = subjects_shuffle[(int(numsubjects*(prop[0]+prop[1]))+2):numsubjects]\n",
    "    \n",
    "    splitID_train = meta_data['participant_id'].isin(train_subjects)\n",
    "    splitID_valid = meta_data['participant_id'].isin(valid_subjects)\n",
    "    splitID_test = meta_data['participant_id'].isin(test_subjects)\n",
    "    \n",
    "    return splitID_train, splitID_valid, splitID_test\n",
    "\n",
    "\n",
    "# Possible, but not used here\n",
    "def dataset_pjt_splitID(meta_data, prop, myseed):\n",
    "    \n",
    "    projects = meta_data['project']\n",
    "\n",
    "    splitID_train = []\n",
    "    splitID_valid = []\n",
    "    splitID_test  = []\n",
    "    \n",
    "    for pjt in np.unique(projects):\n",
    "        \n",
    "        submeta = meta_data[projects == pjt]\n",
    "        subsubjects = list(np.unique(submeta[\"participant_id\"]))\n",
    "        numsub = len(subsubjects)\n",
    "        \n",
    "        subsubjects_shuffle = random.sample(subsubjects, numsub)\n",
    "        \n",
    "        train_subsubjects = subsubjects_shuffle[0:(int(numsub*prop[0])+1)] \n",
    "        valid_subsubjects = subsubjects_shuffle[(int(numsub*prop[0])+2):(int(numsub*(prop[0]+prop[1]))+1)]\n",
    "        test_subsubjects  = subsubjects_shuffle[(int(numsub*(prop[0]+prop[1]))+2):numsub]\n",
    "        \n",
    "        splitID_train.extend(submeta['participant_id'].isin(train_subsubjects))\n",
    "        splitID_valid.extend(submeta['participant_id'].isin(valid_subsubjects))\n",
    "        splitID_test.extend(submeta['participant_id'].isin(test_subsubjects))\n",
    "        \n",
    "    return splitID_train, splitID_valid, splitID_test\n",
    "\n",
    "\n",
    "def Data_Reshaper_Input(data, seq_length):\n",
    "    \n",
    "    numsubjects = len(np.unique(data['participant_id']))\n",
    "    myvary = list(data.columns.values)[2:data.shape[1]]\n",
    "    num_covariates = len(myvary)\n",
    "    \n",
    "    myinput = np.zeros((numsubjects, seq_length, num_covariates), dtype=np.float32)\n",
    "    for i in range(num_covariates):\n",
    "        data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=myvary[i])\n",
    "        data_wide = data_wide.sort_index(axis=1)\n",
    "        data_wide = data_wide.fillna(0)\n",
    "        tmpindex = data_wide._get_numeric_data().columns.values - 1\n",
    "        tmpindex = tmpindex.astype(int)\n",
    "        # time varying variables need to impute all and no records are denoted as 0\n",
    "        for j in range(numsubjects):\n",
    "                myinput[j,tmpindex,i] = data_wide.iloc[[j]]\n",
    "    return myinput\n",
    "\n",
    "\n",
    "\n",
    "def Data_Reshaper_Output_ManytoMany_0(data, seq_length, classlabel):\n",
    "\n",
    "    num_samples = len(np.unique(data['participant_id']))\n",
    "    \n",
    "    data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=classlabel)\n",
    "    data_wide = data_wide.sort_index(axis=1)\n",
    "    \n",
    "    myoutput = np.zeros((num_samples, seq_length, 2), dtype=np.float32)\n",
    "    for i in range(num_samples):\n",
    "        tmp = data_wide.iloc[i,:]\n",
    "        \n",
    "        if np.nanmax(tmp) == 1:\n",
    "            # label linear smoonthing from 0.5 to 1\n",
    "            # fill all position 1 to have final labels equal to 1\n",
    "            myoutput[i,:,0].fill(1)\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=1, num=seq_length)\n",
    "        else:\n",
    "            # label linear smoonthing from 0.5 to 0\n",
    "            # fill all position 0 to have final labels equal to 0 \n",
    "            #     but array alrady initialize as 0\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=0, num=seq_length)\n",
    "            \n",
    "        myoutput[i,:,1] = 1 - myoutput[i,:,0]\n",
    "    return myoutput\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, device, myinput, myoutput, finalperiod, cutoff=0.5):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # predicted labels\n",
    "    myinput  = torch.from_numpy(myinput).float().to(device)\n",
    "    myoutput_nn, hidden = model(myinput, device)\n",
    "    myoutput_nn = myoutput_nn.reshape((myoutput.shape))\n",
    "    output_prob = nn.functional.softmax(myoutput_nn, dim=2)\n",
    "    mypredprob = output_prob[:,finalperiod-1,:].cpu().detach().numpy()\n",
    "    mypred = 1*(mypredprob[:,0] > cutoff)\n",
    "    # observed labels\n",
    "    myobs  = myoutput[:,finalperiod-1,0]\n",
    "    \n",
    "    return myobs, mypred, mypredprob\n",
    "\n",
    "\n",
    "\n",
    "def metadata_loader(meta_dir, alpha_dir, cst_dir, task, finalperiod):\n",
    "    \n",
    "    meta_data = pd.DataFrame(pd.read_csv(meta_dir, delimiter=','))\n",
    "    meta_data.replace('Unknown', np.nan, inplace=True)\n",
    "    meta_data = meta_data[['participant_id', 'project', 'delivery_wk', 'collect_wk', 'age', 'race']]\n",
    "    \n",
    "    alpha_data = pd.DataFrame(pd.read_csv(alpha_dir, delimiter=','))\n",
    "    cst_data = pd.DataFrame(pd.read_csv(cst_dir, delimiter=','))\n",
    "    \n",
    "    meta_data = pd.concat([meta_data, alpha_data['shannon'], alpha_data['inv_simpson'], alpha_data['rooted_pd'], cst_data['CST']], axis=1)\n",
    "\n",
    "    for i in range(1,meta_data.shape[1]):\n",
    "        if meta_data.iloc[:,i].dtypes == object:\n",
    "            meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('category').cat.codes + 1\n",
    "            meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('float64')\n",
    "            \n",
    "    # create new variable 'collect_period'\n",
    "    meta_data['collect_period'] = 1\n",
    "    meta_data.loc[(meta_data['collect_wk']>=8)  & (meta_data['collect_wk']<=14),'collect_period'] = 2\n",
    "    meta_data.loc[(meta_data['collect_wk']>=15) & (meta_data['collect_wk']<=21),'collect_period'] = 3\n",
    "    meta_data.loc[(meta_data['collect_wk']>=22) & (meta_data['collect_wk']<=28),'collect_period'] = 4\n",
    "    meta_data.loc[(meta_data['collect_wk']>=29) & (meta_data['collect_wk']<=32),'collect_period'] = 5\n",
    "    meta_data.loc[(meta_data['collect_wk']>=33), 'collect_period']                                = 6\n",
    "    \n",
    "    # print(meta_data['collect_period'].value_counts())\n",
    "    \n",
    "    # create task class label\n",
    "    if task == \"was_preterm\":\n",
    "        meta_data[task] = 1*(meta_data['delivery_wk'] < 37)\n",
    "    elif task == \"was_early_preterm\":\n",
    "        meta_data[task] = 1*(meta_data['delivery_wk'] < 32)\n",
    "        \n",
    "    # Filtered out observations with \"collect_wk<=32\" OR \"collect_period<=5\" \n",
    "    # Filtered out observations with \"collect_wk<=28\" OR \"collect_period<=4\" \n",
    "    meta_data = meta_data[meta_data['collect_period']<=finalperiod]\n",
    "    # Average within each collection period\n",
    "    meta_data = meta_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "\n",
    "    return meta_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac44e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputLoader(data_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod):\n",
    "    \n",
    "    participant_id = meta_data['participant_id']\n",
    "    collect_period = meta_data['collect_period']\n",
    "   \n",
    "    Input_data = pd.DataFrame(pd.read_csv(data_dir, delimiter=','))\n",
    "    Input_data = pd.concat([participant_id, collect_period, Input_data], axis=1)\n",
    "    \n",
    "    #---- Filter 1 on columns ----#\n",
    "    # columns/taxons that were observed in fewer than 10 samples\n",
    "    Input_reads = Input_data.iloc[:,3:Input_data.shape[1]]\n",
    "    pt = np.where((1*(Input_reads != 0)).sum(axis = 0)  > 10)[0]+3\n",
    "    pt = np.concatenate(([0, 1, 2], pt), axis=None)\n",
    "    Input_data = Input_data.iloc[:,pt]\n",
    "    \n",
    "    #---- Filter 2 on rows    ----#\n",
    "    # Filtered out observations with \"collect_wk<=32\" OR \"collect_period<=6\" \n",
    "    # Filtered out observations with \"collect_wk<=28\" OR \"collect_period<=5\" \n",
    "    Input_data = Input_data[Input_data['collect_period']<=finalperiod]\n",
    "    \n",
    "    # Average within each collection period\n",
    "    Input_data = Input_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "    \n",
    "    Input_data_train = Input_data[trainID]\n",
    "    Input_data_valid = Input_data[validID]\n",
    "    Input_data_test  = Input_data[testID]\n",
    "    \n",
    "    print(\"## Input: train/valid/test (before reshape)\")\n",
    "    print(Input_data_train.shape)\n",
    "    print(Input_data_valid.shape)\n",
    "    print(Input_data_test.shape)\n",
    "    \n",
    "    #---- Input features reshaper ----#\n",
    "    mytrain_input = Data_Reshaper_Input(data=Input_data_train, seq_length=finalperiod)\n",
    "    myvalid_input = Data_Reshaper_Input(data=Input_data_valid, seq_length=finalperiod)\n",
    "    mytest_input  = Data_Reshaper_Input(data=Input_data_test, seq_length=finalperiod)\n",
    "    \n",
    "    print(\"## Input: train/valid/test (after reshape)\")\n",
    "    print(mytrain_input.shape)\n",
    "    print(myvalid_input.shape)\n",
    "    print(mytest_input.shape)\n",
    "    \n",
    "    return mytrain_input, myvalid_input, mytest_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190af89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputLoader(meta_data, trainID, validID, testID, task, finalperiod):\n",
    "    \n",
    "    meta_data_train = meta_data[trainID]\n",
    "    meta_data_valid = meta_data[validID]\n",
    "    meta_data_test  = meta_data[testID]\n",
    "    \n",
    "    print(\"################ Output: train/valid/test (before reshape)\")\n",
    "    print(meta_data_train.shape)\n",
    "    print(meta_data_valid.shape)\n",
    "    print(meta_data_test.shape)\n",
    "    \n",
    "    #---- Output label reshaper ----#\n",
    "    mytrain_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_train, seq_length=finalperiod, classlabel=task)\n",
    "    myvalid_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_valid, seq_length=finalperiod, classlabel=task)\n",
    "    mytest_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_test, seq_length=finalperiod, classlabel=task)\n",
    "    \n",
    "    print(\"################ Output: train/valid/test (after reshape)\")\n",
    "    print(mytrain_output.shape)\n",
    "    print(myvalid_output.shape)\n",
    "    print(mytest_output.shape)\n",
    "    \n",
    "    return mytrain_output, myvalid_output, mytest_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3947df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputLoaderMtd(meta_data, trainID, validID, testID, task, finalperiod):\n",
    "    \n",
    "    meta_data_train = meta_data[trainID]\n",
    "    meta_data_valid = meta_data[validID]\n",
    "    meta_data_test  = meta_data[testID]\n",
    "    \n",
    "    #---- Input features reshaper ----#\n",
    "    mytrain_input_mtd = meta_data_train.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    myvalid_input_mtd = meta_data_valid.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    mytest_input_mtd  = meta_data_test.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    \n",
    "    # scale the input features in this data set\n",
    "    columns = ['collect_wk', 'age', 'race', 'shannon', 'inv_simpson', 'rooted_pd', 'CST']\n",
    "    for col in columns:\n",
    "        mytrain_input_mtd[col] = MinMaxScaler().fit_transform(np.array(mytrain_input_mtd[col]).reshape(-1,1))\n",
    "        myvalid_input_mtd[col] = MinMaxScaler().fit_transform(np.array(myvalid_input_mtd[col]).reshape(-1,1))\n",
    "        mytest_input_mtd[col]  = MinMaxScaler().fit_transform(np.array(mytest_input_mtd[col]).reshape(-1,1))\n",
    "    \n",
    "    print(\"## Input: train/valid/test (before reshape)\")\n",
    "    print(mytrain_input_mtd.shape)\n",
    "    print(myvalid_input_mtd.shape)\n",
    "    print(mytest_input_mtd.shape)\n",
    "    \n",
    "    mytrain_input_mtd = Data_Reshaper_Input(data=mytrain_input_mtd, seq_length=finalperiod)\n",
    "    myvalid_input_mtd = Data_Reshaper_Input(data=myvalid_input_mtd, seq_length=finalperiod)\n",
    "    mytest_input_mtd  = Data_Reshaper_Input(data=mytest_input_mtd,  seq_length=finalperiod) \n",
    "    \n",
    "    print(\"## Input: train/valid/test (after reshape)\")\n",
    "    print(mytrain_input_mtd.shape)\n",
    "    print(myvalid_input_mtd.shape)\n",
    "    print(mytest_input_mtd.shape)\n",
    "    \n",
    "    return mytrain_input_mtd, myvalid_input_mtd, mytest_input_mtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c901b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMtrain(model, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, finalperiod, patience, earlystop='loss', verbose=True):\n",
    "    \n",
    "    # training and validation set class proportion\n",
    "    trainprior = sum(mytrain_output[:,finalperiod-1,0])/mytrain_output.shape[0]\n",
    "    class1ID_train = mytrain_output[:,finalperiod-1,0] == 1\n",
    "    class2ID_train = mytrain_output[:,finalperiod-1,0] == 0\n",
    "    \n",
    "    validprior = sum(myvalid_output[:,finalperiod-1,0])/myvalid_output.shape[0]\n",
    "    class1ID_valid = myvalid_output[:,finalperiod-1,0] == 1\n",
    "    class2ID_valid = myvalid_output[:,finalperiod-1,0] == 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Track the value of the loss function and model accuracy across epochs\n",
    "    history_train_valid = {'TrainLoss': [], 'TrainAcc': [], 'TrainAUC': [],\n",
    "                           'ValidLoss': [], 'ValidAcc': [], 'ValidAUC': []}\n",
    "    \n",
    "    # Same reshaped Validation set for each epoch    \n",
    "    myvalid_input  = torch.from_numpy(myvalid_input).float().to(device)\n",
    "    myvalid_output = torch.from_numpy(myvalid_output).float().to(device)\n",
    "        \n",
    "    valid_loss_min = np.inf\n",
    "    valid_losses = []\n",
    "    \n",
    "    valid_auc_max = np.NINF\n",
    "    valid_auces = []\n",
    "    \n",
    "    last_valid_loss = 100\n",
    "    last_valid_auc  = 100\n",
    "    \n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        #----  shuffle the training set to avoid the batch(project) effects ----#\n",
    "        shuffleindex = list(range(mytrain_output.shape[0]))\n",
    "        random.shuffle(shuffleindex)\n",
    "        mytrain_output = mytrain_output[shuffleindex]\n",
    "        mytrain_input = mytrain_input[shuffleindex]\n",
    "        \n",
    "        #-------------- Batch-wise training model --------------#\n",
    "        model.train()\n",
    "        # train_loss = 0.0\n",
    "        train_num_correct = 0\n",
    "        train_prob = []\n",
    "        for batch_idx in range(0, mytrain_input.shape[0], batch_size):\n",
    "            \n",
    "            # subset a batch of sequences and class labels\n",
    "            tmpindex = list(range(batch_idx, min(batch_idx+batch_size, mytrain_input.shape[0])))\n",
    "            mytrain_input_batch  = mytrain_input[tmpindex,:]\n",
    "            mytrain_output_batch = mytrain_output[tmpindex,:]\n",
    "            \n",
    "            batchprior = sum(mytrain_output_batch[:,finalperiod-1,0])/mytrain_output_batch.shape[0]\n",
    "            class1ID_batch = mytrain_output_batch[:,finalperiod-1,0] == 1\n",
    "            class2ID_batch = mytrain_output_batch[:,finalperiod-1,0] == 0\n",
    "            \n",
    "            mytrain_input_batch  = torch.from_numpy(mytrain_input_batch).float().to(device)\n",
    "            mytrain_output_batch = torch.from_numpy(mytrain_output_batch).float().to(device)\n",
    "            \n",
    "            # forward pass of RNN model\n",
    "            output, hidden = model(mytrain_input_batch, device)\n",
    "            output = output.reshape((mytrain_output_batch.shape))\n",
    "            output_prob = nn.functional.softmax(output, dim=2)\n",
    "            # weighted MSE\n",
    "            loss = batchprior*criterion(output_prob[class1ID_batch,:,0], mytrain_output_batch[class1ID_batch,:,0]) + (1-batchprior)*criterion(output_prob[class2ID_batch,:,1], mytrain_output_batch[class2ID_batch,:,1])\n",
    "            # loss = criterion(output_prob, mytrain_output_batch)\n",
    "            # Clear existing gradients from previous epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Does backpropagation and calculates gradients\n",
    "            loss.backward()\n",
    "            # Updates the weights accordingly\n",
    "            optimizer.step()\n",
    "            # Number correct prediction on trainning set collection\n",
    "            tmppred = 1*(output_prob[:,finalperiod-1,0] > 0.5)\n",
    "            train_num_correct += sum(1*(tmppred == mytrain_output_batch[:,finalperiod-1,0]))\n",
    "            # Training function loss collection\n",
    "            # train_loss += loss.item()\n",
    "            train_prob = np.concatenate((train_prob, output_prob[:,finalperiod-1,0].cpu().detach().numpy()), axis=None)\n",
    "            \n",
    "        train_acc = (float(train_num_correct) / len(mytrain_output))*100\n",
    "        train_auc = metrics.roc_auc_score(mytrain_output[:,finalperiod-1,0], train_prob)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Training loss calculation\n",
    "        tmpmytrain_input  = torch.from_numpy(mytrain_input).float().to(device)\n",
    "        tmpmytrain_output = torch.from_numpy(mytrain_output).float().to(device)\n",
    "        tmpoutputtrain, tmphidden = model(tmpmytrain_input, device)\n",
    "        tmpoutputtrain = tmpoutputtrain.reshape((tmpmytrain_output.shape))\n",
    "        tmpoutputtrain_prob = nn.functional.softmax(tmpoutputtrain, dim=2)\n",
    "        # train_loss = criterion(tmpoutputtrain_prob, tmpmytrain_output)\n",
    "        train_loss = trainprior*criterion(tmpoutputtrain_prob[class1ID_train,:,0], tmpmytrain_output[class1ID_train,:,0]) + (1-trainprior)*criterion(tmpoutputtrain_prob[class2ID_train,:,1], tmpmytrain_output[class2ID_train,:,1])\n",
    "        history_train_valid['TrainLoss'].append(train_loss.item())\n",
    "        history_train_valid['TrainAcc'].append(train_acc)\n",
    "        history_train_valid['TrainAUC'].append(train_auc)\n",
    "        \n",
    "\n",
    "        #--------------       Validate model      --------------#\n",
    "        outputvalid, hidden = model(myvalid_input, device)\n",
    "        outputvalid = outputvalid.reshape((myvalid_output.shape))\n",
    "        outputvalid_prob = nn.functional.softmax(outputvalid, dim=2)\n",
    "        # validation loss\n",
    "        # valid_loss = criterion(outputvalid_prob, myvalid_output)\n",
    "        valid_loss = validprior*criterion(outputvalid_prob[class1ID_valid,:,0], myvalid_output[class1ID_valid,:,0]) + (1-validprior)*criterion(outputvalid_prob[class2ID_valid,:,1], myvalid_output[class2ID_valid,:,1])\n",
    "        # Number correct prediction on trainning set collection\n",
    "        tmppredprob = outputvalid_prob[:,finalperiod-1,0].cpu().detach().numpy()\n",
    "        tmppred = 1*(tmppredprob > 0.5)\n",
    "        tmpobs = myvalid_output[:,finalperiod-1,0].cpu().detach().numpy()\n",
    "        valid_num_correct = sum(1*(tmppred == tmpobs))\n",
    "        valid_acc = (float(valid_num_correct) / len(myvalid_output))*100\n",
    "        valid_auc = metrics.roc_auc_score(tmpobs, tmppredprob)\n",
    "        \n",
    "        history_train_valid['ValidLoss'].append(valid_loss.item())\n",
    "        history_train_valid['ValidAcc'].append(valid_acc)\n",
    "        history_train_valid['ValidAUC'].append(valid_auc)\n",
    "        \n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[E {epoch + 1}/{max_epochs}]'\n",
    "                  f\" T.Loss: {history_train_valid['TrainLoss'][-1]:.4f}, T.Acc: {history_train_valid['TrainAcc'][-1]:2.2f}, T.AUC: {history_train_valid['TrainAUC'][-1]:.4f}\"\n",
    "                  f\" V.Loss: {history_train_valid['ValidLoss'][-1]:.4f}, V.Acc: {history_train_valid['ValidAcc'][-1]:2.2f}, V.AUC: {history_train_valid['ValidAUC'][-1]:.4f};\")\n",
    "        \n",
    "        valid_auces.append(valid_auc.item())\n",
    "        valid_losses.append(valid_loss.item())\n",
    "        \n",
    "        if earlystop == \"auc\":\n",
    "            current_valid_auc = valid_auc\n",
    "            if current_valid_auc < last_valid_auc:\n",
    "                trigger_times += 1\n",
    "                print('AUC Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping by AUC!.')\n",
    "                    break\n",
    "            else:\n",
    "                print('trigger times: 0')\n",
    "                trigger_times = 0\n",
    "            last_valid_auc = np.mean(valid_auces[-10:])\n",
    "        elif earlystop == \"loss\":\n",
    "            current_valid_loss = valid_loss\n",
    "            if current_valid_loss > last_valid_loss:\n",
    "                trigger_times += 1\n",
    "                print('Loss Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping by LOSS!.')\n",
    "                    break\n",
    "            else:\n",
    "                print('Trigger times >= patience: 0')\n",
    "                trigger_times = 0\n",
    "            last_valid_loss = np.mean(valid_losses[-10:])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # if earlystop == \"auc\":\n",
    "        #     # start to considering early-stop after 20 epoch\n",
    "        #     if epoch > 20:\n",
    "        #        if np.mean(valid_auces) < valid_auc_max:\n",
    "        #            print(\"Stopped here by AUC!\")\n",
    "        #            break\n",
    "        #        valid_auc_max = np.mean(valid_auces)\n",
    "        # elif earlystop == \"loss\":\n",
    "        #    # start to considering early-stop after 20 epoch\n",
    "        #    if epoch > 20:\n",
    "        #        if np.mean(valid_losses) > valid_loss_min:\n",
    "        #            print(\"Stopped here by LOSS!\")\n",
    "        #            break\n",
    "        #        # valid_loss_min = np.mean(valid_losses[-20:])\n",
    "        #        valid_loss_min = np.mean(valid_losses)\n",
    "        \n",
    "    return history_train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53778d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Mtd(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_Mtd, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=output_size, bias=False)\n",
    "\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "            \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "\n",
    "        outp = self.fc_1(outp)\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daed6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_Mtd(mytrain_input_mtd, mytrain_output, myvalid_input_mtd, myvalid_output, mytest_input_mtd, mytest_output, finalperiod):\n",
    "    \n",
    "    # 7 -> lstm -> 16 -> 8\n",
    "    \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_mtd.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 8\n",
    "    fc_size     = [16]\n",
    "    n_layers    = 1\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 200\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_Mtd = Model_Mtd(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, \n",
    "                          n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_Mtd.parameters(), lr=lr) \n",
    "    \n",
    "    print(\"################ Mtd LSTM training...\")\n",
    "    Mtd_hist = LSTMtrain(model_Mtd, device, criterion, optimizer, mytrain_input_mtd, mytrain_output, \n",
    "                         myvalid_input_mtd, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    Mtd_obs, Mtd_pred, Mtd_prob = evaluate(model_Mtd, device, mytest_input_mtd, mytest_output, finalperiod, cutoff=0.5)\n",
    "    Mtdtest_auc = metrics.roc_auc_score(Mtd_obs, Mtd_prob[:,0])\n",
    "    Mtdtest_acc = metrics.accuracy_score(Mtd_obs, Mtd_pred)\n",
    "    Mtdtest_conf = metrics.confusion_matrix(Mtd_obs, Mtd_pred)\n",
    "\n",
    "    return model_Mtd, Mtd_hist, Mtd_obs, Mtd_pred, Mtd_prob, Mtdtest_auc, Mtdtest_acc, Mtdtest_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5816b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_pty(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_pty, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=fc_size[2], bias=False)\n",
    "        self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        \n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_4(outp)   # 4th Ouuput\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910645cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_pty(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod):\n",
    "   \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_pty.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 256\n",
    "    n_layers    = 1\n",
    "    fc_size     = [256, 128, 64]\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 200\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_pty = Model_pty(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, \n",
    "                          n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_pty.parameters(), lr=lr) \n",
    "    \n",
    "    #---- training lstm ----#\n",
    "    print(\"################ pty LSTM training...\")\n",
    "    pty_hist = LSTMtrain(model_pty, device, criterion, optimizer, mytrain_input_pty, mytrain_output, \n",
    "                         myvalid_input_pty, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    pty_obs, pty_pred, pty_prob = evaluate(model_pty, device, mytest_input_pty, mytest_output, finalperiod, cutoff=0.5)\n",
    "    ptytest_auc = metrics.roc_auc_score(pty_obs, pty_prob[:,0])\n",
    "    ptytest_acc = metrics.accuracy_score(pty_obs, pty_pred)\n",
    "    ptytest_conf = metrics.confusion_matrix(pty_obs, pty_pred)\n",
    "\n",
    "    return model_pty, pty_hist, pty_obs, pty_pred, pty_prob, ptytest_auc, ptytest_acc, ptytest_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73926e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_txy(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_txy, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=fc_size[2], bias=False)\n",
    "        self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_4(outp)   # 4th Ouuput\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "426a2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_txy(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod):\n",
    "    \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_txy.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 256\n",
    "    n_layers    = 1\n",
    "    fc_size     = [256, 128, 64]\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 200\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_txy = Model_txy(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, \n",
    "                          seq_len=seq_len, n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_txy.parameters(), lr=lr) \n",
    "    \n",
    "    print(\"################ txy LSTM training...\")\n",
    "    txy_hist = LSTMtrain(model_txy, device, criterion, optimizer, mytrain_input_txy, mytrain_output, \n",
    "                         myvalid_input_txy, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    txy_obs, txy_pred, txy_prob = evaluate(model_txy, device, mytest_input_txy, mytest_output, finalperiod, cutoff=0.5)\n",
    "    txytest_auc = metrics.roc_auc_score(txy_obs, txy_prob[:,0])\n",
    "    txytest_acc = metrics.accuracy_score(txy_obs, txy_pred)\n",
    "    txytest_conf = metrics.confusion_matrix(txy_obs, txy_pred)\n",
    "\n",
    "    return model_txy, txy_hist, txy_obs, txy_pred, txy_prob, txytest_auc, txytest_acc, txytest_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0661a",
   "metadata": {},
   "source": [
    "# Main script start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15b65d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Output: train/valid/test (before reshape)\n",
      "(862, 12)\n",
      "(433, 12)\n",
      "(130, 12)\n",
      "################ Output: train/valid/test (after reshape)\n",
      "(695, 4, 2)\n",
      "(346, 4, 2)\n",
      "(114, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "# data directory\n",
    "# meta_dir      = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/metadata/metadata.csv'\n",
    "meta_dir      = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/metadata_imputed1.csv'\n",
    "alpha_dir     = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/alpha_diversity/alpha_diversity.csv'\n",
    "cst_dir       = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/community_state_types/cst_valencia.csv'\n",
    "\n",
    "txy_dir_fam = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.family.csv'\n",
    "txy_dir_gen = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.genus.csv'\n",
    "txy_dir_spe = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.species.csv'\n",
    "\n",
    "pty_dir_1dot = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.1e0.csv'\n",
    "pty_dir_dot5 = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.5e_1.csv'\n",
    "pty_dir_dot1 = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.1e_1.csv'\n",
    "\n",
    "# krdwide_dir   = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/pairwise_distance/krd_distance_wide.csv'\n",
    "\n",
    "\n",
    "txy_dir = txy_dir_fam\n",
    "pty_dir = pty_dir_1dot\n",
    "\n",
    "# task = \"was_preterm\"\n",
    "# finalperiod = 5\n",
    "task = \"was_early_preterm\"\n",
    "finalperiod = 4\n",
    "\n",
    "myprop = [0.6, 0.3, 0.1]\n",
    "myseed = 0\n",
    "\n",
    "\n",
    "#-------------------------------------------#\n",
    "#---- Data Preparation                  ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "meta_data = metadata_loader(meta_dir, alpha_dir, cst_dir, task, finalperiod)\n",
    "\n",
    "#---- data set splitter ----#\n",
    "trainID, validID, testID = dataset_splitID(meta_data=meta_data, prop=myprop, myseed=myseed)\n",
    "\n",
    "#---- output loader ----#\n",
    "mytrain_output, myvalid_output, mytest_output = OutputLoader(meta_data, trainID, validID, testID, task, finalperiod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17876d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ meta:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(862, 9)\n",
      "(433, 9)\n",
      "(130, 9)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(695, 4, 7)\n",
      "(346, 4, 7)\n",
      "(114, 4, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ meta:\")\n",
    "mytrain_input_mtd, myvalid_input_mtd, mytest_input_mtd = InputLoaderMtd(meta_data, trainID, validID, testID, task, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803ac03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ pty:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(862, 297)\n",
      "(433, 297)\n",
      "(130, 297)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(695, 4, 295)\n",
      "(346, 4, 295)\n",
      "(114, 4, 295)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ pty:\")\n",
    "mytrain_input_pty, myvalid_input_pty, mytest_input_pty = InputLoader(pty_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "815b14aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ txy:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(862, 187)\n",
      "(433, 187)\n",
      "(130, 187)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(695, 4, 185)\n",
      "(346, 4, 185)\n",
      "(114, 4, 185)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ txy:\")\n",
    "mytrain_input_txy, myvalid_input_txy, mytest_input_txy = InputLoader(txy_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06738bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Mtd LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0837, T.Acc: 86.47, T.AUC: 0.6056 V.Loss: 0.0838, V.Acc: 85.84, V.AUC: 0.6530;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0788, T.Acc: 86.47, T.AUC: 0.6271 V.Loss: 0.0790, V.Acc: 85.84, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0736, T.Acc: 86.47, T.AUC: 0.5433 V.Loss: 0.0740, V.Acc: 85.84, V.AUC: 0.6960;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0685, T.Acc: 86.47, T.AUC: 0.5500 V.Loss: 0.0690, V.Acc: 85.84, V.AUC: 0.6997;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0636, T.Acc: 86.47, T.AUC: 0.5550 V.Loss: 0.0643, V.Acc: 85.84, V.AUC: 0.6937;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0592, T.Acc: 86.47, T.AUC: 0.5494 V.Loss: 0.0600, V.Acc: 85.84, V.AUC: 0.6861;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0555, T.Acc: 86.47, T.AUC: 0.5444 V.Loss: 0.0564, V.Acc: 85.84, V.AUC: 0.6803;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0524, T.Acc: 86.47, T.AUC: 0.5108 V.Loss: 0.0535, V.Acc: 85.84, V.AUC: 0.6736;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0501, T.Acc: 86.47, T.AUC: 0.5539 V.Loss: 0.0513, V.Acc: 85.84, V.AUC: 0.6703;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0483, T.Acc: 86.47, T.AUC: 0.5101 V.Loss: 0.0496, V.Acc: 85.84, V.AUC: 0.6684;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0470, T.Acc: 86.47, T.AUC: 0.4972 V.Loss: 0.0485, V.Acc: 85.84, V.AUC: 0.6667;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0462, T.Acc: 86.47, T.AUC: 0.5912 V.Loss: 0.0477, V.Acc: 85.84, V.AUC: 0.6652;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.5287 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6647;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5457 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6650;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5108 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6657;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5622 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6665;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5570 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6675;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.6174 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6684;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5578 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6699;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.4998 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6708;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.5786 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6725;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.5472 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6743;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.5049 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6760;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.5718 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6783;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5612 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6797;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5281 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6811;\n",
      "Trigger times >= patience: 0\n",
      "[E 27/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5865 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6827;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5587 V.Loss: 0.0464, V.Acc: 85.84, V.AUC: 0.6843;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5491 V.Loss: 0.0464, V.Acc: 85.84, V.AUC: 0.6884;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.5884 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6900;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.6035 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6939;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.5534 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6955;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.5841 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6959;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.5382 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6962;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.6173 V.Loss: 0.0461, V.Acc: 85.84, V.AUC: 0.6975;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.6271 V.Loss: 0.0461, V.Acc: 85.84, V.AUC: 0.6995;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.5809 V.Loss: 0.0460, V.Acc: 85.84, V.AUC: 0.7013;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.6222 V.Loss: 0.0460, V.Acc: 85.84, V.AUC: 0.7029;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.6123 V.Loss: 0.0459, V.Acc: 85.84, V.AUC: 0.7043;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.5860 V.Loss: 0.0459, V.Acc: 85.84, V.AUC: 0.7063;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.6082 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.7082;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.5838 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.7090;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.5882 V.Loss: 0.0457, V.Acc: 85.84, V.AUC: 0.7111;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.6208 V.Loss: 0.0457, V.Acc: 85.84, V.AUC: 0.7128;\n",
      "Trigger times >= patience: 0\n",
      "[E 45/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.5889 V.Loss: 0.0456, V.Acc: 85.84, V.AUC: 0.7150;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.5852 V.Loss: 0.0456, V.Acc: 85.84, V.AUC: 0.7167;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0443, T.Acc: 86.47, T.AUC: 0.6112 V.Loss: 0.0455, V.Acc: 85.84, V.AUC: 0.7181;\n",
      "Trigger times >= patience: 0\n",
      "[E 48/2000] T.Loss: 0.0443, T.Acc: 86.47, T.AUC: 0.6051 V.Loss: 0.0454, V.Acc: 85.84, V.AUC: 0.7199;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0442, T.Acc: 86.47, T.AUC: 0.6243 V.Loss: 0.0453, V.Acc: 85.84, V.AUC: 0.7203;\n",
      "Trigger times >= patience: 0\n",
      "[E 50/2000] T.Loss: 0.0442, T.Acc: 86.47, T.AUC: 0.5917 V.Loss: 0.0453, V.Acc: 85.84, V.AUC: 0.7203;\n",
      "Trigger times >= patience: 0\n",
      "[E 51/2000] T.Loss: 0.0442, T.Acc: 86.47, T.AUC: 0.5710 V.Loss: 0.0452, V.Acc: 85.84, V.AUC: 0.7209;\n",
      "Trigger times >= patience: 0\n",
      "[E 52/2000] T.Loss: 0.0441, T.Acc: 86.47, T.AUC: 0.6222 V.Loss: 0.0451, V.Acc: 85.84, V.AUC: 0.7222;\n",
      "Trigger times >= patience: 0\n",
      "[E 53/2000] T.Loss: 0.0441, T.Acc: 86.47, T.AUC: 0.6116 V.Loss: 0.0450, V.Acc: 85.84, V.AUC: 0.7220;\n",
      "Trigger times >= patience: 0\n",
      "[E 54/2000] T.Loss: 0.0440, T.Acc: 86.47, T.AUC: 0.6533 V.Loss: 0.0450, V.Acc: 85.84, V.AUC: 0.7223;\n",
      "Trigger times >= patience: 0\n",
      "[E 55/2000] T.Loss: 0.0440, T.Acc: 86.47, T.AUC: 0.6605 V.Loss: 0.0449, V.Acc: 85.84, V.AUC: 0.7219;\n",
      "Trigger times >= patience: 0\n",
      "[E 56/2000] T.Loss: 0.0439, T.Acc: 86.47, T.AUC: 0.6332 V.Loss: 0.0448, V.Acc: 85.84, V.AUC: 0.7231;\n",
      "Trigger times >= patience: 0\n",
      "[E 57/2000] T.Loss: 0.0439, T.Acc: 86.47, T.AUC: 0.6077 V.Loss: 0.0447, V.Acc: 85.84, V.AUC: 0.7227;\n",
      "Trigger times >= patience: 0\n",
      "[E 58/2000] T.Loss: 0.0438, T.Acc: 86.47, T.AUC: 0.6584 V.Loss: 0.0446, V.Acc: 85.84, V.AUC: 0.7226;\n",
      "Trigger times >= patience: 0\n",
      "[E 59/2000] T.Loss: 0.0438, T.Acc: 86.47, T.AUC: 0.6358 V.Loss: 0.0445, V.Acc: 85.84, V.AUC: 0.7223;\n",
      "Trigger times >= patience: 0\n",
      "[E 60/2000] T.Loss: 0.0437, T.Acc: 86.47, T.AUC: 0.6232 V.Loss: 0.0444, V.Acc: 85.84, V.AUC: 0.7225;\n",
      "Trigger times >= patience: 0\n",
      "[E 61/2000] T.Loss: 0.0437, T.Acc: 86.47, T.AUC: 0.6183 V.Loss: 0.0444, V.Acc: 85.84, V.AUC: 0.7222;\n",
      "Trigger times >= patience: 0\n",
      "[E 62/2000] T.Loss: 0.0436, T.Acc: 86.47, T.AUC: 0.6524 V.Loss: 0.0443, V.Acc: 85.84, V.AUC: 0.7223;\n",
      "Trigger times >= patience: 0\n",
      "[E 63/2000] T.Loss: 0.0436, T.Acc: 86.47, T.AUC: 0.6284 V.Loss: 0.0442, V.Acc: 85.84, V.AUC: 0.7228;\n",
      "Trigger times >= patience: 0\n",
      "[E 64/2000] T.Loss: 0.0435, T.Acc: 86.47, T.AUC: 0.6418 V.Loss: 0.0441, V.Acc: 85.84, V.AUC: 0.7227;\n",
      "Trigger times >= patience: 0\n",
      "[E 65/2000] T.Loss: 0.0435, T.Acc: 86.47, T.AUC: 0.6820 V.Loss: 0.0440, V.Acc: 85.84, V.AUC: 0.7223;\n",
      "Trigger times >= patience: 0\n",
      "[E 66/2000] T.Loss: 0.0434, T.Acc: 86.47, T.AUC: 0.6592 V.Loss: 0.0439, V.Acc: 85.84, V.AUC: 0.7221;\n",
      "Trigger times >= patience: 0\n",
      "[E 67/2000] T.Loss: 0.0434, T.Acc: 86.47, T.AUC: 0.6183 V.Loss: 0.0438, V.Acc: 85.84, V.AUC: 0.7213;\n",
      "Trigger times >= patience: 0\n",
      "[E 68/2000] T.Loss: 0.0433, T.Acc: 86.47, T.AUC: 0.6569 V.Loss: 0.0437, V.Acc: 85.84, V.AUC: 0.7212;\n",
      "Trigger times >= patience: 0\n",
      "[E 69/2000] T.Loss: 0.0433, T.Acc: 86.47, T.AUC: 0.6321 V.Loss: 0.0436, V.Acc: 85.84, V.AUC: 0.7207;\n",
      "Trigger times >= patience: 0\n",
      "[E 70/2000] T.Loss: 0.0432, T.Acc: 86.47, T.AUC: 0.6590 V.Loss: 0.0435, V.Acc: 85.84, V.AUC: 0.7202;\n",
      "Trigger times >= patience: 0\n",
      "[E 71/2000] T.Loss: 0.0432, T.Acc: 86.47, T.AUC: 0.6384 V.Loss: 0.0434, V.Acc: 85.84, V.AUC: 0.7195;\n",
      "Trigger times >= patience: 0\n",
      "[E 72/2000] T.Loss: 0.0431, T.Acc: 86.47, T.AUC: 0.6331 V.Loss: 0.0433, V.Acc: 85.84, V.AUC: 0.7188;\n",
      "Trigger times >= patience: 0\n",
      "[E 73/2000] T.Loss: 0.0431, T.Acc: 86.47, T.AUC: 0.6300 V.Loss: 0.0432, V.Acc: 85.84, V.AUC: 0.7184;\n",
      "Trigger times >= patience: 0\n",
      "[E 74/2000] T.Loss: 0.0430, T.Acc: 86.47, T.AUC: 0.6423 V.Loss: 0.0431, V.Acc: 85.84, V.AUC: 0.7183;\n",
      "Trigger times >= patience: 0\n",
      "[E 75/2000] T.Loss: 0.0430, T.Acc: 86.47, T.AUC: 0.6142 V.Loss: 0.0431, V.Acc: 85.84, V.AUC: 0.7176;\n",
      "Trigger times >= patience: 0\n",
      "[E 76/2000] T.Loss: 0.0430, T.Acc: 86.47, T.AUC: 0.6465 V.Loss: 0.0430, V.Acc: 85.84, V.AUC: 0.7174;\n",
      "Trigger times >= patience: 0\n",
      "[E 77/2000] T.Loss: 0.0429, T.Acc: 86.47, T.AUC: 0.6358 V.Loss: 0.0429, V.Acc: 85.84, V.AUC: 0.7169;\n",
      "Trigger times >= patience: 0\n",
      "[E 78/2000] T.Loss: 0.0429, T.Acc: 86.47, T.AUC: 0.6345 V.Loss: 0.0428, V.Acc: 85.84, V.AUC: 0.7170;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 79/2000] T.Loss: 0.0428, T.Acc: 86.47, T.AUC: 0.6692 V.Loss: 0.0428, V.Acc: 85.84, V.AUC: 0.7162;\n",
      "Trigger times >= patience: 0\n",
      "[E 80/2000] T.Loss: 0.0428, T.Acc: 86.47, T.AUC: 0.6531 V.Loss: 0.0427, V.Acc: 85.84, V.AUC: 0.7157;\n",
      "Trigger times >= patience: 0\n",
      "[E 81/2000] T.Loss: 0.0428, T.Acc: 86.47, T.AUC: 0.6453 V.Loss: 0.0426, V.Acc: 85.84, V.AUC: 0.7153;\n",
      "Trigger times >= patience: 0\n",
      "[E 82/2000] T.Loss: 0.0427, T.Acc: 86.47, T.AUC: 0.6230 V.Loss: 0.0426, V.Acc: 85.84, V.AUC: 0.7150;\n",
      "Trigger times >= patience: 0\n",
      "[E 83/2000] T.Loss: 0.0427, T.Acc: 86.47, T.AUC: 0.6448 V.Loss: 0.0425, V.Acc: 85.84, V.AUC: 0.7147;\n",
      "Trigger times >= patience: 0\n",
      "[E 84/2000] T.Loss: 0.0426, T.Acc: 86.47, T.AUC: 0.6700 V.Loss: 0.0424, V.Acc: 85.84, V.AUC: 0.7144;\n",
      "Trigger times >= patience: 0\n",
      "[E 85/2000] T.Loss: 0.0426, T.Acc: 86.47, T.AUC: 0.6832 V.Loss: 0.0423, V.Acc: 85.84, V.AUC: 0.7145;\n",
      "Trigger times >= patience: 0\n",
      "[E 86/2000] T.Loss: 0.0426, T.Acc: 86.47, T.AUC: 0.6696 V.Loss: 0.0422, V.Acc: 85.84, V.AUC: 0.7136;\n",
      "Trigger times >= patience: 0\n",
      "[E 87/2000] T.Loss: 0.0425, T.Acc: 86.47, T.AUC: 0.6395 V.Loss: 0.0421, V.Acc: 85.84, V.AUC: 0.7130;\n",
      "Trigger times >= patience: 0\n",
      "[E 88/2000] T.Loss: 0.0425, T.Acc: 86.47, T.AUC: 0.6627 V.Loss: 0.0421, V.Acc: 85.84, V.AUC: 0.7125;\n",
      "Trigger times >= patience: 0\n",
      "[E 89/2000] T.Loss: 0.0424, T.Acc: 86.47, T.AUC: 0.6621 V.Loss: 0.0420, V.Acc: 85.84, V.AUC: 0.7121;\n",
      "Trigger times >= patience: 0\n",
      "[E 90/2000] T.Loss: 0.0424, T.Acc: 86.47, T.AUC: 0.6691 V.Loss: 0.0419, V.Acc: 85.84, V.AUC: 0.7115;\n",
      "Trigger times >= patience: 0\n",
      "[E 91/2000] T.Loss: 0.0424, T.Acc: 86.47, T.AUC: 0.6348 V.Loss: 0.0419, V.Acc: 85.84, V.AUC: 0.7113;\n",
      "Trigger times >= patience: 0\n",
      "[E 92/2000] T.Loss: 0.0423, T.Acc: 86.47, T.AUC: 0.6626 V.Loss: 0.0418, V.Acc: 85.84, V.AUC: 0.7107;\n",
      "Trigger times >= patience: 0\n",
      "[E 93/2000] T.Loss: 0.0423, T.Acc: 86.47, T.AUC: 0.6333 V.Loss: 0.0417, V.Acc: 85.84, V.AUC: 0.7110;\n",
      "Trigger times >= patience: 0\n",
      "[E 94/2000] T.Loss: 0.0423, T.Acc: 86.47, T.AUC: 0.6477 V.Loss: 0.0417, V.Acc: 85.84, V.AUC: 0.7106;\n",
      "Trigger times >= patience: 0\n",
      "[E 95/2000] T.Loss: 0.0422, T.Acc: 86.47, T.AUC: 0.6494 V.Loss: 0.0417, V.Acc: 85.84, V.AUC: 0.7101;\n",
      "Trigger times >= patience: 0\n",
      "[E 96/2000] T.Loss: 0.0422, T.Acc: 86.47, T.AUC: 0.6697 V.Loss: 0.0416, V.Acc: 85.84, V.AUC: 0.7098;\n",
      "Trigger times >= patience: 0\n",
      "[E 97/2000] T.Loss: 0.0422, T.Acc: 86.47, T.AUC: 0.6682 V.Loss: 0.0416, V.Acc: 85.84, V.AUC: 0.7099;\n",
      "Trigger times >= patience: 0\n",
      "[E 98/2000] T.Loss: 0.0421, T.Acc: 86.47, T.AUC: 0.6413 V.Loss: 0.0415, V.Acc: 85.84, V.AUC: 0.7102;\n",
      "Trigger times >= patience: 0\n",
      "[E 99/2000] T.Loss: 0.0421, T.Acc: 86.47, T.AUC: 0.6655 V.Loss: 0.0415, V.Acc: 85.84, V.AUC: 0.7102;\n",
      "Trigger times >= patience: 0\n",
      "[E 100/2000] T.Loss: 0.0421, T.Acc: 86.47, T.AUC: 0.6682 V.Loss: 0.0414, V.Acc: 85.84, V.AUC: 0.7104;\n",
      "Trigger times >= patience: 0\n",
      "[E 101/2000] T.Loss: 0.0420, T.Acc: 86.47, T.AUC: 0.6562 V.Loss: 0.0414, V.Acc: 85.84, V.AUC: 0.7106;\n",
      "Trigger times >= patience: 0\n",
      "[E 102/2000] T.Loss: 0.0420, T.Acc: 86.47, T.AUC: 0.6388 V.Loss: 0.0413, V.Acc: 85.84, V.AUC: 0.7106;\n",
      "Trigger times >= patience: 0\n",
      "[E 103/2000] T.Loss: 0.0420, T.Acc: 86.47, T.AUC: 0.6609 V.Loss: 0.0413, V.Acc: 85.84, V.AUC: 0.7111;\n",
      "Trigger times >= patience: 0\n",
      "[E 104/2000] T.Loss: 0.0420, T.Acc: 86.47, T.AUC: 0.6685 V.Loss: 0.0412, V.Acc: 85.84, V.AUC: 0.7117;\n",
      "Trigger times >= patience: 0\n",
      "[E 105/2000] T.Loss: 0.0419, T.Acc: 86.47, T.AUC: 0.6648 V.Loss: 0.0412, V.Acc: 85.84, V.AUC: 0.7117;\n",
      "Trigger times >= patience: 0\n",
      "[E 106/2000] T.Loss: 0.0419, T.Acc: 86.47, T.AUC: 0.6924 V.Loss: 0.0412, V.Acc: 85.84, V.AUC: 0.7111;\n",
      "Trigger times >= patience: 0\n",
      "[E 107/2000] T.Loss: 0.0419, T.Acc: 86.62, T.AUC: 0.6585 V.Loss: 0.0411, V.Acc: 85.84, V.AUC: 0.7110;\n",
      "Trigger times >= patience: 0\n",
      "[E 108/2000] T.Loss: 0.0418, T.Acc: 86.47, T.AUC: 0.6802 V.Loss: 0.0411, V.Acc: 85.84, V.AUC: 0.7111;\n",
      "Trigger times >= patience: 0\n",
      "[E 109/2000] T.Loss: 0.0418, T.Acc: 86.47, T.AUC: 0.6801 V.Loss: 0.0410, V.Acc: 85.84, V.AUC: 0.7110;\n",
      "Trigger times >= patience: 0\n",
      "[E 110/2000] T.Loss: 0.0418, T.Acc: 86.62, T.AUC: 0.6414 V.Loss: 0.0410, V.Acc: 85.84, V.AUC: 0.7111;\n",
      "Trigger times >= patience: 0\n",
      "[E 111/2000] T.Loss: 0.0418, T.Acc: 86.47, T.AUC: 0.6662 V.Loss: 0.0410, V.Acc: 85.84, V.AUC: 0.7106;\n",
      "Trigger times >= patience: 0\n",
      "[E 112/2000] T.Loss: 0.0417, T.Acc: 86.62, T.AUC: 0.6792 V.Loss: 0.0410, V.Acc: 85.84, V.AUC: 0.7106;\n",
      "Trigger times >= patience: 0\n",
      "[E 113/2000] T.Loss: 0.0417, T.Acc: 86.47, T.AUC: 0.6571 V.Loss: 0.0410, V.Acc: 85.84, V.AUC: 0.7106;\n",
      "Trigger times >= patience: 0\n",
      "[E 114/2000] T.Loss: 0.0417, T.Acc: 86.62, T.AUC: 0.6516 V.Loss: 0.0410, V.Acc: 85.84, V.AUC: 0.7107;\n",
      "Trigger times >= patience: 0\n",
      "[E 115/2000] T.Loss: 0.0417, T.Acc: 86.62, T.AUC: 0.6615 V.Loss: 0.0409, V.Acc: 85.84, V.AUC: 0.7110;\n",
      "Trigger times >= patience: 0\n",
      "[E 116/2000] T.Loss: 0.0416, T.Acc: 86.62, T.AUC: 0.6584 V.Loss: 0.0409, V.Acc: 85.84, V.AUC: 0.7111;\n",
      "Trigger times >= patience: 0\n",
      "[E 117/2000] T.Loss: 0.0416, T.Acc: 86.76, T.AUC: 0.6450 V.Loss: 0.0409, V.Acc: 85.84, V.AUC: 0.7113;\n",
      "Trigger times >= patience: 0\n",
      "[E 118/2000] T.Loss: 0.0416, T.Acc: 86.62, T.AUC: 0.6654 V.Loss: 0.0408, V.Acc: 85.84, V.AUC: 0.7113;\n",
      "Trigger times >= patience: 0\n",
      "[E 119/2000] T.Loss: 0.0416, T.Acc: 86.62, T.AUC: 0.6781 V.Loss: 0.0408, V.Acc: 86.42, V.AUC: 0.7113;\n",
      "Trigger times >= patience: 0\n",
      "[E 120/2000] T.Loss: 0.0415, T.Acc: 86.62, T.AUC: 0.6491 V.Loss: 0.0408, V.Acc: 86.71, V.AUC: 0.7115;\n",
      "Trigger times >= patience: 0\n",
      "[E 121/2000] T.Loss: 0.0415, T.Acc: 86.62, T.AUC: 0.6801 V.Loss: 0.0407, V.Acc: 86.71, V.AUC: 0.7117;\n",
      "Trigger times >= patience: 0\n",
      "[E 122/2000] T.Loss: 0.0415, T.Acc: 86.62, T.AUC: 0.6799 V.Loss: 0.0407, V.Acc: 86.71, V.AUC: 0.7112;\n",
      "Trigger times >= patience: 0\n",
      "[E 123/2000] T.Loss: 0.0415, T.Acc: 86.47, T.AUC: 0.6656 V.Loss: 0.0407, V.Acc: 86.42, V.AUC: 0.7111;\n",
      "Trigger times >= patience: 0\n",
      "[E 124/2000] T.Loss: 0.0415, T.Acc: 86.62, T.AUC: 0.6925 V.Loss: 0.0407, V.Acc: 86.13, V.AUC: 0.7111;\n",
      "Trigger times >= patience: 0\n",
      "[E 125/2000] T.Loss: 0.0414, T.Acc: 86.76, T.AUC: 0.6954 V.Loss: 0.0406, V.Acc: 85.84, V.AUC: 0.7110;\n",
      "Trigger times >= patience: 0\n",
      "[E 126/2000] T.Loss: 0.0414, T.Acc: 86.91, T.AUC: 0.6704 V.Loss: 0.0406, V.Acc: 85.84, V.AUC: 0.7112;\n",
      "Trigger times >= patience: 0\n",
      "[E 127/2000] T.Loss: 0.0414, T.Acc: 86.76, T.AUC: 0.6778 V.Loss: 0.0406, V.Acc: 85.84, V.AUC: 0.7109;\n",
      "Trigger times >= patience: 0\n",
      "[E 128/2000] T.Loss: 0.0414, T.Acc: 86.76, T.AUC: 0.6497 V.Loss: 0.0406, V.Acc: 85.84, V.AUC: 0.7115;\n",
      "Trigger times >= patience: 0\n",
      "[E 129/2000] T.Loss: 0.0414, T.Acc: 86.62, T.AUC: 0.6772 V.Loss: 0.0406, V.Acc: 85.84, V.AUC: 0.7118;\n",
      "Trigger times >= patience: 0\n",
      "[E 130/2000] T.Loss: 0.0414, T.Acc: 86.62, T.AUC: 0.6755 V.Loss: 0.0406, V.Acc: 86.13, V.AUC: 0.7122;\n",
      "Trigger times >= patience: 0\n",
      "[E 131/2000] T.Loss: 0.0413, T.Acc: 86.91, T.AUC: 0.6657 V.Loss: 0.0406, V.Acc: 85.84, V.AUC: 0.7122;\n",
      "Trigger times >= patience: 0\n",
      "[E 132/2000] T.Loss: 0.0413, T.Acc: 87.19, T.AUC: 0.6744 V.Loss: 0.0405, V.Acc: 85.84, V.AUC: 0.7124;\n",
      "Trigger times >= patience: 0\n",
      "[E 133/2000] T.Loss: 0.0413, T.Acc: 86.62, T.AUC: 0.6361 V.Loss: 0.0405, V.Acc: 86.13, V.AUC: 0.7131;\n",
      "Trigger times >= patience: 0\n",
      "[E 134/2000] T.Loss: 0.0413, T.Acc: 86.76, T.AUC: 0.6709 V.Loss: 0.0405, V.Acc: 86.71, V.AUC: 0.7139;\n",
      "Trigger times >= patience: 0\n",
      "[E 135/2000] T.Loss: 0.0413, T.Acc: 87.19, T.AUC: 0.6990 V.Loss: 0.0405, V.Acc: 87.28, V.AUC: 0.7143;\n",
      "Trigger times >= patience: 0\n",
      "[E 136/2000] T.Loss: 0.0412, T.Acc: 87.19, T.AUC: 0.6704 V.Loss: 0.0404, V.Acc: 86.99, V.AUC: 0.7144;\n",
      "Trigger times >= patience: 0\n",
      "[E 137/2000] T.Loss: 0.0412, T.Acc: 87.34, T.AUC: 0.6539 V.Loss: 0.0404, V.Acc: 86.99, V.AUC: 0.7144;\n",
      "Trigger times >= patience: 0\n",
      "[E 138/2000] T.Loss: 0.0412, T.Acc: 87.05, T.AUC: 0.6435 V.Loss: 0.0404, V.Acc: 87.28, V.AUC: 0.7146;\n",
      "Trigger times >= patience: 0\n",
      "[E 139/2000] T.Loss: 0.0412, T.Acc: 87.05, T.AUC: 0.6747 V.Loss: 0.0404, V.Acc: 87.28, V.AUC: 0.7144;\n",
      "Trigger times >= patience: 0\n",
      "[E 140/2000] T.Loss: 0.0411, T.Acc: 87.05, T.AUC: 0.6710 V.Loss: 0.0403, V.Acc: 86.99, V.AUC: 0.7143;\n",
      "Trigger times >= patience: 0\n",
      "[E 141/2000] T.Loss: 0.0411, T.Acc: 87.34, T.AUC: 0.6809 V.Loss: 0.0403, V.Acc: 86.99, V.AUC: 0.7139;\n",
      "Trigger times >= patience: 0\n",
      "[E 142/2000] T.Loss: 0.0411, T.Acc: 87.19, T.AUC: 0.6592 V.Loss: 0.0403, V.Acc: 87.57, V.AUC: 0.7141;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 143/2000] T.Loss: 0.0411, T.Acc: 87.34, T.AUC: 0.6730 V.Loss: 0.0403, V.Acc: 87.86, V.AUC: 0.7140;\n",
      "Trigger times >= patience: 0\n",
      "[E 144/2000] T.Loss: 0.0411, T.Acc: 87.19, T.AUC: 0.6700 V.Loss: 0.0403, V.Acc: 87.57, V.AUC: 0.7137;\n",
      "Trigger times >= patience: 0\n",
      "[E 145/2000] T.Loss: 0.0411, T.Acc: 87.34, T.AUC: 0.6857 V.Loss: 0.0403, V.Acc: 86.99, V.AUC: 0.7130;\n",
      "Trigger times >= patience: 0\n",
      "[E 146/2000] T.Loss: 0.0411, T.Acc: 87.05, T.AUC: 0.6748 V.Loss: 0.0403, V.Acc: 86.99, V.AUC: 0.7131;\n",
      "Trigger times >= patience: 0\n",
      "[E 147/2000] T.Loss: 0.0410, T.Acc: 87.34, T.AUC: 0.6729 V.Loss: 0.0403, V.Acc: 86.99, V.AUC: 0.7136;\n",
      "Trigger times >= patience: 0\n",
      "[E 148/2000] T.Loss: 0.0410, T.Acc: 87.05, T.AUC: 0.6770 V.Loss: 0.0403, V.Acc: 86.99, V.AUC: 0.7141;\n",
      "Trigger times >= patience: 0\n",
      "[E 149/2000] T.Loss: 0.0410, T.Acc: 87.19, T.AUC: 0.6630 V.Loss: 0.0403, V.Acc: 87.57, V.AUC: 0.7148;\n",
      "Trigger times >= patience: 0\n",
      "[E 150/2000] T.Loss: 0.0410, T.Acc: 87.19, T.AUC: 0.6773 V.Loss: 0.0403, V.Acc: 87.57, V.AUC: 0.7150;\n",
      "Trigger times >= patience: 0\n",
      "[E 151/2000] T.Loss: 0.0410, T.Acc: 87.05, T.AUC: 0.6756 V.Loss: 0.0403, V.Acc: 87.86, V.AUC: 0.7150;\n",
      "Trigger times >= patience: 0\n",
      "[E 152/2000] T.Loss: 0.0410, T.Acc: 87.05, T.AUC: 0.6718 V.Loss: 0.0403, V.Acc: 88.15, V.AUC: 0.7154;\n",
      "Trigger times >= patience: 0\n",
      "[E 153/2000] T.Loss: 0.0409, T.Acc: 86.91, T.AUC: 0.6890 V.Loss: 0.0403, V.Acc: 88.15, V.AUC: 0.7162;\n",
      "Trigger times >= patience: 0\n",
      "[E 154/2000] T.Loss: 0.0409, T.Acc: 87.05, T.AUC: 0.6748 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7159;\n",
      "Trigger times >= patience: 0\n",
      "[E 155/2000] T.Loss: 0.0409, T.Acc: 86.76, T.AUC: 0.6874 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7160;\n",
      "Trigger times >= patience: 0\n",
      "[E 156/2000] T.Loss: 0.0409, T.Acc: 87.19, T.AUC: 0.6787 V.Loss: 0.0402, V.Acc: 87.86, V.AUC: 0.7161;\n",
      "Trigger times >= patience: 0\n",
      "[E 157/2000] T.Loss: 0.0409, T.Acc: 87.19, T.AUC: 0.6737 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7153;\n",
      "Trigger times >= patience: 0\n",
      "[E 158/2000] T.Loss: 0.0409, T.Acc: 87.05, T.AUC: 0.6833 V.Loss: 0.0403, V.Acc: 88.15, V.AUC: 0.7150;\n",
      "Loss Trigger Times: 1\n",
      "[E 159/2000] T.Loss: 0.0409, T.Acc: 87.05, T.AUC: 0.6760 V.Loss: 0.0403, V.Acc: 88.15, V.AUC: 0.7148;\n",
      "Loss Trigger Times: 2\n",
      "[E 160/2000] T.Loss: 0.0408, T.Acc: 87.05, T.AUC: 0.6735 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7148;\n",
      "Trigger times >= patience: 0\n",
      "[E 161/2000] T.Loss: 0.0408, T.Acc: 87.19, T.AUC: 0.7064 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7150;\n",
      "Trigger times >= patience: 0\n",
      "[E 162/2000] T.Loss: 0.0408, T.Acc: 86.91, T.AUC: 0.6861 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7152;\n",
      "Trigger times >= patience: 0\n",
      "[E 163/2000] T.Loss: 0.0408, T.Acc: 87.19, T.AUC: 0.6741 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7157;\n",
      "Trigger times >= patience: 0\n",
      "[E 164/2000] T.Loss: 0.0408, T.Acc: 87.63, T.AUC: 0.6818 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7163;\n",
      "Trigger times >= patience: 0\n",
      "[E 165/2000] T.Loss: 0.0408, T.Acc: 87.05, T.AUC: 0.6802 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7174;\n",
      "Trigger times >= patience: 0\n",
      "[E 166/2000] T.Loss: 0.0408, T.Acc: 87.05, T.AUC: 0.6991 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7183;\n",
      "Trigger times >= patience: 0\n",
      "[E 167/2000] T.Loss: 0.0408, T.Acc: 86.62, T.AUC: 0.6789 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7188;\n",
      "Trigger times >= patience: 0\n",
      "[E 168/2000] T.Loss: 0.0407, T.Acc: 86.91, T.AUC: 0.6819 V.Loss: 0.0403, V.Acc: 88.15, V.AUC: 0.7194;\n",
      "Loss Trigger Times: 1\n",
      "[E 169/2000] T.Loss: 0.0407, T.Acc: 87.34, T.AUC: 0.6622 V.Loss: 0.0403, V.Acc: 88.15, V.AUC: 0.7192;\n",
      "Loss Trigger Times: 2\n",
      "[E 170/2000] T.Loss: 0.0407, T.Acc: 87.19, T.AUC: 0.6818 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7189;\n",
      "Loss Trigger Times: 3\n",
      "[E 171/2000] T.Loss: 0.0407, T.Acc: 87.05, T.AUC: 0.6939 V.Loss: 0.0402, V.Acc: 88.15, V.AUC: 0.7192;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.8421052631578947\n",
      "0.8382978723404255\n",
      "[[94  0]\n",
      " [18  2]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: Metadata             ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_Mtd, Mtd_hist, Mtdtest_obs, Mtdtest_pred, Mtdtest_prob, Mtdtest_auc, Mtdtest_acc, Mtdtest_conf = FirstStage_Mtd(mytrain_input_mtd, mytrain_output, myvalid_input_mtd, myvalid_output, mytest_input_mtd, mytest_output, finalperiod)\n",
    "print(Mtdtest_acc)\n",
    "print(Mtdtest_auc)\n",
    "print(Mtdtest_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d9b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ pty LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0609, T.Acc: 64.75, T.AUC: 0.4610 V.Loss: 0.0618, V.Acc: 85.84, V.AUC: 0.4824;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0493, T.Acc: 86.47, T.AUC: 0.5189 V.Loss: 0.0516, V.Acc: 85.84, V.AUC: 0.5108;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0495, T.Acc: 86.47, T.AUC: 0.5506 V.Loss: 0.0517, V.Acc: 85.84, V.AUC: 0.5285;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0482, T.Acc: 86.47, T.AUC: 0.5130 V.Loss: 0.0503, V.Acc: 85.84, V.AUC: 0.5418;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0471, T.Acc: 86.47, T.AUC: 0.4718 V.Loss: 0.0491, V.Acc: 85.84, V.AUC: 0.5510;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0462, T.Acc: 86.47, T.AUC: 0.5118 V.Loss: 0.0482, V.Acc: 85.84, V.AUC: 0.5590;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.4474 V.Loss: 0.0476, V.Acc: 85.84, V.AUC: 0.5688;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5569 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.5939;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5123 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6052;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5428 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6094;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5882 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.6193;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5719 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6302;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.6176 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6396;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.6599 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6491;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0440, T.Acc: 86.47, T.AUC: 0.5879 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6536;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0435, T.Acc: 86.47, T.AUC: 0.6553 V.Loss: 0.0459, V.Acc: 85.84, V.AUC: 0.6613;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0432, T.Acc: 86.47, T.AUC: 0.6952 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6653;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0421, T.Acc: 86.47, T.AUC: 0.7159 V.Loss: 0.0450, V.Acc: 85.84, V.AUC: 0.6664;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0407, T.Acc: 86.47, T.AUC: 0.7492 V.Loss: 0.0440, V.Acc: 85.55, V.AUC: 0.6637;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0395, T.Acc: 86.62, T.AUC: 0.7324 V.Loss: 0.0435, V.Acc: 86.13, V.AUC: 0.6614;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0390, T.Acc: 88.06, T.AUC: 0.7583 V.Loss: 0.0435, V.Acc: 86.42, V.AUC: 0.6608;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0391, T.Acc: 87.63, T.AUC: 0.7525 V.Loss: 0.0438, V.Acc: 86.42, V.AUC: 0.6669;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0391, T.Acc: 87.77, T.AUC: 0.7581 V.Loss: 0.0446, V.Acc: 86.99, V.AUC: 0.6665;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0389, T.Acc: 87.34, T.AUC: 0.7765 V.Loss: 0.0440, V.Acc: 86.71, V.AUC: 0.6696;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0378, T.Acc: 87.34, T.AUC: 0.7538 V.Loss: 0.0433, V.Acc: 86.71, V.AUC: 0.6691;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0387, T.Acc: 88.06, T.AUC: 0.7895 V.Loss: 0.0452, V.Acc: 86.99, V.AUC: 0.6696;\n",
      "Loss Trigger Times: 1\n",
      "[E 27/2000] T.Loss: 0.0377, T.Acc: 88.06, T.AUC: 0.7608 V.Loss: 0.0440, V.Acc: 86.71, V.AUC: 0.6618;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0366, T.Acc: 88.20, T.AUC: 0.7834 V.Loss: 0.0435, V.Acc: 86.42, V.AUC: 0.6581;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0372, T.Acc: 88.49, T.AUC: 0.7965 V.Loss: 0.0449, V.Acc: 86.99, V.AUC: 0.6584;\n",
      "Loss Trigger Times: 1\n",
      "[E 30/2000] T.Loss: 0.0366, T.Acc: 88.20, T.AUC: 0.7866 V.Loss: 0.0439, V.Acc: 86.99, V.AUC: 0.6615;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0356, T.Acc: 88.06, T.AUC: 0.7984 V.Loss: 0.0439, V.Acc: 87.28, V.AUC: 0.6656;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0354, T.Acc: 88.78, T.AUC: 0.8079 V.Loss: 0.0444, V.Acc: 87.28, V.AUC: 0.6654;\n",
      "Loss Trigger Times: 1\n",
      "[E 33/2000] T.Loss: 0.0352, T.Acc: 88.78, T.AUC: 0.7969 V.Loss: 0.0442, V.Acc: 87.28, V.AUC: 0.6640;\n",
      "Loss Trigger Times: 2\n",
      "[E 34/2000] T.Loss: 0.0347, T.Acc: 88.49, T.AUC: 0.8152 V.Loss: 0.0449, V.Acc: 87.28, V.AUC: 0.6644;\n",
      "Loss Trigger Times: 3\n",
      "[E 35/2000] T.Loss: 0.0343, T.Acc: 88.92, T.AUC: 0.8180 V.Loss: 0.0447, V.Acc: 87.28, V.AUC: 0.6592;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.8245614035087719\n",
      "0.6585106382978725\n",
      "[[92  2]\n",
      " [18  2]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: phylotype data       ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_pty, pty_hist, ptytest_obs, ptytest_pred, ptytest_prob, ptytest_auc, ptytest_acc, ptytest_conf = FirstStage_pty(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod)\n",
    "print(ptytest_acc)\n",
    "print(ptytest_auc)\n",
    "print(ptytest_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b67692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ txy LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0534, T.Acc: 83.88, T.AUC: 0.4550 V.Loss: 0.0545, V.Acc: 85.84, V.AUC: 0.4799;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0484, T.Acc: 86.47, T.AUC: 0.4669 V.Loss: 0.0505, V.Acc: 85.84, V.AUC: 0.5214;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0480, T.Acc: 86.47, T.AUC: 0.5482 V.Loss: 0.0499, V.Acc: 85.84, V.AUC: 0.5392;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0461, T.Acc: 86.47, T.AUC: 0.4870 V.Loss: 0.0480, V.Acc: 85.84, V.AUC: 0.5460;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0462, T.Acc: 86.47, T.AUC: 0.5288 V.Loss: 0.0480, V.Acc: 85.84, V.AUC: 0.5479;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0457, T.Acc: 86.47, T.AUC: 0.4701 V.Loss: 0.0475, V.Acc: 85.84, V.AUC: 0.5467;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0461, T.Acc: 86.47, T.AUC: 0.5000 V.Loss: 0.0481, V.Acc: 85.84, V.AUC: 0.5426;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0460, T.Acc: 86.47, T.AUC: 0.4928 V.Loss: 0.0479, V.Acc: 85.84, V.AUC: 0.5404;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4801 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.5419;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.4701 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.5428;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4558 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.5456;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.5193 V.Loss: 0.0475, V.Acc: 85.84, V.AUC: 0.5476;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.4745 V.Loss: 0.0475, V.Acc: 85.84, V.AUC: 0.5473;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.4749 V.Loss: 0.0475, V.Acc: 85.84, V.AUC: 0.5511;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4975 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.5490;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4822 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.5546;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4828 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.5563;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.5322 V.Loss: 0.0475, V.Acc: 85.84, V.AUC: 0.5613;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4834 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.5643;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5115 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5663;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5354 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5685;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5156 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.5715;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5018 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5734;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4965 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5791;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4913 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5844;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5065 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5892;\n",
      "Trigger times >= patience: 0\n",
      "[E 27/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5118 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5919;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5307 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5937;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5207 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5954;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5366 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5957;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5329 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.5949;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0458, T.Acc: 86.47, T.AUC: 0.5035 V.Loss: 0.0477, V.Acc: 85.84, V.AUC: 0.5968;\n",
      "Loss Trigger Times: 1\n",
      "[E 33/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.4977 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.5986;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.6084 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6050;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.6085 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6106;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.5987 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6162;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0434, T.Acc: 86.47, T.AUC: 0.6641 V.Loss: 0.0459, V.Acc: 86.13, V.AUC: 0.6184;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0424, T.Acc: 86.62, T.AUC: 0.6926 V.Loss: 0.0451, V.Acc: 85.55, V.AUC: 0.6175;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0441, T.Acc: 87.05, T.AUC: 0.6736 V.Loss: 0.0485, V.Acc: 85.55, V.AUC: 0.6133;\n",
      "Loss Trigger Times: 1\n",
      "[E 40/2000] T.Loss: 0.0430, T.Acc: 87.19, T.AUC: 0.6816 V.Loss: 0.0458, V.Acc: 86.42, V.AUC: 0.6112;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0405, T.Acc: 87.48, T.AUC: 0.7057 V.Loss: 0.0441, V.Acc: 86.42, V.AUC: 0.6127;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0399, T.Acc: 87.77, T.AUC: 0.7103 V.Loss: 0.0435, V.Acc: 86.99, V.AUC: 0.6182;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0402, T.Acc: 87.48, T.AUC: 0.7052 V.Loss: 0.0437, V.Acc: 86.99, V.AUC: 0.6244;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0395, T.Acc: 87.91, T.AUC: 0.6864 V.Loss: 0.0438, V.Acc: 86.99, V.AUC: 0.6352;\n",
      "Trigger times >= patience: 0\n",
      "[E 45/2000] T.Loss: 0.0397, T.Acc: 88.20, T.AUC: 0.7297 V.Loss: 0.0437, V.Acc: 86.99, V.AUC: 0.6419;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0396, T.Acc: 87.77, T.AUC: 0.7217 V.Loss: 0.0438, V.Acc: 86.99, V.AUC: 0.6474;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0390, T.Acc: 88.35, T.AUC: 0.6836 V.Loss: 0.0441, V.Acc: 86.71, V.AUC: 0.6551;\n",
      "Trigger times >= patience: 0\n",
      "[E 48/2000] T.Loss: 0.0391, T.Acc: 88.35, T.AUC: 0.7302 V.Loss: 0.0436, V.Acc: 86.99, V.AUC: 0.6515;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0392, T.Acc: 88.35, T.AUC: 0.7268 V.Loss: 0.0438, V.Acc: 86.99, V.AUC: 0.6480;\n",
      "Trigger times >= patience: 0\n",
      "[E 50/2000] T.Loss: 0.0382, T.Acc: 88.49, T.AUC: 0.7120 V.Loss: 0.0440, V.Acc: 86.71, V.AUC: 0.6502;\n",
      "Loss Trigger Times: 1\n",
      "[E 51/2000] T.Loss: 0.0379, T.Acc: 88.49, T.AUC: 0.7215 V.Loss: 0.0434, V.Acc: 86.99, V.AUC: 0.6473;\n",
      "Trigger times >= patience: 0\n",
      "[E 52/2000] T.Loss: 0.0381, T.Acc: 88.20, T.AUC: 0.7373 V.Loss: 0.0437, V.Acc: 86.99, V.AUC: 0.6474;\n",
      "Trigger times >= patience: 0\n",
      "[E 53/2000] T.Loss: 0.0374, T.Acc: 88.49, T.AUC: 0.7294 V.Loss: 0.0447, V.Acc: 85.84, V.AUC: 0.6461;\n",
      "Loss Trigger Times: 1\n",
      "[E 54/2000] T.Loss: 0.0371, T.Acc: 89.35, T.AUC: 0.7288 V.Loss: 0.0446, V.Acc: 85.84, V.AUC: 0.6482;\n",
      "Loss Trigger Times: 2\n",
      "[E 55/2000] T.Loss: 0.0374, T.Acc: 89.06, T.AUC: 0.7360 V.Loss: 0.0443, V.Acc: 86.71, V.AUC: 0.6458;\n",
      "Loss Trigger Times: 3\n",
      "[E 56/2000] T.Loss: 0.0368, T.Acc: 88.92, T.AUC: 0.7565 V.Loss: 0.0446, V.Acc: 86.13, V.AUC: 0.6451;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.8508771929824561\n",
      "0.7196808510638297\n",
      "[[94  0]\n",
      " [17  3]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: taxonomy data        ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_txy, txy_hist, txytest_obs, txytest_pred, txytest_prob, txytest_auc, txytest_acc, txytest_conf = FirstStage_txy(mytrain_input_txy, mytrain_output, myvalid_input_txy, myvalid_output, mytest_input_txy, mytest_output, finalperiod)\n",
    "print(txytest_acc)\n",
    "print(txytest_auc)\n",
    "print(txytest_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b6192b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17543859649122806"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mytest_output[:,finalperiod-1,0])/mytest_output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d415957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1416184971098266\n",
      "0.8333333333333334\n",
      "0.7696808510638298\n",
      "[[94  0]\n",
      " [19  1]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use validation set only without class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "Mtdvalid_obs, Mtdvalid_pred, Mtdvalid_prob = evaluate(model_Mtd, device, myvalid_input_mtd, myvalid_output, finalperiod, cutoff=0.5)\n",
    "ptyvalid_obs, ptyvalid_pred, ptyvalid_prob = evaluate(model_pty, device, myvalid_input_pty, myvalid_output, finalperiod, cutoff=0.5)\n",
    "txyvalid_obs, txyvalid_pred, txyvalid_prob = evaluate(model_txy, device, myvalid_input_txy, myvalid_output, finalperiod, cutoff=0.5)\n",
    "\n",
    "x_valid = np.array(np.column_stack([Mtdvalid_prob, ptyvalid_prob, txyvalid_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(Mtdvalid_obs)/len(Mtdvalid_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag')\n",
    "L2Logistic_model.fit(x_valid, Mtdvalid_obs)\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ebac487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1416184971098266\n",
      "0.8333333333333334\n",
      "0.7664893617021277\n",
      "[[86  8]\n",
      " [11  9]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use validation set only with class weights (Best)\n",
    "#-------------------------------------------#\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "Mtdvalid_obs, Mtdvalid_pred, Mtdvalid_prob = evaluate(model_Mtd, device, myvalid_input_mtd, myvalid_output, finalperiod, cutoff=0.5)\n",
    "ptyvalid_obs, ptyvalid_pred, ptyvalid_prob = evaluate(model_pty, device, myvalid_input_pty, myvalid_output, finalperiod, cutoff=0.5)\n",
    "txyvalid_obs, txyvalid_pred, txyvalid_prob = evaluate(model_txy, device, myvalid_input_txy, myvalid_output, finalperiod, cutoff=0.5)\n",
    "\n",
    "x_valid = np.array(np.column_stack([Mtdvalid_prob, ptyvalid_prob, txyvalid_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(Mtdvalid_obs)/len(Mtdvalid_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag', class_weight=wt)\n",
    "L2Logistic_model.fit(x_valid, Mtdvalid_obs)\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd20ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1373679154658982\n",
      "0.8596491228070176\n",
      "0.7425531914893617\n",
      "[[93  1]\n",
      " [15  5]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use trianing+validation set without class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "MtdS2train_input = np.concatenate((mytrain_input_mtd, myvalid_input_mtd), axis=0)\n",
    "ptyS2train_input = np.concatenate((mytrain_input_pty, myvalid_input_pty), axis=0)\n",
    "txyS2train_input = np.concatenate((mytrain_input_txy, myvalid_input_txy), axis=0)\n",
    "\n",
    "S2train_output = np.concatenate((mytrain_output, myvalid_output), axis=0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "\n",
    "MtdS2_obs, MtdS2_pred, MtdS2_prob = evaluate(model_Mtd, device, MtdS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "ptyS2_obs, ptyS2_pred, ptyS2_prob = evaluate(model_pty, device, ptyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "txyS2_obs, txyS2_pred, txyS2_prob = evaluate(model_txy, device, txyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "x_valid = np.array(np.column_stack([MtdS2_prob, ptyS2_prob, txyS2_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(MtdS2_obs)/len(MtdS2_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag')\n",
    "L2Logistic_model.fit(x_valid, MtdS2_obs)\n",
    "\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "# x_test = np.array(np.transpose([Mtdtest_prob, ptytest_prob, txytest_prob, krdtest_prob])).reshape(-1, 3*2)\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55178019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1373679154658982\n",
      "0.8245614035087719\n",
      "0.7425531914893616\n",
      "[[85  9]\n",
      " [11  9]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use trianing + validation set with class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "MtdS2train_input = np.concatenate((mytrain_input_mtd, myvalid_input_mtd), axis=0)\n",
    "ptyS2train_input = np.concatenate((mytrain_input_pty, myvalid_input_pty), axis=0)\n",
    "txyS2train_input = np.concatenate((mytrain_input_txy, myvalid_input_txy), axis=0)\n",
    "\n",
    "S2train_output = np.concatenate((mytrain_output, myvalid_output), axis=0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "\n",
    "MtdS2_obs, MtdS2_pred, MtdS2_prob = evaluate(model_Mtd, device, MtdS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "ptyS2_obs, ptyS2_pred, ptyS2_prob = evaluate(model_pty, device, ptyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "txyS2_obs, txyS2_pred, txyS2_prob = evaluate(model_txy, device, txyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "x_valid = np.array(np.column_stack([MtdS2_prob, ptyS2_prob, txyS2_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(MtdS2_obs)/len(MtdS2_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag', class_weight=wt)\n",
    "L2Logistic_model.fit(x_valid, MtdS2_obs)\n",
    "\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "# x_test = np.array(np.transpose([Mtdtest_prob, ptytest_prob, txytest_prob, krdtest_prob])).reshape(-1, 3*2)\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
