{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1124b28",
   "metadata": {},
   "source": [
    "# Preterm Birth Prediction Microbiome Model Framework (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a2a71",
   "metadata": {},
   "source": [
    "Challenge website:\n",
    "https://www.synapse.org/#!Synapse:syn26133770/wiki/618018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabb89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from collections import Counter,defaultdict, OrderedDict\n",
    "from itertools import islice\n",
    "from joblib import dump, load\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dataset_splitID(meta_data, prop, myseed):\n",
    "    \n",
    "    subjects = list(np.unique(meta_data[\"participant_id\"]))\n",
    "    numsubjects = len(subjects)\n",
    "    \n",
    "    if myseed != None:\n",
    "        random.seed(myseed)\n",
    "\n",
    "    subjects_shuffle = random.sample(subjects, numsubjects)\n",
    "    \n",
    "    train_subjects = subjects_shuffle[0:(int(numsubjects*prop[0])+1)] \n",
    "    valid_subjects = subjects_shuffle[(int(numsubjects*prop[0])+2):(int(numsubjects*(prop[0]+prop[1]))+1)]\n",
    "    test_subjects = subjects_shuffle[(int(numsubjects*(prop[0]+prop[1]))+2):numsubjects]\n",
    "    \n",
    "    splitID_train = meta_data['participant_id'].isin(train_subjects)\n",
    "    splitID_valid = meta_data['participant_id'].isin(valid_subjects)\n",
    "    splitID_test = meta_data['participant_id'].isin(test_subjects)\n",
    "    \n",
    "    return splitID_train, splitID_valid, splitID_test\n",
    "\n",
    "\n",
    "# Possible, but not used here\n",
    "def dataset_pjt_splitID(meta_data, prop, myseed):\n",
    "    \n",
    "    projects = meta_data['project']\n",
    "\n",
    "    splitID_train = []\n",
    "    splitID_valid = []\n",
    "    splitID_test  = []\n",
    "    \n",
    "    for pjt in np.unique(projects):\n",
    "        \n",
    "        submeta = meta_data[projects == pjt]\n",
    "        subsubjects = list(np.unique(submeta[\"participant_id\"]))\n",
    "        numsub = len(subsubjects)\n",
    "        \n",
    "        subsubjects_shuffle = random.sample(subsubjects, numsub)\n",
    "        \n",
    "        train_subsubjects = subsubjects_shuffle[0:(int(numsub*prop[0])+1)] \n",
    "        valid_subsubjects = subsubjects_shuffle[(int(numsub*prop[0])+2):(int(numsub*(prop[0]+prop[1]))+1)]\n",
    "        test_subsubjects  = subsubjects_shuffle[(int(numsub*(prop[0]+prop[1]))+2):numsub]\n",
    "        \n",
    "        splitID_train.extend(submeta['participant_id'].isin(train_subsubjects))\n",
    "        splitID_valid.extend(submeta['participant_id'].isin(valid_subsubjects))\n",
    "        splitID_test.extend(submeta['participant_id'].isin(test_subsubjects))\n",
    "        \n",
    "    return splitID_train, splitID_valid, splitID_test\n",
    "\n",
    "\n",
    "def Data_Reshaper_Input(data, seq_length):\n",
    "    \n",
    "    numsubjects = len(np.unique(data['participant_id']))\n",
    "    myvary = list(data.columns.values)[2:data.shape[1]]\n",
    "    num_covariates = len(myvary)\n",
    "    \n",
    "    myinput = np.zeros((numsubjects, seq_length, num_covariates), dtype=np.float32)\n",
    "    for i in range(num_covariates):\n",
    "        data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=myvary[i])\n",
    "        data_wide = data_wide.sort_index(axis=1)\n",
    "        data_wide = data_wide.fillna(0)\n",
    "        tmpindex = data_wide._get_numeric_data().columns.values - 1\n",
    "        tmpindex = tmpindex.astype(int)\n",
    "        # time varying variables need to impute all and no records are denoted as 0\n",
    "        for j in range(numsubjects):\n",
    "                myinput[j,tmpindex,i] = data_wide.iloc[[j]]\n",
    "    return myinput\n",
    "\n",
    "\n",
    "\n",
    "def Data_Reshaper_Output_ManytoMany_0(data, seq_length, classlabel):\n",
    "\n",
    "    num_samples = len(np.unique(data['participant_id']))\n",
    "    \n",
    "    data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=classlabel)\n",
    "    data_wide = data_wide.sort_index(axis=1)\n",
    "    \n",
    "    myoutput = np.zeros((num_samples, seq_length, 2), dtype=np.float32)\n",
    "    for i in range(num_samples):\n",
    "        tmp = data_wide.iloc[i,:]\n",
    "        \n",
    "        if np.nanmax(tmp) == 1:\n",
    "            # label linear smoonthing from 0.5 to 1\n",
    "            # fill all position 1 to have final labels equal to 1\n",
    "            myoutput[i,:,0].fill(1)\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=1, num=seq_length)\n",
    "        else:\n",
    "            # label linear smoonthing from 0.5 to 0\n",
    "            # fill all position 0 to have final labels equal to 0 \n",
    "            #     but array alrady initialize as 0\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=0, num=seq_length)\n",
    "            \n",
    "        myoutput[i,:,1] = 1 - myoutput[i,:,0]\n",
    "    return myoutput\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, device, myinput, myoutput, finalperiod, cutoff=0.5):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # predicted labels\n",
    "    myinput  = torch.from_numpy(myinput).float().to(device)\n",
    "    myoutput_nn, hidden = model(myinput, device)\n",
    "    myoutput_nn = myoutput_nn.reshape((myoutput.shape))\n",
    "    output_prob = nn.functional.softmax(myoutput_nn, dim=2)\n",
    "    mypredprob = output_prob[:,finalperiod-1,:].cpu().detach().numpy()\n",
    "    mypred = 1*(mypredprob[:,0] > cutoff)\n",
    "    # observed labels\n",
    "    myobs  = myoutput[:,finalperiod-1,0]\n",
    "    \n",
    "    return myobs, mypred, mypredprob\n",
    "\n",
    "\n",
    "\n",
    "def metadata_loader(meta_dir, alpha_dir, cst_dir, task, finalperiod):\n",
    "    \n",
    "    meta_data = pd.DataFrame(pd.read_csv(meta_dir, delimiter=','))\n",
    "    meta_data.replace('Unknown', np.nan, inplace=True)\n",
    "    meta_data = meta_data[['participant_id', 'project', 'delivery_wk', 'collect_wk', 'age', 'race']]\n",
    "    \n",
    "    alpha_data = pd.DataFrame(pd.read_csv(alpha_dir, delimiter=','))\n",
    "    cst_data = pd.DataFrame(pd.read_csv(cst_dir, delimiter=','))\n",
    "    \n",
    "    meta_data = pd.concat([meta_data, alpha_data['shannon'], alpha_data['inv_simpson'], alpha_data['rooted_pd'], cst_data['CST']], axis=1)\n",
    "\n",
    "    for i in range(1,meta_data.shape[1]):\n",
    "        if meta_data.iloc[:,i].dtypes == object:\n",
    "            meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('category').cat.codes + 1\n",
    "            meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('float64')\n",
    "            \n",
    "    # create new variable 'collect_period'\n",
    "    meta_data['collect_period'] = 1\n",
    "    meta_data.loc[(meta_data['collect_wk']>=8)  & (meta_data['collect_wk']<=14),'collect_period'] = 2\n",
    "    meta_data.loc[(meta_data['collect_wk']>=15) & (meta_data['collect_wk']<=21),'collect_period'] = 3\n",
    "    meta_data.loc[(meta_data['collect_wk']>=22) & (meta_data['collect_wk']<=28),'collect_period'] = 4\n",
    "    meta_data.loc[(meta_data['collect_wk']>=29) & (meta_data['collect_wk']<=32),'collect_period'] = 5\n",
    "    meta_data.loc[(meta_data['collect_wk']>=33), 'collect_period']                                = 6\n",
    "    \n",
    "    # print(meta_data['collect_period'].value_counts())\n",
    "    \n",
    "    # create task class label\n",
    "    if task == \"was_preterm\":\n",
    "        meta_data[task] = 1*(meta_data['delivery_wk'] < 37)\n",
    "    elif task == \"was_early_preterm\":\n",
    "        meta_data[task] = 1*(meta_data['delivery_wk'] < 32)\n",
    "        \n",
    "    # Filtered out observations with \"collect_wk<=32\" OR \"collect_period<=5\" \n",
    "    # Filtered out observations with \"collect_wk<=28\" OR \"collect_period<=4\" \n",
    "    meta_data = meta_data[meta_data['collect_period']<=finalperiod]\n",
    "    # Average within each collection period\n",
    "    meta_data = meta_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "\n",
    "    return meta_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac44e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputLoader(data_dir, feature_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod):\n",
    "    \n",
    "    participant_id = meta_data['participant_id']\n",
    "    collect_period = meta_data['collect_period']\n",
    "   \n",
    "    Input_data = pd.DataFrame(pd.read_csv(data_dir, delimiter=','))\n",
    "    selectedfeature = pd.DataFrame(pd.read_csv(feature_dir, delimiter=','))\n",
    "    Input_data = Input_data.iloc[:,selectedfeature['id']]\n",
    "    Input_data = pd.concat([participant_id, collect_period, Input_data], axis=1)\n",
    "        \n",
    "    # Average within each collection period\n",
    "    Input_data = Input_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "    \n",
    "    Input_data_train = Input_data[trainID]\n",
    "    Input_data_valid = Input_data[validID]\n",
    "    Input_data_test  = Input_data[testID]\n",
    "    \n",
    "    print(\"## Input: train/valid/test (before reshape)\")\n",
    "    print(Input_data_train.shape)\n",
    "    print(Input_data_valid.shape)\n",
    "    print(Input_data_test.shape)\n",
    "    \n",
    "    #---- Input features reshaper ----#\n",
    "    mytrain_input = Data_Reshaper_Input(data=Input_data_train, seq_length=finalperiod)\n",
    "    myvalid_input = Data_Reshaper_Input(data=Input_data_valid, seq_length=finalperiod)\n",
    "    mytest_input  = Data_Reshaper_Input(data=Input_data_test, seq_length=finalperiod)\n",
    "    \n",
    "    print(\"## Input: train/valid/test (after reshape)\")\n",
    "    print(mytrain_input.shape)\n",
    "    print(myvalid_input.shape)\n",
    "    print(mytest_input.shape)\n",
    "    \n",
    "    return mytrain_input, myvalid_input, mytest_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190af89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputLoader(meta_data, trainID, validID, testID, task, finalperiod):\n",
    "    \n",
    "    meta_data_train = meta_data[trainID]\n",
    "    meta_data_valid = meta_data[validID]\n",
    "    meta_data_test  = meta_data[testID]\n",
    "    \n",
    "    print(\"################ Output: train/valid/test (before reshape)\")\n",
    "    print(meta_data_train.shape)\n",
    "    print(meta_data_valid.shape)\n",
    "    print(meta_data_test.shape)\n",
    "    \n",
    "    #---- Output label reshaper ----#\n",
    "    mytrain_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_train, seq_length=finalperiod, classlabel=task)\n",
    "    myvalid_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_valid, seq_length=finalperiod, classlabel=task)\n",
    "    mytest_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_test, seq_length=finalperiod, classlabel=task)\n",
    "    \n",
    "    print(\"################ Output: train/valid/test (after reshape)\")\n",
    "    print(mytrain_output.shape)\n",
    "    print(myvalid_output.shape)\n",
    "    print(mytest_output.shape)\n",
    "    \n",
    "    return mytrain_output, myvalid_output, mytest_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3947df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputLoaderMtd(meta_data, trainID, validID, testID, task, finalperiod):\n",
    "    \n",
    "    meta_data_train = meta_data[trainID]\n",
    "    meta_data_valid = meta_data[validID]\n",
    "    meta_data_test  = meta_data[testID]\n",
    "    \n",
    "    #---- Input features reshaper ----#\n",
    "    mytrain_input_mtd = meta_data_train.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    myvalid_input_mtd = meta_data_valid.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    mytest_input_mtd  = meta_data_test.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    \n",
    "    # scale the input features in this data set\n",
    "    columns = ['collect_wk', 'age', 'race', 'shannon', 'inv_simpson', 'rooted_pd', 'CST']\n",
    "    for col in columns:\n",
    "        mytrain_input_mtd[col] = MinMaxScaler().fit_transform(np.array(mytrain_input_mtd[col]).reshape(-1,1))\n",
    "        myvalid_input_mtd[col] = MinMaxScaler().fit_transform(np.array(myvalid_input_mtd[col]).reshape(-1,1))\n",
    "        mytest_input_mtd[col]  = MinMaxScaler().fit_transform(np.array(mytest_input_mtd[col]).reshape(-1,1))\n",
    "    \n",
    "    print(\"## Input: train/valid/test (before reshape)\")\n",
    "    print(mytrain_input_mtd.shape)\n",
    "    print(myvalid_input_mtd.shape)\n",
    "    print(mytest_input_mtd.shape)\n",
    "    \n",
    "    mytrain_input_mtd = Data_Reshaper_Input(data=mytrain_input_mtd, seq_length=finalperiod)\n",
    "    myvalid_input_mtd = Data_Reshaper_Input(data=myvalid_input_mtd, seq_length=finalperiod)\n",
    "    mytest_input_mtd  = Data_Reshaper_Input(data=mytest_input_mtd,  seq_length=finalperiod) \n",
    "    \n",
    "    print(\"## Input: train/valid/test (after reshape)\")\n",
    "    print(mytrain_input_mtd.shape)\n",
    "    print(myvalid_input_mtd.shape)\n",
    "    print(mytest_input_mtd.shape)\n",
    "    \n",
    "    return mytrain_input_mtd, myvalid_input_mtd, mytest_input_mtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c901b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMtrain(model, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, finalperiod, patience, earlystop='loss', verbose=True):\n",
    "    \n",
    "    # training and validation set class proportion\n",
    "    trainprior = sum(mytrain_output[:,finalperiod-1,0])/mytrain_output.shape[0]\n",
    "    class1ID_train = mytrain_output[:,finalperiod-1,0] == 1\n",
    "    class2ID_train = mytrain_output[:,finalperiod-1,0] == 0\n",
    "    \n",
    "    validprior = sum(myvalid_output[:,finalperiod-1,0])/myvalid_output.shape[0]\n",
    "    class1ID_valid = myvalid_output[:,finalperiod-1,0] == 1\n",
    "    class2ID_valid = myvalid_output[:,finalperiod-1,0] == 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Track the value of the loss function and model accuracy across epochs\n",
    "    history_train_valid = {'TrainLoss': [], 'TrainAcc': [], 'TrainAUC': [],\n",
    "                           'ValidLoss': [], 'ValidAcc': [], 'ValidAUC': []}\n",
    "    \n",
    "    # Same reshaped Validation set for each epoch    \n",
    "    myvalid_input  = torch.from_numpy(myvalid_input).float().to(device)\n",
    "    myvalid_output = torch.from_numpy(myvalid_output).float().to(device)\n",
    "        \n",
    "    valid_loss_min = np.inf\n",
    "    valid_losses = []\n",
    "    \n",
    "    valid_auc_max = np.NINF\n",
    "    valid_auces = []\n",
    "    \n",
    "    last_valid_loss = 100\n",
    "    last_valid_auc  = 100\n",
    "    \n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        #----  shuffle the training set to avoid the batch(project) effects ----#\n",
    "        shuffleindex = list(range(mytrain_output.shape[0]))\n",
    "        random.shuffle(shuffleindex)\n",
    "        mytrain_output = mytrain_output[shuffleindex]\n",
    "        mytrain_input = mytrain_input[shuffleindex]\n",
    "        \n",
    "        #-------------- Batch-wise training model --------------#\n",
    "        model.train()\n",
    "        # train_loss = 0.0\n",
    "        train_num_correct = 0\n",
    "        train_prob = []\n",
    "        for batch_idx in range(0, mytrain_input.shape[0], batch_size):\n",
    "            \n",
    "            # subset a batch of sequences and class labels\n",
    "            tmpindex = list(range(batch_idx, min(batch_idx+batch_size, mytrain_input.shape[0])))\n",
    "            mytrain_input_batch  = mytrain_input[tmpindex,:]\n",
    "            mytrain_output_batch = mytrain_output[tmpindex,:]\n",
    "            \n",
    "            batchprior = sum(mytrain_output_batch[:,finalperiod-1,0])/mytrain_output_batch.shape[0]\n",
    "            class1ID_batch = mytrain_output_batch[:,finalperiod-1,0] == 1\n",
    "            class2ID_batch = mytrain_output_batch[:,finalperiod-1,0] == 0\n",
    "            \n",
    "            mytrain_input_batch  = torch.from_numpy(mytrain_input_batch).float().to(device)\n",
    "            mytrain_output_batch = torch.from_numpy(mytrain_output_batch).float().to(device)\n",
    "            \n",
    "            # forward pass of RNN model\n",
    "            output, hidden = model(mytrain_input_batch, device)\n",
    "            output = output.reshape((mytrain_output_batch.shape))\n",
    "            output_prob = nn.functional.softmax(output, dim=2)\n",
    "            # weighted MSE\n",
    "            loss = batchprior*criterion(output_prob[class1ID_batch,:,0], mytrain_output_batch[class1ID_batch,:,0]) + (1-batchprior)*criterion(output_prob[class2ID_batch,:,1], mytrain_output_batch[class2ID_batch,:,1])\n",
    "            # loss = criterion(output_prob, mytrain_output_batch)\n",
    "            # Clear existing gradients from previous epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Does backpropagation and calculates gradients\n",
    "            loss.backward()\n",
    "            # Updates the weights accordingly\n",
    "            optimizer.step()\n",
    "            # Number correct prediction on trainning set collection\n",
    "            tmppred = 1*(output_prob[:,finalperiod-1,0] > 0.5)\n",
    "            train_num_correct += sum(1*(tmppred == mytrain_output_batch[:,finalperiod-1,0]))\n",
    "            # Training function loss collection\n",
    "            # train_loss += loss.item()\n",
    "            train_prob = np.concatenate((train_prob, output_prob[:,finalperiod-1,0].cpu().detach().numpy()), axis=None)\n",
    "            \n",
    "        train_acc = (float(train_num_correct) / len(mytrain_output))*100\n",
    "        train_auc = metrics.roc_auc_score(mytrain_output[:,finalperiod-1,0], train_prob)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Training loss calculation\n",
    "        tmpmytrain_input  = torch.from_numpy(mytrain_input).float().to(device)\n",
    "        tmpmytrain_output = torch.from_numpy(mytrain_output).float().to(device)\n",
    "        tmpoutputtrain, tmphidden = model(tmpmytrain_input, device)\n",
    "        tmpoutputtrain = tmpoutputtrain.reshape((tmpmytrain_output.shape))\n",
    "        tmpoutputtrain_prob = nn.functional.softmax(tmpoutputtrain, dim=2)\n",
    "        # train_loss = criterion(tmpoutputtrain_prob, tmpmytrain_output)\n",
    "        train_loss = trainprior*criterion(tmpoutputtrain_prob[class1ID_train,:,0], tmpmytrain_output[class1ID_train,:,0]) + (1-trainprior)*criterion(tmpoutputtrain_prob[class2ID_train,:,1], tmpmytrain_output[class2ID_train,:,1])\n",
    "        history_train_valid['TrainLoss'].append(train_loss.item())\n",
    "        history_train_valid['TrainAcc'].append(train_acc)\n",
    "        history_train_valid['TrainAUC'].append(train_auc)\n",
    "        \n",
    "\n",
    "        #--------------       Validate model      --------------#\n",
    "        outputvalid, hidden = model(myvalid_input, device)\n",
    "        outputvalid = outputvalid.reshape((myvalid_output.shape))\n",
    "        outputvalid_prob = nn.functional.softmax(outputvalid, dim=2)\n",
    "        # validation loss\n",
    "        # valid_loss = criterion(outputvalid_prob, myvalid_output)\n",
    "        valid_loss = validprior*criterion(outputvalid_prob[class1ID_valid,:,0], myvalid_output[class1ID_valid,:,0]) + (1-validprior)*criterion(outputvalid_prob[class2ID_valid,:,1], myvalid_output[class2ID_valid,:,1])\n",
    "        # Number correct prediction on trainning set collection\n",
    "        tmppredprob = outputvalid_prob[:,finalperiod-1,0].cpu().detach().numpy()\n",
    "        tmppred = 1*(tmppredprob > 0.5)\n",
    "        tmpobs = myvalid_output[:,finalperiod-1,0].cpu().detach().numpy()\n",
    "        valid_num_correct = sum(1*(tmppred == tmpobs))\n",
    "        valid_acc = (float(valid_num_correct) / len(myvalid_output))*100\n",
    "        valid_auc = metrics.roc_auc_score(tmpobs, tmppredprob)\n",
    "        \n",
    "        history_train_valid['ValidLoss'].append(valid_loss.item())\n",
    "        history_train_valid['ValidAcc'].append(valid_acc)\n",
    "        history_train_valid['ValidAUC'].append(valid_auc)\n",
    "        \n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[E {epoch + 1}/{max_epochs}]'\n",
    "                  f\" T.Loss: {history_train_valid['TrainLoss'][-1]:.4f}, T.Acc: {history_train_valid['TrainAcc'][-1]:2.2f}, T.AUC: {history_train_valid['TrainAUC'][-1]:.4f}\"\n",
    "                  f\" V.Loss: {history_train_valid['ValidLoss'][-1]:.4f}, V.Acc: {history_train_valid['ValidAcc'][-1]:2.2f}, V.AUC: {history_train_valid['ValidAUC'][-1]:.4f};\")\n",
    "        \n",
    "        valid_auces.append(valid_auc.item())\n",
    "        valid_losses.append(valid_loss.item())\n",
    "        \n",
    "        if earlystop == \"auc\":\n",
    "            current_valid_auc = valid_auc\n",
    "            if current_valid_auc < last_valid_auc:\n",
    "                trigger_times += 1\n",
    "                print('AUC Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping by AUC!.')\n",
    "                    break\n",
    "            else:\n",
    "                print('trigger times: 0')\n",
    "                trigger_times = 0\n",
    "            last_valid_auc = np.mean(valid_auces[-10:])\n",
    "        elif earlystop == \"loss\":\n",
    "            current_valid_loss = valid_loss\n",
    "            if current_valid_loss > last_valid_loss:\n",
    "                trigger_times += 1\n",
    "                print('Loss Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping by LOSS!.')\n",
    "                    break\n",
    "            else:\n",
    "                print('Trigger times >= patience: 0')\n",
    "                trigger_times = 0\n",
    "            last_valid_loss = np.mean(valid_losses[-10:])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # if earlystop == \"auc\":\n",
    "        #     # start to considering early-stop after 20 epoch\n",
    "        #     if epoch > 20:\n",
    "        #        if np.mean(valid_auces) < valid_auc_max:\n",
    "        #            print(\"Stopped here by AUC!\")\n",
    "        #            break\n",
    "        #        valid_auc_max = np.mean(valid_auces)\n",
    "        # elif earlystop == \"loss\":\n",
    "        #    # start to considering early-stop after 20 epoch\n",
    "        #    if epoch > 20:\n",
    "        #        if np.mean(valid_losses) > valid_loss_min:\n",
    "        #            print(\"Stopped here by LOSS!\")\n",
    "        #            break\n",
    "        #        # valid_loss_min = np.mean(valid_losses[-20:])\n",
    "        #        valid_loss_min = np.mean(valid_losses)\n",
    "        \n",
    "    return history_train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53778d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Mtd(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_Mtd, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=output_size, bias=False)\n",
    "\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "            \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "\n",
    "        outp = self.fc_1(outp)\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daed6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_Mtd(mytrain_input_mtd, mytrain_output, myvalid_input_mtd, myvalid_output, mytest_input_mtd, mytest_output, finalperiod):\n",
    "    \n",
    "    # 7 -> lstm -> 16 -> 8\n",
    "    \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_mtd.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 8\n",
    "    fc_size     = [16]\n",
    "    n_layers    = 1\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 200\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_Mtd = Model_Mtd(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, \n",
    "                          n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_Mtd.parameters(), lr=lr) \n",
    "    \n",
    "    print(\"################ Mtd LSTM training...\")\n",
    "    Mtd_hist = LSTMtrain(model_Mtd, device, criterion, optimizer, mytrain_input_mtd, mytrain_output, \n",
    "                         myvalid_input_mtd, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    Mtd_obs, Mtd_pred, Mtd_prob = evaluate(model_Mtd, device, mytest_input_mtd, mytest_output, finalperiod, cutoff=0.5)\n",
    "    Mtdtest_auc = metrics.roc_auc_score(Mtd_obs, Mtd_prob[:,0])\n",
    "    Mtdtest_acc = metrics.accuracy_score(Mtd_obs, Mtd_pred)\n",
    "    Mtdtest_conf = metrics.confusion_matrix(Mtd_obs, Mtd_pred)\n",
    "\n",
    "    return model_Mtd, Mtd_hist, Mtd_obs, Mtd_pred, Mtd_prob, Mtdtest_auc, Mtdtest_acc, Mtdtest_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5816b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_pty(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_pty, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=fc_size[2], bias=False)\n",
    "        self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        \n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_4(outp)   # 4th Ouuput\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910645cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_pty(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod):\n",
    "   \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_pty.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 128\n",
    "    n_layers    = 1\n",
    "    fc_size     = [256, 128, 64]\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.0001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 20\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_pty = Model_pty(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, \n",
    "                          n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_pty.parameters(), lr=lr) \n",
    "    \n",
    "    #---- training lstm ----#\n",
    "    print(\"################ pty LSTM training...\")\n",
    "    pty_hist = LSTMtrain(model_pty, device, criterion, optimizer, mytrain_input_pty, mytrain_output, \n",
    "                         myvalid_input_pty, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    pty_obs, pty_pred, pty_prob = evaluate(model_pty, device, mytest_input_pty, mytest_output, finalperiod, cutoff=0.5)\n",
    "    ptytest_auc = metrics.roc_auc_score(pty_obs, pty_prob[:,0])\n",
    "    ptytest_acc = metrics.accuracy_score(pty_obs, pty_pred)\n",
    "    ptytest_conf = metrics.confusion_matrix(pty_obs, pty_pred)\n",
    "\n",
    "    return model_pty, pty_hist, pty_obs, pty_pred, pty_prob, ptytest_auc, ptytest_acc, ptytest_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73926e9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModel_txy\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Model_txy, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Model_txy(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_txy, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=fc_size[2], bias=False)\n",
    "        self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_4(outp)   # 4th Ouuput\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "426a2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_txy(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod):\n",
    "    \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_txy.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 128\n",
    "    n_layers    = 1\n",
    "    fc_size     = [256, 128, 64]\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.0001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 200\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_txy = Model_txy(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, \n",
    "                          seq_len=seq_len, n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_txy.parameters(), lr=lr) \n",
    "    \n",
    "    print(\"################ txy LSTM training...\")\n",
    "    txy_hist = LSTMtrain(model_txy, device, criterion, optimizer, mytrain_input_txy, mytrain_output, \n",
    "                         myvalid_input_txy, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    txy_obs, txy_pred, txy_prob = evaluate(model_txy, device, mytest_input_txy, mytest_output, finalperiod, cutoff=0.5)\n",
    "    txytest_auc = metrics.roc_auc_score(txy_obs, txy_prob[:,0])\n",
    "    txytest_acc = metrics.accuracy_score(txy_obs, txy_pred)\n",
    "    txytest_conf = metrics.confusion_matrix(txy_obs, txy_pred)\n",
    "\n",
    "    return model_txy, txy_hist, txy_obs, txy_pred, txy_prob, txytest_auc, txytest_acc, txytest_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0661a",
   "metadata": {},
   "source": [
    "# Main script start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15b65d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Output: train/valid/test (before reshape)\n",
      "(998, 12)\n",
      "(488, 12)\n",
      "(162, 12)\n",
      "################ Output: train/valid/test (after reshape)\n",
      "(729, 5, 2)\n",
      "(363, 5, 2)\n",
      "(120, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "# data directory\n",
    "# meta_dir      = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/metadata/metadata.csv'\n",
    "meta_dir      = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/metadata_imputed1.csv'\n",
    "alpha_dir     = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/alpha_diversity/alpha_diversity.csv'\n",
    "cst_dir       = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/community_state_types/cst_valencia.csv'\n",
    "\n",
    "txy_dir_fam = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.family.csv'\n",
    "txy_dir_gen = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.genus.csv'\n",
    "txy_dir_spe = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.species.csv'\n",
    "\n",
    "pty_dir_1dot = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.1e0.csv'\n",
    "pty_dir_dot5 = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.5e_1.csv'\n",
    "pty_dir_dot1 = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.1e_1.csv'\n",
    "\n",
    "# krdwide_dir   = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/pairwise_distance/krd_distance_wide.csv'\n",
    "\n",
    "\n",
    "txy_dir = txy_dir_gen\n",
    "txy_feature_dir = \"/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/selectedfeature/txyfeature_was_preterm_gen.csv\"\n",
    "\n",
    "pty_dir = pty_dir_dot5\n",
    "pty_feature_dir = \"/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/selectedfeature/ptyfeature_was_preterm_dot5.csv\"\n",
    "\n",
    "task = \"was_preterm\"\n",
    "finalperiod = 5\n",
    "# task = \"was_early_preterm\"\n",
    "# finalperiod = 4\n",
    "\n",
    "myprop = [0.6, 0.3, 0.1]\n",
    "myseed = 0\n",
    "\n",
    "\n",
    "#-------------------------------------------#\n",
    "#---- Data Preparation                  ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "meta_data = metadata_loader(meta_dir, alpha_dir, cst_dir, task, finalperiod)\n",
    "\n",
    "#---- data set splitter ----#\n",
    "trainID, validID, testID = dataset_splitID(meta_data=meta_data, prop=myprop, myseed=myseed)\n",
    "\n",
    "#---- output loader ----#\n",
    "mytrain_output, myvalid_output, mytest_output = OutputLoader(meta_data, trainID, validID, testID, task, finalperiod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17876d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ meta:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(998, 9)\n",
      "(488, 9)\n",
      "(162, 9)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(729, 5, 7)\n",
      "(363, 5, 7)\n",
      "(120, 5, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ meta:\")\n",
    "mytrain_input_mtd, myvalid_input_mtd, mytest_input_mtd = InputLoaderMtd(meta_data, trainID, validID, testID, task, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803ac03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ pty:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(998, 258)\n",
      "(488, 258)\n",
      "(162, 258)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(729, 5, 256)\n",
      "(363, 5, 256)\n",
      "(120, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ pty:\")\n",
    "mytrain_input_pty, myvalid_input_pty, mytest_input_pty = InputLoader(pty_dir, pty_feature_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "815b14aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ txy:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(998, 258)\n",
      "(488, 258)\n",
      "(162, 258)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(729, 5, 256)\n",
      "(363, 5, 256)\n",
      "(120, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ txy:\")\n",
    "mytrain_input_txy, myvalid_input_txy, mytest_input_txy = InputLoader(txy_dir, txy_feature_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06738bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Mtd LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0915, T.Acc: 43.07, T.AUC: 0.5451 V.Loss: 0.0920, V.Acc: 65.84, V.AUC: 0.5668;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0901, T.Acc: 63.65, T.AUC: 0.5350 V.Loss: 0.0908, V.Acc: 65.01, V.AUC: 0.5690;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0886, T.Acc: 67.35, T.AUC: 0.4985 V.Loss: 0.0896, V.Acc: 65.84, V.AUC: 0.5681;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0871, T.Acc: 68.72, T.AUC: 0.5124 V.Loss: 0.0884, V.Acc: 65.84, V.AUC: 0.5689;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0855, T.Acc: 69.41, T.AUC: 0.5191 V.Loss: 0.0873, V.Acc: 65.84, V.AUC: 0.5698;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0840, T.Acc: 69.27, T.AUC: 0.5186 V.Loss: 0.0862, V.Acc: 65.84, V.AUC: 0.5717;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0825, T.Acc: 69.27, T.AUC: 0.5217 V.Loss: 0.0853, V.Acc: 65.84, V.AUC: 0.5744;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0814, T.Acc: 69.27, T.AUC: 0.5294 V.Loss: 0.0847, V.Acc: 65.84, V.AUC: 0.5794;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0805, T.Acc: 69.27, T.AUC: 0.5376 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.5848;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0800, T.Acc: 69.27, T.AUC: 0.5066 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.5900;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5360 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.5977;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5286 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6059;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5020 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6181;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5668 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6288;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5382 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6393;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5618 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6494;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.5480 V.Loss: 0.0835, V.Acc: 65.84, V.AUC: 0.6584;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.5668 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6664;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.5390 V.Loss: 0.0832, V.Acc: 65.84, V.AUC: 0.6714;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0787, T.Acc: 69.27, T.AUC: 0.5972 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6773;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.5706 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6811;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0785, T.Acc: 69.27, T.AUC: 0.5741 V.Loss: 0.0825, V.Acc: 65.84, V.AUC: 0.6843;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0784, T.Acc: 69.27, T.AUC: 0.5914 V.Loss: 0.0822, V.Acc: 65.84, V.AUC: 0.6880;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.6072 V.Loss: 0.0821, V.Acc: 65.84, V.AUC: 0.6894;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0781, T.Acc: 69.27, T.AUC: 0.6178 V.Loss: 0.0819, V.Acc: 65.84, V.AUC: 0.6907;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0779, T.Acc: 69.27, T.AUC: 0.6040 V.Loss: 0.0818, V.Acc: 65.84, V.AUC: 0.6918;\n",
      "Trigger times >= patience: 0\n",
      "[E 27/2000] T.Loss: 0.0778, T.Acc: 69.27, T.AUC: 0.6137 V.Loss: 0.0815, V.Acc: 65.84, V.AUC: 0.6923;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0776, T.Acc: 69.27, T.AUC: 0.5988 V.Loss: 0.0812, V.Acc: 65.84, V.AUC: 0.6935;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0775, T.Acc: 69.27, T.AUC: 0.6155 V.Loss: 0.0809, V.Acc: 65.84, V.AUC: 0.6947;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0773, T.Acc: 69.27, T.AUC: 0.6131 V.Loss: 0.0805, V.Acc: 65.84, V.AUC: 0.6962;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0771, T.Acc: 69.27, T.AUC: 0.6497 V.Loss: 0.0803, V.Acc: 65.84, V.AUC: 0.6980;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0769, T.Acc: 69.14, T.AUC: 0.6302 V.Loss: 0.0800, V.Acc: 65.84, V.AUC: 0.6987;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0768, T.Acc: 69.14, T.AUC: 0.6357 V.Loss: 0.0799, V.Acc: 65.84, V.AUC: 0.6996;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0766, T.Acc: 69.00, T.AUC: 0.6518 V.Loss: 0.0795, V.Acc: 65.84, V.AUC: 0.6997;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0764, T.Acc: 69.00, T.AUC: 0.6351 V.Loss: 0.0793, V.Acc: 65.84, V.AUC: 0.7004;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0762, T.Acc: 69.00, T.AUC: 0.6392 V.Loss: 0.0791, V.Acc: 65.84, V.AUC: 0.7007;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0760, T.Acc: 69.00, T.AUC: 0.6494 V.Loss: 0.0786, V.Acc: 65.84, V.AUC: 0.7009;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0758, T.Acc: 69.00, T.AUC: 0.6449 V.Loss: 0.0782, V.Acc: 65.84, V.AUC: 0.7011;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0757, T.Acc: 68.86, T.AUC: 0.6503 V.Loss: 0.0778, V.Acc: 65.84, V.AUC: 0.7011;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0755, T.Acc: 68.86, T.AUC: 0.6465 V.Loss: 0.0775, V.Acc: 65.84, V.AUC: 0.7021;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0753, T.Acc: 68.86, T.AUC: 0.6547 V.Loss: 0.0774, V.Acc: 65.84, V.AUC: 0.7018;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0751, T.Acc: 68.86, T.AUC: 0.6575 V.Loss: 0.0772, V.Acc: 66.12, V.AUC: 0.7022;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0750, T.Acc: 68.86, T.AUC: 0.6475 V.Loss: 0.0771, V.Acc: 66.12, V.AUC: 0.7026;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0748, T.Acc: 68.86, T.AUC: 0.6478 V.Loss: 0.0769, V.Acc: 66.67, V.AUC: 0.7024;\n",
      "Trigger times >= patience: 0\n",
      "[E 45/2000] T.Loss: 0.0747, T.Acc: 68.86, T.AUC: 0.6521 V.Loss: 0.0767, V.Acc: 66.67, V.AUC: 0.7022;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0746, T.Acc: 68.86, T.AUC: 0.6559 V.Loss: 0.0764, V.Acc: 66.39, V.AUC: 0.7025;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0744, T.Acc: 68.45, T.AUC: 0.6682 V.Loss: 0.0760, V.Acc: 67.22, V.AUC: 0.7028;\n",
      "Trigger times >= patience: 0\n",
      "[E 48/2000] T.Loss: 0.0743, T.Acc: 69.68, T.AUC: 0.6634 V.Loss: 0.0760, V.Acc: 67.22, V.AUC: 0.7030;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0742, T.Acc: 69.14, T.AUC: 0.6590 V.Loss: 0.0759, V.Acc: 67.22, V.AUC: 0.7028;\n",
      "Trigger times >= patience: 0\n",
      "[E 50/2000] T.Loss: 0.0740, T.Acc: 69.55, T.AUC: 0.6537 V.Loss: 0.0758, V.Acc: 67.22, V.AUC: 0.7026;\n",
      "Trigger times >= patience: 0\n",
      "[E 51/2000] T.Loss: 0.0739, T.Acc: 69.68, T.AUC: 0.6576 V.Loss: 0.0757, V.Acc: 67.22, V.AUC: 0.7025;\n",
      "Trigger times >= patience: 0\n",
      "[E 52/2000] T.Loss: 0.0738, T.Acc: 70.10, T.AUC: 0.6581 V.Loss: 0.0755, V.Acc: 68.60, V.AUC: 0.7026;\n",
      "Trigger times >= patience: 0\n",
      "[E 53/2000] T.Loss: 0.0737, T.Acc: 70.23, T.AUC: 0.6606 V.Loss: 0.0752, V.Acc: 69.15, V.AUC: 0.7021;\n",
      "Trigger times >= patience: 0\n",
      "[E 54/2000] T.Loss: 0.0736, T.Acc: 70.51, T.AUC: 0.6566 V.Loss: 0.0751, V.Acc: 69.15, V.AUC: 0.7020;\n",
      "Trigger times >= patience: 0\n",
      "[E 55/2000] T.Loss: 0.0735, T.Acc: 70.78, T.AUC: 0.6673 V.Loss: 0.0751, V.Acc: 69.70, V.AUC: 0.7018;\n",
      "Trigger times >= patience: 0\n",
      "[E 56/2000] T.Loss: 0.0734, T.Acc: 70.51, T.AUC: 0.6583 V.Loss: 0.0751, V.Acc: 69.70, V.AUC: 0.7020;\n",
      "Trigger times >= patience: 0\n",
      "[E 57/2000] T.Loss: 0.0733, T.Acc: 71.06, T.AUC: 0.6559 V.Loss: 0.0747, V.Acc: 69.97, V.AUC: 0.7015;\n",
      "Trigger times >= patience: 0\n",
      "[E 58/2000] T.Loss: 0.0732, T.Acc: 70.92, T.AUC: 0.6747 V.Loss: 0.0747, V.Acc: 69.97, V.AUC: 0.7014;\n",
      "Trigger times >= patience: 0\n",
      "[E 59/2000] T.Loss: 0.0731, T.Acc: 70.92, T.AUC: 0.6531 V.Loss: 0.0747, V.Acc: 69.97, V.AUC: 0.7009;\n",
      "Trigger times >= patience: 0\n",
      "[E 60/2000] T.Loss: 0.0731, T.Acc: 70.92, T.AUC: 0.6555 V.Loss: 0.0746, V.Acc: 69.70, V.AUC: 0.7001;\n",
      "Trigger times >= patience: 0\n",
      "[E 61/2000] T.Loss: 0.0730, T.Acc: 70.23, T.AUC: 0.6702 V.Loss: 0.0745, V.Acc: 69.97, V.AUC: 0.6999;\n",
      "Trigger times >= patience: 0\n",
      "[E 62/2000] T.Loss: 0.0729, T.Acc: 71.33, T.AUC: 0.6677 V.Loss: 0.0746, V.Acc: 69.97, V.AUC: 0.6988;\n",
      "Trigger times >= patience: 0\n",
      "[E 63/2000] T.Loss: 0.0729, T.Acc: 71.19, T.AUC: 0.6618 V.Loss: 0.0745, V.Acc: 69.97, V.AUC: 0.6989;\n",
      "Trigger times >= patience: 0\n",
      "[E 64/2000] T.Loss: 0.0728, T.Acc: 71.74, T.AUC: 0.6578 V.Loss: 0.0747, V.Acc: 69.70, V.AUC: 0.6980;\n",
      "Trigger times >= patience: 0\n",
      "[E 65/2000] T.Loss: 0.0727, T.Acc: 71.88, T.AUC: 0.6646 V.Loss: 0.0745, V.Acc: 70.25, V.AUC: 0.6977;\n",
      "Trigger times >= patience: 0\n",
      "[E 66/2000] T.Loss: 0.0727, T.Acc: 70.92, T.AUC: 0.6657 V.Loss: 0.0744, V.Acc: 70.25, V.AUC: 0.6978;\n",
      "Trigger times >= patience: 0\n",
      "[E 67/2000] T.Loss: 0.0726, T.Acc: 71.19, T.AUC: 0.6620 V.Loss: 0.0745, V.Acc: 70.25, V.AUC: 0.6974;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 68/2000] T.Loss: 0.0725, T.Acc: 71.33, T.AUC: 0.6575 V.Loss: 0.0745, V.Acc: 70.25, V.AUC: 0.6971;\n",
      "Trigger times >= patience: 0\n",
      "[E 69/2000] T.Loss: 0.0725, T.Acc: 71.19, T.AUC: 0.6691 V.Loss: 0.0745, V.Acc: 70.25, V.AUC: 0.6971;\n",
      "Trigger times >= patience: 0\n",
      "[E 70/2000] T.Loss: 0.0724, T.Acc: 71.33, T.AUC: 0.6660 V.Loss: 0.0743, V.Acc: 70.80, V.AUC: 0.6968;\n",
      "Trigger times >= patience: 0\n",
      "[E 71/2000] T.Loss: 0.0724, T.Acc: 71.88, T.AUC: 0.6551 V.Loss: 0.0744, V.Acc: 70.80, V.AUC: 0.6970;\n",
      "Trigger times >= patience: 0\n",
      "[E 72/2000] T.Loss: 0.0723, T.Acc: 71.74, T.AUC: 0.6615 V.Loss: 0.0742, V.Acc: 70.80, V.AUC: 0.6970;\n",
      "Trigger times >= patience: 0\n",
      "[E 73/2000] T.Loss: 0.0723, T.Acc: 71.88, T.AUC: 0.6583 V.Loss: 0.0741, V.Acc: 70.80, V.AUC: 0.6968;\n",
      "Trigger times >= patience: 0\n",
      "[E 74/2000] T.Loss: 0.0722, T.Acc: 72.15, T.AUC: 0.6549 V.Loss: 0.0741, V.Acc: 70.80, V.AUC: 0.6967;\n",
      "Trigger times >= patience: 0\n",
      "[E 75/2000] T.Loss: 0.0721, T.Acc: 71.88, T.AUC: 0.6707 V.Loss: 0.0742, V.Acc: 70.80, V.AUC: 0.6960;\n",
      "Trigger times >= patience: 0\n",
      "[E 76/2000] T.Loss: 0.0721, T.Acc: 71.74, T.AUC: 0.6548 V.Loss: 0.0742, V.Acc: 70.80, V.AUC: 0.6955;\n",
      "Trigger times >= patience: 0\n",
      "[E 77/2000] T.Loss: 0.0721, T.Acc: 72.02, T.AUC: 0.6604 V.Loss: 0.0743, V.Acc: 70.80, V.AUC: 0.6950;\n",
      "Trigger times >= patience: 0\n",
      "[E 78/2000] T.Loss: 0.0720, T.Acc: 72.02, T.AUC: 0.6584 V.Loss: 0.0743, V.Acc: 70.80, V.AUC: 0.6948;\n",
      "Loss Trigger Times: 1\n",
      "[E 79/2000] T.Loss: 0.0720, T.Acc: 71.88, T.AUC: 0.6472 V.Loss: 0.0743, V.Acc: 70.80, V.AUC: 0.6949;\n",
      "Loss Trigger Times: 2\n",
      "[E 80/2000] T.Loss: 0.0719, T.Acc: 72.29, T.AUC: 0.6721 V.Loss: 0.0743, V.Acc: 70.80, V.AUC: 0.6949;\n",
      "Loss Trigger Times: 3\n",
      "[E 81/2000] T.Loss: 0.0719, T.Acc: 71.47, T.AUC: 0.6670 V.Loss: 0.0744, V.Acc: 70.80, V.AUC: 0.6945;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.7583333333333333\n",
      "0.7777091694967583\n",
      "[[76  3]\n",
      " [26 15]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/Mtd_waspreterm.save']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: Metadata             ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_Mtd, Mtd_hist, Mtdtest_obs, Mtdtest_pred, Mtdtest_prob, Mtdtest_auc, Mtdtest_acc, Mtdtest_conf = FirstStage_Mtd(mytrain_input_mtd, mytrain_output, myvalid_input_mtd, myvalid_output, mytest_input_mtd, mytest_output, finalperiod)\n",
    "print(Mtdtest_acc)\n",
    "print(Mtdtest_auc)\n",
    "print(Mtdtest_conf)\n",
    "dump(model_Mtd, '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/Mtd_waspreterm.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d9b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ pty LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0815, T.Acc: 69.27, T.AUC: 0.4731 V.Loss: 0.0848, V.Acc: 65.84, V.AUC: 0.4635;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0798, T.Acc: 69.27, T.AUC: 0.4878 V.Loss: 0.0849, V.Acc: 65.84, V.AUC: 0.5257;\n",
      "Loss Trigger Times: 1\n",
      "[E 3/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.4626 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.5760;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.4673 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.5810;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.4860 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.5935;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5162 V.Loss: 0.0849, V.Acc: 65.84, V.AUC: 0.5945;\n",
      "Loss Trigger Times: 1\n",
      "[E 7/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.4832 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6095;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5390 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.6133;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5412 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.6160;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5128 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6163;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5468 V.Loss: 0.0847, V.Acc: 65.84, V.AUC: 0.6223;\n",
      "Loss Trigger Times: 1\n",
      "[E 12/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.5183 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6179;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.5657 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6174;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.5915 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6211;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0787, T.Acc: 69.27, T.AUC: 0.5772 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6232;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0785, T.Acc: 69.27, T.AUC: 0.5709 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6258;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0783, T.Acc: 69.27, T.AUC: 0.6087 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6273;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.6262 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6291;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0780, T.Acc: 69.27, T.AUC: 0.5853 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6305;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0777, T.Acc: 69.27, T.AUC: 0.6219 V.Loss: 0.0830, V.Acc: 65.84, V.AUC: 0.6310;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0773, T.Acc: 69.27, T.AUC: 0.6225 V.Loss: 0.0830, V.Acc: 65.84, V.AUC: 0.6353;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0771, T.Acc: 69.14, T.AUC: 0.6372 V.Loss: 0.0824, V.Acc: 65.84, V.AUC: 0.6350;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0765, T.Acc: 69.55, T.AUC: 0.6255 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6350;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0761, T.Acc: 69.27, T.AUC: 0.6348 V.Loss: 0.0829, V.Acc: 66.12, V.AUC: 0.6382;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0759, T.Acc: 69.27, T.AUC: 0.6375 V.Loss: 0.0831, V.Acc: 66.39, V.AUC: 0.6411;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0755, T.Acc: 69.68, T.AUC: 0.6557 V.Loss: 0.0814, V.Acc: 64.46, V.AUC: 0.6449;\n",
      "Trigger times >= patience: 0\n",
      "[E 27/2000] T.Loss: 0.0747, T.Acc: 69.14, T.AUC: 0.6652 V.Loss: 0.0814, V.Acc: 64.46, V.AUC: 0.6448;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0749, T.Acc: 69.14, T.AUC: 0.6643 V.Loss: 0.0834, V.Acc: 64.74, V.AUC: 0.6472;\n",
      "Loss Trigger Times: 1\n",
      "[E 29/2000] T.Loss: 0.0754, T.Acc: 69.27, T.AUC: 0.6492 V.Loss: 0.0845, V.Acc: 65.56, V.AUC: 0.6454;\n",
      "Loss Trigger Times: 2\n",
      "[E 30/2000] T.Loss: 0.0741, T.Acc: 68.31, T.AUC: 0.6676 V.Loss: 0.0828, V.Acc: 64.46, V.AUC: 0.6459;\n",
      "Loss Trigger Times: 3\n",
      "[E 31/2000] T.Loss: 0.0731, T.Acc: 68.45, T.AUC: 0.6711 V.Loss: 0.0810, V.Acc: 65.29, V.AUC: 0.6432;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0728, T.Acc: 69.27, T.AUC: 0.6762 V.Loss: 0.0814, V.Acc: 65.29, V.AUC: 0.6434;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0727, T.Acc: 68.86, T.AUC: 0.6855 V.Loss: 0.0819, V.Acc: 65.56, V.AUC: 0.6414;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0725, T.Acc: 68.59, T.AUC: 0.6919 V.Loss: 0.0822, V.Acc: 66.12, V.AUC: 0.6390;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0718, T.Acc: 69.41, T.AUC: 0.6837 V.Loss: 0.0808, V.Acc: 65.84, V.AUC: 0.6374;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0720, T.Acc: 69.96, T.AUC: 0.6981 V.Loss: 0.0823, V.Acc: 66.12, V.AUC: 0.6344;\n",
      "Loss Trigger Times: 1\n",
      "[E 37/2000] T.Loss: 0.0712, T.Acc: 70.37, T.AUC: 0.7019 V.Loss: 0.0809, V.Acc: 67.22, V.AUC: 0.6341;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0715, T.Acc: 69.27, T.AUC: 0.7003 V.Loss: 0.0809, V.Acc: 67.49, V.AUC: 0.6304;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0707, T.Acc: 70.37, T.AUC: 0.6950 V.Loss: 0.0812, V.Acc: 67.22, V.AUC: 0.6291;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0704, T.Acc: 70.92, T.AUC: 0.7068 V.Loss: 0.0811, V.Acc: 68.32, V.AUC: 0.6279;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0707, T.Acc: 69.96, T.AUC: 0.7075 V.Loss: 0.0810, V.Acc: 67.77, V.AUC: 0.6259;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0705, T.Acc: 70.92, T.AUC: 0.7133 V.Loss: 0.0813, V.Acc: 68.04, V.AUC: 0.6232;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0698, T.Acc: 70.78, T.AUC: 0.7082 V.Loss: 0.0813, V.Acc: 68.04, V.AUC: 0.6222;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0700, T.Acc: 71.88, T.AUC: 0.7051 V.Loss: 0.0831, V.Acc: 67.49, V.AUC: 0.6183;\n",
      "Loss Trigger Times: 1\n",
      "[E 45/2000] T.Loss: 0.0696, T.Acc: 70.10, T.AUC: 0.7046 V.Loss: 0.0814, V.Acc: 68.32, V.AUC: 0.6164;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0695, T.Acc: 71.19, T.AUC: 0.7158 V.Loss: 0.0825, V.Acc: 67.49, V.AUC: 0.6154;\n",
      "Loss Trigger Times: 1\n",
      "[E 47/2000] T.Loss: 0.0703, T.Acc: 70.64, T.AUC: 0.7167 V.Loss: 0.0843, V.Acc: 68.32, V.AUC: 0.6138;\n",
      "Loss Trigger Times: 2\n",
      "[E 48/2000] T.Loss: 0.0687, T.Acc: 71.88, T.AUC: 0.7164 V.Loss: 0.0820, V.Acc: 68.60, V.AUC: 0.6117;\n",
      "Loss Trigger Times: 3\n",
      "[E 49/2000] T.Loss: 0.0685, T.Acc: 72.43, T.AUC: 0.7271 V.Loss: 0.0826, V.Acc: 68.87, V.AUC: 0.6089;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.7333333333333333\n",
      "0.6242667489966038\n",
      "[[76  3]\n",
      " [29 12]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/pty_waspreterm.save']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: phylotype data       ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_pty, pty_hist, ptytest_obs, ptytest_pred, ptytest_prob, ptytest_auc, ptytest_acc, ptytest_conf = FirstStage_pty(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod)\n",
    "print(ptytest_acc)\n",
    "print(ptytest_auc)\n",
    "print(ptytest_conf)\n",
    "dump(model_pty, '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/pty_waspreterm.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b67692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ txy LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0917, T.Acc: 68.59, T.AUC: 0.5346 V.Loss: 0.0921, V.Acc: 65.84, V.AUC: 0.5738;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0901, T.Acc: 69.27, T.AUC: 0.5005 V.Loss: 0.0908, V.Acc: 65.84, V.AUC: 0.5362;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0884, T.Acc: 69.27, T.AUC: 0.5151 V.Loss: 0.0894, V.Acc: 65.84, V.AUC: 0.5188;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0864, T.Acc: 69.27, T.AUC: 0.4988 V.Loss: 0.0880, V.Acc: 65.84, V.AUC: 0.5089;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0844, T.Acc: 69.27, T.AUC: 0.4883 V.Loss: 0.0865, V.Acc: 65.84, V.AUC: 0.5045;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0824, T.Acc: 69.27, T.AUC: 0.4847 V.Loss: 0.0853, V.Acc: 65.84, V.AUC: 0.4999;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0809, T.Acc: 69.27, T.AUC: 0.4872 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.4996;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0801, T.Acc: 69.27, T.AUC: 0.4808 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.5011;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0798, T.Acc: 69.27, T.AUC: 0.4529 V.Loss: 0.0849, V.Acc: 65.84, V.AUC: 0.5048;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0799, T.Acc: 69.27, T.AUC: 0.4836 V.Loss: 0.0854, V.Acc: 65.84, V.AUC: 0.5106;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0800, T.Acc: 69.27, T.AUC: 0.4912 V.Loss: 0.0855, V.Acc: 65.84, V.AUC: 0.5190;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0800, T.Acc: 69.27, T.AUC: 0.4947 V.Loss: 0.0855, V.Acc: 65.84, V.AUC: 0.5261;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0799, T.Acc: 69.27, T.AUC: 0.4685 V.Loss: 0.0853, V.Acc: 65.84, V.AUC: 0.5346;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0798, T.Acc: 69.27, T.AUC: 0.4975 V.Loss: 0.0850, V.Acc: 65.84, V.AUC: 0.5513;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.4990 V.Loss: 0.0849, V.Acc: 65.84, V.AUC: 0.5642;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5143 V.Loss: 0.0847, V.Acc: 65.84, V.AUC: 0.5722;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5274 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.5784;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5214 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.5868;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5134 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.5912;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5506 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.5955;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.4943 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6004;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5367 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6034;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5039 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6060;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5194 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.6071;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5090 V.Loss: 0.0847, V.Acc: 65.84, V.AUC: 0.6090;\n",
      "Loss Trigger Times: 1\n",
      "[E 26/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5182 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.6098;\n",
      "Loss Trigger Times: 2\n",
      "[E 27/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5113 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.6102;\n",
      "Loss Trigger Times: 3\n",
      "[E 28/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5660 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6113;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5434 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6109;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.5537 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6100;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5636 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6108;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5183 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6121;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5491 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6129;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5745 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6136;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0794, T.Acc: 69.27, T.AUC: 0.5794 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6144;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5755 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6144;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5574 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6150;\n",
      "Loss Trigger Times: 1\n",
      "[E 38/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5430 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6156;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5399 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6161;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5902 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6164;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5886 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6169;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5827 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6180;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.5822 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6184;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.5428 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6192;\n",
      "Trigger times >= patience: 0\n",
      "[E 45/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.5574 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6196;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.5726 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6190;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.6024 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6192;\n",
      "Trigger times >= patience: 0\n",
      "[E 48/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.5623 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6193;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.5920 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6201;\n",
      "Trigger times >= patience: 0\n",
      "[E 50/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.5996 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6202;\n",
      "Trigger times >= patience: 0\n",
      "[E 51/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.5793 V.Loss: 0.0838, V.Acc: 65.84, V.AUC: 0.6199;\n",
      "Trigger times >= patience: 0\n",
      "[E 52/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.6238 V.Loss: 0.0838, V.Acc: 65.84, V.AUC: 0.6200;\n",
      "Trigger times >= patience: 0\n",
      "[E 53/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.6010 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6196;\n",
      "Trigger times >= patience: 0\n",
      "[E 54/2000] T.Loss: 0.0787, T.Acc: 69.27, T.AUC: 0.5819 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6200;\n",
      "Trigger times >= patience: 0\n",
      "[E 55/2000] T.Loss: 0.0787, T.Acc: 69.27, T.AUC: 0.5919 V.Loss: 0.0836, V.Acc: 65.84, V.AUC: 0.6201;\n",
      "Trigger times >= patience: 0\n",
      "[E 56/2000] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.5990 V.Loss: 0.0836, V.Acc: 65.84, V.AUC: 0.6198;\n",
      "Trigger times >= patience: 0\n",
      "[E 57/2000] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.5743 V.Loss: 0.0835, V.Acc: 65.84, V.AUC: 0.6197;\n",
      "Trigger times >= patience: 0\n",
      "[E 58/2000] T.Loss: 0.0785, T.Acc: 69.27, T.AUC: 0.6052 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6198;\n",
      "Trigger times >= patience: 0\n",
      "[E 59/2000] T.Loss: 0.0784, T.Acc: 69.27, T.AUC: 0.6046 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6198;\n",
      "Trigger times >= patience: 0\n",
      "[E 60/2000] T.Loss: 0.0784, T.Acc: 69.27, T.AUC: 0.6073 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6198;\n",
      "Trigger times >= patience: 0\n",
      "[E 61/2000] T.Loss: 0.0783, T.Acc: 69.27, T.AUC: 0.5941 V.Loss: 0.0833, V.Acc: 65.84, V.AUC: 0.6199;\n",
      "Trigger times >= patience: 0\n",
      "[E 62/2000] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.6270 V.Loss: 0.0832, V.Acc: 65.84, V.AUC: 0.6193;\n",
      "Trigger times >= patience: 0\n",
      "[E 63/2000] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.6276 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6189;\n",
      "Trigger times >= patience: 0\n",
      "[E 64/2000] T.Loss: 0.0781, T.Acc: 69.27, T.AUC: 0.6149 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6186;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 65/2000] T.Loss: 0.0780, T.Acc: 69.27, T.AUC: 0.6286 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6190;\n",
      "Trigger times >= patience: 0\n",
      "[E 66/2000] T.Loss: 0.0779, T.Acc: 69.27, T.AUC: 0.6334 V.Loss: 0.0829, V.Acc: 65.84, V.AUC: 0.6190;\n",
      "Trigger times >= patience: 0\n",
      "[E 67/2000] T.Loss: 0.0778, T.Acc: 69.27, T.AUC: 0.6059 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6184;\n",
      "Trigger times >= patience: 0\n",
      "[E 68/2000] T.Loss: 0.0778, T.Acc: 69.27, T.AUC: 0.6199 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6185;\n",
      "Trigger times >= patience: 0\n",
      "[E 69/2000] T.Loss: 0.0777, T.Acc: 69.27, T.AUC: 0.6001 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6180;\n",
      "Trigger times >= patience: 0\n",
      "[E 70/2000] T.Loss: 0.0776, T.Acc: 69.27, T.AUC: 0.6373 V.Loss: 0.0826, V.Acc: 65.84, V.AUC: 0.6179;\n",
      "Trigger times >= patience: 0\n",
      "[E 71/2000] T.Loss: 0.0775, T.Acc: 69.27, T.AUC: 0.6288 V.Loss: 0.0825, V.Acc: 65.84, V.AUC: 0.6182;\n",
      "Trigger times >= patience: 0\n",
      "[E 72/2000] T.Loss: 0.0774, T.Acc: 69.27, T.AUC: 0.6236 V.Loss: 0.0824, V.Acc: 65.56, V.AUC: 0.6176;\n",
      "Trigger times >= patience: 0\n",
      "[E 73/2000] T.Loss: 0.0773, T.Acc: 69.27, T.AUC: 0.6373 V.Loss: 0.0824, V.Acc: 65.84, V.AUC: 0.6171;\n",
      "Trigger times >= patience: 0\n",
      "[E 74/2000] T.Loss: 0.0772, T.Acc: 69.41, T.AUC: 0.6561 V.Loss: 0.0820, V.Acc: 65.29, V.AUC: 0.6171;\n",
      "Trigger times >= patience: 0\n",
      "[E 75/2000] T.Loss: 0.0771, T.Acc: 69.14, T.AUC: 0.6399 V.Loss: 0.0820, V.Acc: 65.29, V.AUC: 0.6168;\n",
      "Trigger times >= patience: 0\n",
      "[E 76/2000] T.Loss: 0.0770, T.Acc: 69.14, T.AUC: 0.6287 V.Loss: 0.0822, V.Acc: 65.29, V.AUC: 0.6162;\n",
      "Trigger times >= patience: 0\n",
      "[E 77/2000] T.Loss: 0.0769, T.Acc: 69.00, T.AUC: 0.6281 V.Loss: 0.0822, V.Acc: 65.29, V.AUC: 0.6159;\n",
      "Trigger times >= patience: 0\n",
      "[E 78/2000] T.Loss: 0.0768, T.Acc: 68.45, T.AUC: 0.6493 V.Loss: 0.0820, V.Acc: 65.01, V.AUC: 0.6155;\n",
      "Trigger times >= patience: 0\n",
      "[E 79/2000] T.Loss: 0.0767, T.Acc: 69.00, T.AUC: 0.6438 V.Loss: 0.0819, V.Acc: 65.01, V.AUC: 0.6156;\n",
      "Trigger times >= patience: 0\n",
      "[E 80/2000] T.Loss: 0.0766, T.Acc: 69.00, T.AUC: 0.6399 V.Loss: 0.0816, V.Acc: 65.01, V.AUC: 0.6152;\n",
      "Trigger times >= patience: 0\n",
      "[E 81/2000] T.Loss: 0.0765, T.Acc: 68.18, T.AUC: 0.6394 V.Loss: 0.0815, V.Acc: 64.46, V.AUC: 0.6154;\n",
      "Trigger times >= patience: 0\n",
      "[E 82/2000] T.Loss: 0.0765, T.Acc: 68.59, T.AUC: 0.6346 V.Loss: 0.0821, V.Acc: 65.01, V.AUC: 0.6155;\n",
      "Loss Trigger Times: 1\n",
      "[E 83/2000] T.Loss: 0.0764, T.Acc: 69.14, T.AUC: 0.6424 V.Loss: 0.0819, V.Acc: 65.01, V.AUC: 0.6151;\n",
      "Trigger times >= patience: 0\n",
      "[E 84/2000] T.Loss: 0.0762, T.Acc: 68.86, T.AUC: 0.6360 V.Loss: 0.0816, V.Acc: 64.46, V.AUC: 0.6150;\n",
      "Trigger times >= patience: 0\n",
      "[E 85/2000] T.Loss: 0.0762, T.Acc: 68.86, T.AUC: 0.6443 V.Loss: 0.0812, V.Acc: 64.74, V.AUC: 0.6153;\n",
      "Trigger times >= patience: 0\n",
      "[E 86/2000] T.Loss: 0.0761, T.Acc: 67.90, T.AUC: 0.6421 V.Loss: 0.0812, V.Acc: 64.74, V.AUC: 0.6153;\n",
      "Trigger times >= patience: 0\n",
      "[E 87/2000] T.Loss: 0.0760, T.Acc: 68.18, T.AUC: 0.6395 V.Loss: 0.0813, V.Acc: 64.74, V.AUC: 0.6158;\n",
      "Trigger times >= patience: 0\n",
      "[E 88/2000] T.Loss: 0.0759, T.Acc: 68.45, T.AUC: 0.6447 V.Loss: 0.0813, V.Acc: 64.74, V.AUC: 0.6163;\n",
      "Trigger times >= patience: 0\n",
      "[E 89/2000] T.Loss: 0.0758, T.Acc: 68.18, T.AUC: 0.6560 V.Loss: 0.0815, V.Acc: 64.46, V.AUC: 0.6165;\n",
      "Trigger times >= patience: 0\n",
      "[E 90/2000] T.Loss: 0.0757, T.Acc: 68.59, T.AUC: 0.6414 V.Loss: 0.0815, V.Acc: 64.46, V.AUC: 0.6166;\n",
      "Trigger times >= patience: 0\n",
      "[E 91/2000] T.Loss: 0.0757, T.Acc: 68.45, T.AUC: 0.6401 V.Loss: 0.0814, V.Acc: 64.74, V.AUC: 0.6169;\n",
      "Trigger times >= patience: 0\n",
      "[E 92/2000] T.Loss: 0.0756, T.Acc: 69.14, T.AUC: 0.6568 V.Loss: 0.0812, V.Acc: 66.12, V.AUC: 0.6176;\n",
      "Trigger times >= patience: 0\n",
      "[E 93/2000] T.Loss: 0.0755, T.Acc: 68.72, T.AUC: 0.6366 V.Loss: 0.0810, V.Acc: 67.22, V.AUC: 0.6177;\n",
      "Trigger times >= patience: 0\n",
      "[E 94/2000] T.Loss: 0.0754, T.Acc: 69.27, T.AUC: 0.6394 V.Loss: 0.0811, V.Acc: 66.67, V.AUC: 0.6179;\n",
      "Trigger times >= patience: 0\n",
      "[E 95/2000] T.Loss: 0.0753, T.Acc: 69.00, T.AUC: 0.6487 V.Loss: 0.0811, V.Acc: 66.67, V.AUC: 0.6176;\n",
      "Trigger times >= patience: 0\n",
      "[E 96/2000] T.Loss: 0.0753, T.Acc: 69.14, T.AUC: 0.6496 V.Loss: 0.0814, V.Acc: 64.74, V.AUC: 0.6175;\n",
      "Loss Trigger Times: 1\n",
      "[E 97/2000] T.Loss: 0.0751, T.Acc: 69.27, T.AUC: 0.6522 V.Loss: 0.0812, V.Acc: 67.22, V.AUC: 0.6181;\n",
      "Trigger times >= patience: 0\n",
      "[E 98/2000] T.Loss: 0.0750, T.Acc: 68.59, T.AUC: 0.6479 V.Loss: 0.0809, V.Acc: 67.22, V.AUC: 0.6182;\n",
      "Trigger times >= patience: 0\n",
      "[E 99/2000] T.Loss: 0.0750, T.Acc: 68.31, T.AUC: 0.6537 V.Loss: 0.0807, V.Acc: 67.22, V.AUC: 0.6190;\n",
      "Trigger times >= patience: 0\n",
      "[E 100/2000] T.Loss: 0.0748, T.Acc: 69.00, T.AUC: 0.6545 V.Loss: 0.0809, V.Acc: 67.22, V.AUC: 0.6202;\n",
      "Trigger times >= patience: 0\n",
      "[E 101/2000] T.Loss: 0.0748, T.Acc: 69.27, T.AUC: 0.6515 V.Loss: 0.0807, V.Acc: 67.22, V.AUC: 0.6208;\n",
      "Trigger times >= patience: 0\n",
      "[E 102/2000] T.Loss: 0.0747, T.Acc: 70.23, T.AUC: 0.6629 V.Loss: 0.0808, V.Acc: 67.22, V.AUC: 0.6219;\n",
      "Trigger times >= patience: 0\n",
      "[E 103/2000] T.Loss: 0.0746, T.Acc: 69.27, T.AUC: 0.6496 V.Loss: 0.0809, V.Acc: 66.94, V.AUC: 0.6220;\n",
      "Trigger times >= patience: 0\n",
      "[E 104/2000] T.Loss: 0.0745, T.Acc: 70.10, T.AUC: 0.6654 V.Loss: 0.0806, V.Acc: 66.94, V.AUC: 0.6226;\n",
      "Trigger times >= patience: 0\n",
      "[E 105/2000] T.Loss: 0.0744, T.Acc: 69.55, T.AUC: 0.6625 V.Loss: 0.0806, V.Acc: 67.22, V.AUC: 0.6226;\n",
      "Trigger times >= patience: 0\n",
      "[E 106/2000] T.Loss: 0.0743, T.Acc: 69.41, T.AUC: 0.6700 V.Loss: 0.0804, V.Acc: 67.22, V.AUC: 0.6227;\n",
      "Trigger times >= patience: 0\n",
      "[E 107/2000] T.Loss: 0.0742, T.Acc: 70.10, T.AUC: 0.6593 V.Loss: 0.0805, V.Acc: 67.22, V.AUC: 0.6238;\n",
      "Trigger times >= patience: 0\n",
      "[E 108/2000] T.Loss: 0.0741, T.Acc: 70.10, T.AUC: 0.6580 V.Loss: 0.0805, V.Acc: 67.22, V.AUC: 0.6245;\n",
      "Trigger times >= patience: 0\n",
      "[E 109/2000] T.Loss: 0.0740, T.Acc: 70.23, T.AUC: 0.6628 V.Loss: 0.0806, V.Acc: 67.22, V.AUC: 0.6260;\n",
      "Trigger times >= patience: 0\n",
      "[E 110/2000] T.Loss: 0.0739, T.Acc: 69.68, T.AUC: 0.6575 V.Loss: 0.0805, V.Acc: 67.22, V.AUC: 0.6272;\n",
      "Trigger times >= patience: 0\n",
      "[E 111/2000] T.Loss: 0.0737, T.Acc: 69.68, T.AUC: 0.6773 V.Loss: 0.0803, V.Acc: 68.04, V.AUC: 0.6272;\n",
      "Trigger times >= patience: 0\n",
      "[E 112/2000] T.Loss: 0.0736, T.Acc: 70.23, T.AUC: 0.6654 V.Loss: 0.0802, V.Acc: 68.32, V.AUC: 0.6278;\n",
      "Trigger times >= patience: 0\n",
      "[E 113/2000] T.Loss: 0.0736, T.Acc: 69.00, T.AUC: 0.6579 V.Loss: 0.0806, V.Acc: 67.22, V.AUC: 0.6284;\n",
      "Loss Trigger Times: 1\n",
      "[E 114/2000] T.Loss: 0.0734, T.Acc: 70.10, T.AUC: 0.6629 V.Loss: 0.0802, V.Acc: 68.04, V.AUC: 0.6292;\n",
      "Trigger times >= patience: 0\n",
      "[E 115/2000] T.Loss: 0.0734, T.Acc: 69.68, T.AUC: 0.6596 V.Loss: 0.0800, V.Acc: 69.15, V.AUC: 0.6289;\n",
      "Trigger times >= patience: 0\n",
      "[E 116/2000] T.Loss: 0.0733, T.Acc: 69.96, T.AUC: 0.6617 V.Loss: 0.0802, V.Acc: 67.77, V.AUC: 0.6282;\n",
      "Trigger times >= patience: 0\n",
      "[E 117/2000] T.Loss: 0.0732, T.Acc: 69.68, T.AUC: 0.6678 V.Loss: 0.0802, V.Acc: 67.77, V.AUC: 0.6282;\n",
      "Trigger times >= patience: 0\n",
      "[E 118/2000] T.Loss: 0.0731, T.Acc: 69.96, T.AUC: 0.6729 V.Loss: 0.0798, V.Acc: 69.70, V.AUC: 0.6278;\n",
      "Trigger times >= patience: 0\n",
      "[E 119/2000] T.Loss: 0.0730, T.Acc: 70.92, T.AUC: 0.6724 V.Loss: 0.0798, V.Acc: 69.70, V.AUC: 0.6284;\n",
      "Trigger times >= patience: 0\n",
      "[E 120/2000] T.Loss: 0.0731, T.Acc: 70.23, T.AUC: 0.6631 V.Loss: 0.0807, V.Acc: 67.22, V.AUC: 0.6289;\n",
      "Loss Trigger Times: 1\n",
      "[E 121/2000] T.Loss: 0.0728, T.Acc: 69.82, T.AUC: 0.6699 V.Loss: 0.0802, V.Acc: 67.77, V.AUC: 0.6288;\n",
      "Trigger times >= patience: 0\n",
      "[E 122/2000] T.Loss: 0.0728, T.Acc: 69.82, T.AUC: 0.6712 V.Loss: 0.0796, V.Acc: 69.97, V.AUC: 0.6289;\n",
      "Trigger times >= patience: 0\n",
      "[E 123/2000] T.Loss: 0.0726, T.Acc: 70.10, T.AUC: 0.6763 V.Loss: 0.0800, V.Acc: 68.32, V.AUC: 0.6294;\n",
      "Trigger times >= patience: 0\n",
      "[E 124/2000] T.Loss: 0.0725, T.Acc: 70.10, T.AUC: 0.6770 V.Loss: 0.0796, V.Acc: 70.25, V.AUC: 0.6310;\n",
      "Trigger times >= patience: 0\n",
      "[E 125/2000] T.Loss: 0.0724, T.Acc: 70.23, T.AUC: 0.6746 V.Loss: 0.0796, V.Acc: 70.52, V.AUC: 0.6322;\n",
      "Trigger times >= patience: 0\n",
      "[E 126/2000] T.Loss: 0.0723, T.Acc: 70.64, T.AUC: 0.6885 V.Loss: 0.0799, V.Acc: 68.87, V.AUC: 0.6316;\n",
      "Trigger times >= patience: 0\n",
      "[E 127/2000] T.Loss: 0.0722, T.Acc: 70.37, T.AUC: 0.6851 V.Loss: 0.0798, V.Acc: 69.70, V.AUC: 0.6321;\n",
      "Trigger times >= patience: 0\n",
      "[E 128/2000] T.Loss: 0.0721, T.Acc: 70.64, T.AUC: 0.6879 V.Loss: 0.0798, V.Acc: 69.15, V.AUC: 0.6321;\n",
      "Trigger times >= patience: 0\n",
      "[E 129/2000] T.Loss: 0.0720, T.Acc: 69.96, T.AUC: 0.6833 V.Loss: 0.0795, V.Acc: 70.52, V.AUC: 0.6323;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 130/2000] T.Loss: 0.0721, T.Acc: 70.51, T.AUC: 0.6839 V.Loss: 0.0794, V.Acc: 70.52, V.AUC: 0.6324;\n",
      "Trigger times >= patience: 0\n",
      "[E 131/2000] T.Loss: 0.0719, T.Acc: 70.64, T.AUC: 0.6790 V.Loss: 0.0798, V.Acc: 69.15, V.AUC: 0.6330;\n",
      "Loss Trigger Times: 1\n",
      "[E 132/2000] T.Loss: 0.0718, T.Acc: 70.37, T.AUC: 0.6821 V.Loss: 0.0798, V.Acc: 69.42, V.AUC: 0.6330;\n",
      "Loss Trigger Times: 2\n",
      "[E 133/2000] T.Loss: 0.0717, T.Acc: 70.92, T.AUC: 0.6939 V.Loss: 0.0795, V.Acc: 70.25, V.AUC: 0.6328;\n",
      "Trigger times >= patience: 0\n",
      "[E 134/2000] T.Loss: 0.0717, T.Acc: 70.92, T.AUC: 0.6946 V.Loss: 0.0794, V.Acc: 70.25, V.AUC: 0.6343;\n",
      "Trigger times >= patience: 0\n",
      "[E 135/2000] T.Loss: 0.0715, T.Acc: 71.06, T.AUC: 0.6883 V.Loss: 0.0795, V.Acc: 69.70, V.AUC: 0.6377;\n",
      "Trigger times >= patience: 0\n",
      "[E 136/2000] T.Loss: 0.0715, T.Acc: 70.23, T.AUC: 0.6956 V.Loss: 0.0800, V.Acc: 68.87, V.AUC: 0.6378;\n",
      "Loss Trigger Times: 1\n",
      "[E 137/2000] T.Loss: 0.0713, T.Acc: 70.23, T.AUC: 0.6799 V.Loss: 0.0796, V.Acc: 69.70, V.AUC: 0.6375;\n",
      "Trigger times >= patience: 0\n",
      "[E 138/2000] T.Loss: 0.0713, T.Acc: 70.78, T.AUC: 0.6891 V.Loss: 0.0793, V.Acc: 69.70, V.AUC: 0.6377;\n",
      "Trigger times >= patience: 0\n",
      "[E 139/2000] T.Loss: 0.0711, T.Acc: 71.33, T.AUC: 0.6911 V.Loss: 0.0794, V.Acc: 70.52, V.AUC: 0.6387;\n",
      "Trigger times >= patience: 0\n",
      "[E 140/2000] T.Loss: 0.0711, T.Acc: 70.92, T.AUC: 0.6895 V.Loss: 0.0796, V.Acc: 69.70, V.AUC: 0.6374;\n",
      "Loss Trigger Times: 1\n",
      "[E 141/2000] T.Loss: 0.0709, T.Acc: 71.19, T.AUC: 0.6966 V.Loss: 0.0794, V.Acc: 70.52, V.AUC: 0.6372;\n",
      "Trigger times >= patience: 0\n",
      "[E 142/2000] T.Loss: 0.0710, T.Acc: 71.47, T.AUC: 0.6989 V.Loss: 0.0792, V.Acc: 70.25, V.AUC: 0.6372;\n",
      "Trigger times >= patience: 0\n",
      "[E 143/2000] T.Loss: 0.0707, T.Acc: 71.06, T.AUC: 0.6998 V.Loss: 0.0793, V.Acc: 70.52, V.AUC: 0.6374;\n",
      "Trigger times >= patience: 0\n",
      "[E 144/2000] T.Loss: 0.0710, T.Acc: 70.78, T.AUC: 0.6946 V.Loss: 0.0801, V.Acc: 68.60, V.AUC: 0.6376;\n",
      "Loss Trigger Times: 1\n",
      "[E 145/2000] T.Loss: 0.0706, T.Acc: 70.92, T.AUC: 0.7088 V.Loss: 0.0792, V.Acc: 70.52, V.AUC: 0.6382;\n",
      "Trigger times >= patience: 0\n",
      "[E 146/2000] T.Loss: 0.0707, T.Acc: 72.15, T.AUC: 0.7018 V.Loss: 0.0792, V.Acc: 69.97, V.AUC: 0.6383;\n",
      "Trigger times >= patience: 0\n",
      "[E 147/2000] T.Loss: 0.0705, T.Acc: 71.88, T.AUC: 0.7081 V.Loss: 0.0797, V.Acc: 69.42, V.AUC: 0.6385;\n",
      "Loss Trigger Times: 1\n",
      "[E 148/2000] T.Loss: 0.0707, T.Acc: 72.15, T.AUC: 0.7109 V.Loss: 0.0801, V.Acc: 68.87, V.AUC: 0.6382;\n",
      "Loss Trigger Times: 2\n",
      "[E 149/2000] T.Loss: 0.0703, T.Acc: 71.47, T.AUC: 0.6928 V.Loss: 0.0792, V.Acc: 70.52, V.AUC: 0.6380;\n",
      "Trigger times >= patience: 0\n",
      "[E 150/2000] T.Loss: 0.0701, T.Acc: 71.47, T.AUC: 0.7044 V.Loss: 0.0793, V.Acc: 70.52, V.AUC: 0.6381;\n",
      "Trigger times >= patience: 0\n",
      "[E 151/2000] T.Loss: 0.0700, T.Acc: 72.02, T.AUC: 0.7079 V.Loss: 0.0793, V.Acc: 70.80, V.AUC: 0.6377;\n",
      "Trigger times >= patience: 0\n",
      "[E 152/2000] T.Loss: 0.0700, T.Acc: 71.88, T.AUC: 0.7097 V.Loss: 0.0795, V.Acc: 69.42, V.AUC: 0.6381;\n",
      "Loss Trigger Times: 1\n",
      "[E 153/2000] T.Loss: 0.0698, T.Acc: 71.88, T.AUC: 0.6985 V.Loss: 0.0794, V.Acc: 69.70, V.AUC: 0.6384;\n",
      "Trigger times >= patience: 0\n",
      "[E 154/2000] T.Loss: 0.0698, T.Acc: 72.15, T.AUC: 0.7011 V.Loss: 0.0795, V.Acc: 69.15, V.AUC: 0.6379;\n",
      "Trigger times >= patience: 0\n",
      "[E 155/2000] T.Loss: 0.0698, T.Acc: 71.74, T.AUC: 0.6994 V.Loss: 0.0793, V.Acc: 69.97, V.AUC: 0.6382;\n",
      "Trigger times >= patience: 0\n",
      "[E 156/2000] T.Loss: 0.0696, T.Acc: 72.29, T.AUC: 0.7063 V.Loss: 0.0795, V.Acc: 69.70, V.AUC: 0.6381;\n",
      "Loss Trigger Times: 1\n",
      "[E 157/2000] T.Loss: 0.0696, T.Acc: 72.70, T.AUC: 0.7049 V.Loss: 0.0797, V.Acc: 68.87, V.AUC: 0.6380;\n",
      "Loss Trigger Times: 2\n",
      "[E 158/2000] T.Loss: 0.0694, T.Acc: 71.88, T.AUC: 0.7074 V.Loss: 0.0795, V.Acc: 69.70, V.AUC: 0.6376;\n",
      "Loss Trigger Times: 3\n",
      "[E 159/2000] T.Loss: 0.0693, T.Acc: 71.74, T.AUC: 0.7070 V.Loss: 0.0795, V.Acc: 69.97, V.AUC: 0.6373;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.725\n",
      "0.7119481321395492\n",
      "[[74  5]\n",
      " [28 13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/txy_waspreterm.save']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: taxonomy data        ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_txy, txy_hist, txytest_obs, txytest_pred, txytest_prob, txytest_auc, txytest_acc, txytest_conf = FirstStage_txy(mytrain_input_txy, mytrain_output, myvalid_input_txy, myvalid_output, mytest_input_txy, mytest_output, finalperiod)\n",
    "print(txytest_acc)\n",
    "print(txytest_auc)\n",
    "print(txytest_conf)\n",
    "dump(model_txy, '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/txy_waspreterm.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b6192b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3416666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mytest_output[:,finalperiod-1,0])/mytest_output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d415957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3415977961432507\n",
      "0.725\n",
      "0.7582587218277246\n",
      "[[71  8]\n",
      " [25 16]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use validation set only without class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "Mtdvalid_obs, Mtdvalid_pred, Mtdvalid_prob = evaluate(model_Mtd, device, myvalid_input_mtd, myvalid_output, finalperiod, cutoff=0.5)\n",
    "ptyvalid_obs, ptyvalid_pred, ptyvalid_prob = evaluate(model_pty, device, myvalid_input_pty, myvalid_output, finalperiod, cutoff=0.5)\n",
    "txyvalid_obs, txyvalid_pred, txyvalid_prob = evaluate(model_txy, device, myvalid_input_txy, myvalid_output, finalperiod, cutoff=0.5)\n",
    "\n",
    "x_valid = np.array(np.column_stack([Mtdvalid_prob, ptyvalid_prob, txyvalid_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(Mtdvalid_obs)/len(Mtdvalid_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag')\n",
    "L2Logistic_model.fit(x_valid, Mtdvalid_obs)\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ebac487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3415977961432507\n",
      "0.75\n",
      "0.75177523927138\n",
      "[[62 17]\n",
      " [13 28]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use validation set only with class weights (Best)\n",
    "#-------------------------------------------#\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "Mtdvalid_obs, Mtdvalid_pred, Mtdvalid_prob = evaluate(model_Mtd, device, myvalid_input_mtd, myvalid_output, finalperiod, cutoff=0.5)\n",
    "ptyvalid_obs, ptyvalid_pred, ptyvalid_prob = evaluate(model_pty, device, myvalid_input_pty, myvalid_output, finalperiod, cutoff=0.5)\n",
    "txyvalid_obs, txyvalid_pred, txyvalid_prob = evaluate(model_txy, device, myvalid_input_txy, myvalid_output, finalperiod, cutoff=0.5)\n",
    "\n",
    "x_valid = np.array(np.column_stack([Mtdvalid_prob, ptyvalid_prob, txyvalid_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(Mtdvalid_obs)/len(Mtdvalid_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag', class_weight=wt)\n",
    "L2Logistic_model.fit(x_valid, Mtdvalid_obs)\n",
    "\n",
    "dump(L2Logistic_model, '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/L2logistic_waspreterm.save')\n",
    "     \n",
    "#---- testing set evaluation ----#\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd20ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31868131868131866\n",
      "0.7416666666666667\n",
      "0.7431305958629205\n",
      "[[71  8]\n",
      " [23 18]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use trianing+validation set without class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "MtdS2train_input = np.concatenate((mytrain_input_mtd, myvalid_input_mtd), axis=0)\n",
    "ptyS2train_input = np.concatenate((mytrain_input_pty, myvalid_input_pty), axis=0)\n",
    "txyS2train_input = np.concatenate((mytrain_input_txy, myvalid_input_txy), axis=0)\n",
    "\n",
    "S2train_output = np.concatenate((mytrain_output, myvalid_output), axis=0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "\n",
    "MtdS2_obs, MtdS2_pred, MtdS2_prob = evaluate(model_Mtd, device, MtdS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "ptyS2_obs, ptyS2_pred, ptyS2_prob = evaluate(model_pty, device, ptyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "txyS2_obs, txyS2_pred, txyS2_prob = evaluate(model_txy, device, txyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "x_valid = np.array(np.column_stack([MtdS2_prob, ptyS2_prob, txyS2_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(MtdS2_obs)/len(MtdS2_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag')\n",
    "L2Logistic_model.fit(x_valid, MtdS2_obs)\n",
    "\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "# x_test = np.array(np.transpose([Mtdtest_prob, ptytest_prob, txytest_prob, krdtest_prob])).reshape(-1, 3*2)\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55178019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31868131868131866\n",
      "0.7333333333333333\n",
      "0.7428218585983329\n",
      "[[62 17]\n",
      " [15 26]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use trianing + validation set with class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "MtdS2train_input = np.concatenate((mytrain_input_mtd, myvalid_input_mtd), axis=0)\n",
    "ptyS2train_input = np.concatenate((mytrain_input_pty, myvalid_input_pty), axis=0)\n",
    "txyS2train_input = np.concatenate((mytrain_input_txy, myvalid_input_txy), axis=0)\n",
    "\n",
    "S2train_output = np.concatenate((mytrain_output, myvalid_output), axis=0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "\n",
    "MtdS2_obs, MtdS2_pred, MtdS2_prob = evaluate(model_Mtd, device, MtdS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "ptyS2_obs, ptyS2_pred, ptyS2_prob = evaluate(model_pty, device, ptyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "txyS2_obs, txyS2_pred, txyS2_prob = evaluate(model_txy, device, txyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "x_valid = np.array(np.column_stack([MtdS2_prob, ptyS2_prob, txyS2_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(MtdS2_obs)/len(MtdS2_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag', class_weight=wt)\n",
    "L2Logistic_model.fit(x_valid, MtdS2_obs)\n",
    "\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "# x_test = np.array(np.transpose([Mtdtest_prob, ptytest_prob, txytest_prob, krdtest_prob])).reshape(-1, 3*2)\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
