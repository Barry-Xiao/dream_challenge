{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1124b28",
   "metadata": {},
   "source": [
    "# Preterm Birth Prediction Microbiome Model Framework (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a2a71",
   "metadata": {},
   "source": [
    "Challenge website:\n",
    "https://www.synapse.org/#!Synapse:syn26133770/wiki/618018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabb89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from collections import Counter,defaultdict, OrderedDict\n",
    "from itertools import islice\n",
    "from joblib import dump, load\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dataset_splitID(meta_data, prop, myseed):\n",
    "    \n",
    "    subjects = list(np.unique(meta_data[\"participant_id\"]))\n",
    "    numsubjects = len(subjects)\n",
    "    \n",
    "    if myseed != None:\n",
    "        random.seed(myseed)\n",
    "\n",
    "    subjects_shuffle = random.sample(subjects, numsubjects)\n",
    "    \n",
    "    train_subjects = subjects_shuffle[0:(int(numsubjects*prop[0])+1)] \n",
    "    valid_subjects = subjects_shuffle[(int(numsubjects*prop[0])+2):(int(numsubjects*(prop[0]+prop[1]))+1)]\n",
    "    test_subjects = subjects_shuffle[(int(numsubjects*(prop[0]+prop[1]))+2):numsubjects]\n",
    "    \n",
    "    splitID_train = meta_data['participant_id'].isin(train_subjects)\n",
    "    splitID_valid = meta_data['participant_id'].isin(valid_subjects)\n",
    "    splitID_test = meta_data['participant_id'].isin(test_subjects)\n",
    "    \n",
    "    return splitID_train, splitID_valid, splitID_test\n",
    "\n",
    "\n",
    "# Possible, but not used here\n",
    "def dataset_pjt_splitID(meta_data, prop, myseed):\n",
    "    \n",
    "    projects = meta_data['project']\n",
    "\n",
    "    splitID_train = []\n",
    "    splitID_valid = []\n",
    "    splitID_test  = []\n",
    "    \n",
    "    for pjt in np.unique(projects):\n",
    "        \n",
    "        submeta = meta_data[projects == pjt]\n",
    "        subsubjects = list(np.unique(submeta[\"participant_id\"]))\n",
    "        numsub = len(subsubjects)\n",
    "        \n",
    "        subsubjects_shuffle = random.sample(subsubjects, numsub)\n",
    "        \n",
    "        train_subsubjects = subsubjects_shuffle[0:(int(numsub*prop[0])+1)] \n",
    "        valid_subsubjects = subsubjects_shuffle[(int(numsub*prop[0])+2):(int(numsub*(prop[0]+prop[1]))+1)]\n",
    "        test_subsubjects  = subsubjects_shuffle[(int(numsub*(prop[0]+prop[1]))+2):numsub]\n",
    "        \n",
    "        splitID_train.extend(submeta['participant_id'].isin(train_subsubjects))\n",
    "        splitID_valid.extend(submeta['participant_id'].isin(valid_subsubjects))\n",
    "        splitID_test.extend(submeta['participant_id'].isin(test_subsubjects))\n",
    "        \n",
    "    return splitID_train, splitID_valid, splitID_test\n",
    "\n",
    "\n",
    "def Data_Reshaper_Input(data, seq_length):\n",
    "    \n",
    "    numsubjects = len(np.unique(data['participant_id']))\n",
    "    myvary = list(data.columns.values)[2:data.shape[1]]\n",
    "    num_covariates = len(myvary)\n",
    "    \n",
    "    myinput = np.zeros((numsubjects, seq_length, num_covariates), dtype=np.float32)\n",
    "    for i in range(num_covariates):\n",
    "        data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=myvary[i])\n",
    "        data_wide = data_wide.sort_index(axis=1)\n",
    "        data_wide = data_wide.fillna(0)\n",
    "        tmpindex = data_wide._get_numeric_data().columns.values - 1\n",
    "        tmpindex = tmpindex.astype(int)\n",
    "        # time varying variables need to impute all and no records are denoted as 0\n",
    "        for j in range(numsubjects):\n",
    "                myinput[j,tmpindex,i] = data_wide.iloc[[j]]\n",
    "    return myinput\n",
    "\n",
    "\n",
    "\n",
    "def Data_Reshaper_Output_ManytoMany_0(data, seq_length, classlabel):\n",
    "\n",
    "    num_samples = len(np.unique(data['participant_id']))\n",
    "    \n",
    "    data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=classlabel)\n",
    "    data_wide = data_wide.sort_index(axis=1)\n",
    "    \n",
    "    myoutput = np.zeros((num_samples, seq_length, 2), dtype=np.float32)\n",
    "    for i in range(num_samples):\n",
    "        tmp = data_wide.iloc[i,:]\n",
    "        \n",
    "        if np.nanmax(tmp) == 1:\n",
    "            # label linear smoonthing from 0.5 to 1\n",
    "            # fill all position 1 to have final labels equal to 1\n",
    "            myoutput[i,:,0].fill(1)\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=1, num=seq_length)\n",
    "        else:\n",
    "            # label linear smoonthing from 0.5 to 0\n",
    "            # fill all position 0 to have final labels equal to 0 \n",
    "            #     but array alrady initialize as 0\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=0, num=seq_length)\n",
    "            \n",
    "        myoutput[i,:,1] = 1 - myoutput[i,:,0]\n",
    "    return myoutput\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, device, myinput, myoutput, finalperiod, cutoff=0.5):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # predicted labels\n",
    "    myinput  = torch.from_numpy(myinput).float().to(device)\n",
    "    myoutput_nn, hidden = model(myinput, device)\n",
    "    myoutput_nn = myoutput_nn.reshape((myoutput.shape))\n",
    "    output_prob = nn.functional.softmax(myoutput_nn, dim=2)\n",
    "    mypredprob = output_prob[:,finalperiod-1,:].cpu().detach().numpy()\n",
    "    mypred = 1*(mypredprob[:,0] > cutoff)\n",
    "    # observed labels\n",
    "    myobs  = myoutput[:,finalperiod-1,0]\n",
    "    \n",
    "    return myobs, mypred, mypredprob\n",
    "\n",
    "\n",
    "\n",
    "def metadata_loader(meta_dir, alpha_dir, cst_dir, task, finalperiod):\n",
    "    \n",
    "    meta_data = pd.DataFrame(pd.read_csv(meta_dir, delimiter=','))\n",
    "    meta_data.replace('Unknown', np.nan, inplace=True)\n",
    "    meta_data = meta_data[['participant_id', 'project', 'delivery_wk', 'collect_wk', 'age', 'race']]\n",
    "    \n",
    "    alpha_data = pd.DataFrame(pd.read_csv(alpha_dir, delimiter=','))\n",
    "    cst_data = pd.DataFrame(pd.read_csv(cst_dir, delimiter=','))\n",
    "    \n",
    "    meta_data = pd.concat([meta_data, alpha_data['shannon'], alpha_data['inv_simpson'], alpha_data['rooted_pd'], cst_data['CST']], axis=1)\n",
    "\n",
    "    for i in range(1,meta_data.shape[1]):\n",
    "        if meta_data.iloc[:,i].dtypes == object:\n",
    "            meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('category').cat.codes + 1\n",
    "            meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('float64')\n",
    "            \n",
    "    # create new variable 'collect_period'\n",
    "    meta_data['collect_period'] = 1\n",
    "    meta_data.loc[(meta_data['collect_wk']>=8)  & (meta_data['collect_wk']<=14),'collect_period'] = 2\n",
    "    meta_data.loc[(meta_data['collect_wk']>=15) & (meta_data['collect_wk']<=21),'collect_period'] = 3\n",
    "    meta_data.loc[(meta_data['collect_wk']>=22) & (meta_data['collect_wk']<=28),'collect_period'] = 4\n",
    "    meta_data.loc[(meta_data['collect_wk']>=29) & (meta_data['collect_wk']<=32),'collect_period'] = 5\n",
    "    meta_data.loc[(meta_data['collect_wk']>=33), 'collect_period']                                = 6\n",
    "    \n",
    "    # print(meta_data['collect_period'].value_counts())\n",
    "    \n",
    "    # create task class label\n",
    "    if task == \"was_preterm\":\n",
    "        meta_data[task] = 1*(meta_data['delivery_wk'] < 37)\n",
    "    elif task == \"was_early_preterm\":\n",
    "        meta_data[task] = 1*(meta_data['delivery_wk'] < 32)\n",
    "        \n",
    "    # Filtered out observations with \"collect_wk<=32\" OR \"collect_period<=5\" \n",
    "    # Filtered out observations with \"collect_wk<=28\" OR \"collect_period<=4\" \n",
    "    meta_data = meta_data[meta_data['collect_period']<=finalperiod]\n",
    "    # Average within each collection period\n",
    "    meta_data = meta_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "\n",
    "    return meta_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac44e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputLoader(data_dir, feature_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod):\n",
    "    \n",
    "    participant_id = meta_data['participant_id']\n",
    "    collect_period = meta_data['collect_period']\n",
    "   \n",
    "    Input_data = pd.DataFrame(pd.read_csv(data_dir, delimiter=','))\n",
    "    selectedfeature = pd.DataFrame(pd.read_csv(feature_dir, delimiter=','))\n",
    "    Input_data = Input_data.iloc[:,selectedfeature['id']]\n",
    "    Input_data = pd.concat([participant_id, collect_period, Input_data], axis=1)\n",
    "        \n",
    "    # Average within each collection period\n",
    "    Input_data = Input_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "    \n",
    "    Input_data_train = Input_data[trainID]\n",
    "    Input_data_valid = Input_data[validID]\n",
    "    Input_data_test  = Input_data[testID]\n",
    "    \n",
    "    print(\"## Input: train/valid/test (before reshape)\")\n",
    "    print(Input_data_train.shape)\n",
    "    print(Input_data_valid.shape)\n",
    "    print(Input_data_test.shape)\n",
    "    \n",
    "    #---- Input features reshaper ----#\n",
    "    mytrain_input = Data_Reshaper_Input(data=Input_data_train, seq_length=finalperiod)\n",
    "    myvalid_input = Data_Reshaper_Input(data=Input_data_valid, seq_length=finalperiod)\n",
    "    mytest_input  = Data_Reshaper_Input(data=Input_data_test, seq_length=finalperiod)\n",
    "    \n",
    "    print(\"## Input: train/valid/test (after reshape)\")\n",
    "    print(mytrain_input.shape)\n",
    "    print(myvalid_input.shape)\n",
    "    print(mytest_input.shape)\n",
    "    \n",
    "    return mytrain_input, myvalid_input, mytest_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "190af89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputLoader(meta_data, trainID, validID, testID, task, finalperiod):\n",
    "    \n",
    "    meta_data_train = meta_data[trainID]\n",
    "    meta_data_valid = meta_data[validID]\n",
    "    meta_data_test  = meta_data[testID]\n",
    "    \n",
    "    print(\"################ Output: train/valid/test (before reshape)\")\n",
    "    print(meta_data_train.shape)\n",
    "    print(meta_data_valid.shape)\n",
    "    print(meta_data_test.shape)\n",
    "    \n",
    "    #---- Output label reshaper ----#\n",
    "    mytrain_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_train, seq_length=finalperiod, classlabel=task)\n",
    "    myvalid_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_valid, seq_length=finalperiod, classlabel=task)\n",
    "    mytest_output = Data_Reshaper_Output_ManytoMany_0(data=meta_data_test, seq_length=finalperiod, classlabel=task)\n",
    "    \n",
    "    print(\"################ Output: train/valid/test (after reshape)\")\n",
    "    print(mytrain_output.shape)\n",
    "    print(myvalid_output.shape)\n",
    "    print(mytest_output.shape)\n",
    "    \n",
    "    return mytrain_output, myvalid_output, mytest_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3947df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputLoaderMtd(meta_data, trainID, validID, testID, task, finalperiod):\n",
    "    \n",
    "    meta_data_train = meta_data[trainID]\n",
    "    meta_data_valid = meta_data[validID]\n",
    "    meta_data_test  = meta_data[testID]\n",
    "    \n",
    "    #---- Input features reshaper ----#\n",
    "    mytrain_input_mtd = meta_data_train.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    myvalid_input_mtd = meta_data_valid.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    mytest_input_mtd  = meta_data_test.drop(['project', 'delivery_wk', task], axis=1)\n",
    "    \n",
    "    # scale the input features in this data set\n",
    "    columns = ['collect_wk', 'age', 'race', 'shannon', 'inv_simpson', 'rooted_pd', 'CST']\n",
    "    for col in columns:\n",
    "        mytrain_input_mtd[col] = MinMaxScaler().fit_transform(np.array(mytrain_input_mtd[col]).reshape(-1,1))\n",
    "        myvalid_input_mtd[col] = MinMaxScaler().fit_transform(np.array(myvalid_input_mtd[col]).reshape(-1,1))\n",
    "        mytest_input_mtd[col]  = MinMaxScaler().fit_transform(np.array(mytest_input_mtd[col]).reshape(-1,1))\n",
    "    \n",
    "    print(\"## Input: train/valid/test (before reshape)\")\n",
    "    print(mytrain_input_mtd.shape)\n",
    "    print(myvalid_input_mtd.shape)\n",
    "    print(mytest_input_mtd.shape)\n",
    "    \n",
    "    mytrain_input_mtd = Data_Reshaper_Input(data=mytrain_input_mtd, seq_length=finalperiod)\n",
    "    myvalid_input_mtd = Data_Reshaper_Input(data=myvalid_input_mtd, seq_length=finalperiod)\n",
    "    mytest_input_mtd  = Data_Reshaper_Input(data=mytest_input_mtd,  seq_length=finalperiod) \n",
    "    \n",
    "    print(\"## Input: train/valid/test (after reshape)\")\n",
    "    print(mytrain_input_mtd.shape)\n",
    "    print(myvalid_input_mtd.shape)\n",
    "    print(mytest_input_mtd.shape)\n",
    "    \n",
    "    return mytrain_input_mtd, myvalid_input_mtd, mytest_input_mtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c901b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMtrain(model, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, finalperiod, patience, earlystop='loss', verbose=True):\n",
    "    \n",
    "    # training and validation set class proportion\n",
    "    trainprior = sum(mytrain_output[:,finalperiod-1,0])/mytrain_output.shape[0]\n",
    "    class1ID_train = mytrain_output[:,finalperiod-1,0] == 1\n",
    "    class2ID_train = mytrain_output[:,finalperiod-1,0] == 0\n",
    "    \n",
    "    validprior = sum(myvalid_output[:,finalperiod-1,0])/myvalid_output.shape[0]\n",
    "    class1ID_valid = myvalid_output[:,finalperiod-1,0] == 1\n",
    "    class2ID_valid = myvalid_output[:,finalperiod-1,0] == 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Track the value of the loss function and model accuracy across epochs\n",
    "    history_train_valid = {'TrainLoss': [], 'TrainAcc': [], 'TrainAUC': [],\n",
    "                           'ValidLoss': [], 'ValidAcc': [], 'ValidAUC': []}\n",
    "    \n",
    "    # Same reshaped Validation set for each epoch    \n",
    "    myvalid_input  = torch.from_numpy(myvalid_input).float().to(device)\n",
    "    myvalid_output = torch.from_numpy(myvalid_output).float().to(device)\n",
    "        \n",
    "    valid_loss_min = np.inf\n",
    "    valid_losses = []\n",
    "    \n",
    "    valid_auc_max = np.NINF\n",
    "    valid_auces = []\n",
    "    \n",
    "    last_valid_loss = 100\n",
    "    last_valid_auc  = 100\n",
    "    \n",
    "    trigger_times = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        #----  shuffle the training set to avoid the batch(project) effects ----#\n",
    "        shuffleindex = list(range(mytrain_output.shape[0]))\n",
    "        random.shuffle(shuffleindex)\n",
    "        mytrain_output = mytrain_output[shuffleindex]\n",
    "        mytrain_input = mytrain_input[shuffleindex]\n",
    "        \n",
    "        #-------------- Batch-wise training model --------------#\n",
    "        model.train()\n",
    "        # train_loss = 0.0\n",
    "        train_num_correct = 0\n",
    "        train_prob = []\n",
    "        for batch_idx in range(0, mytrain_input.shape[0], batch_size):\n",
    "            \n",
    "            # subset a batch of sequences and class labels\n",
    "            tmpindex = list(range(batch_idx, min(batch_idx+batch_size, mytrain_input.shape[0])))\n",
    "            mytrain_input_batch  = mytrain_input[tmpindex,:]\n",
    "            mytrain_output_batch = mytrain_output[tmpindex,:]\n",
    "            \n",
    "            batchprior = sum(mytrain_output_batch[:,finalperiod-1,0])/mytrain_output_batch.shape[0]\n",
    "            class1ID_batch = mytrain_output_batch[:,finalperiod-1,0] == 1\n",
    "            class2ID_batch = mytrain_output_batch[:,finalperiod-1,0] == 0\n",
    "            \n",
    "            mytrain_input_batch  = torch.from_numpy(mytrain_input_batch).float().to(device)\n",
    "            mytrain_output_batch = torch.from_numpy(mytrain_output_batch).float().to(device)\n",
    "            \n",
    "            # forward pass of RNN model\n",
    "            output, hidden = model(mytrain_input_batch, device)\n",
    "            output = output.reshape((mytrain_output_batch.shape))\n",
    "            output_prob = nn.functional.softmax(output, dim=2)\n",
    "            # weighted MSE\n",
    "            loss = batchprior*criterion(output_prob[class1ID_batch,:,0], mytrain_output_batch[class1ID_batch,:,0]) + (1-batchprior)*criterion(output_prob[class2ID_batch,:,1], mytrain_output_batch[class2ID_batch,:,1])\n",
    "            # loss = criterion(output_prob, mytrain_output_batch)\n",
    "            # Clear existing gradients from previous epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Does backpropagation and calculates gradients\n",
    "            loss.backward()\n",
    "            # Updates the weights accordingly\n",
    "            optimizer.step()\n",
    "            # Number correct prediction on trainning set collection\n",
    "            tmppred = 1*(output_prob[:,finalperiod-1,0] > 0.5)\n",
    "            train_num_correct += sum(1*(tmppred == mytrain_output_batch[:,finalperiod-1,0]))\n",
    "            # Training function loss collection\n",
    "            # train_loss += loss.item()\n",
    "            train_prob = np.concatenate((train_prob, output_prob[:,finalperiod-1,0].cpu().detach().numpy()), axis=None)\n",
    "            \n",
    "        train_acc = (float(train_num_correct) / len(mytrain_output))*100\n",
    "        train_auc = metrics.roc_auc_score(mytrain_output[:,finalperiod-1,0], train_prob)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Training loss calculation\n",
    "        tmpmytrain_input  = torch.from_numpy(mytrain_input).float().to(device)\n",
    "        tmpmytrain_output = torch.from_numpy(mytrain_output).float().to(device)\n",
    "        tmpoutputtrain, tmphidden = model(tmpmytrain_input, device)\n",
    "        tmpoutputtrain = tmpoutputtrain.reshape((tmpmytrain_output.shape))\n",
    "        tmpoutputtrain_prob = nn.functional.softmax(tmpoutputtrain, dim=2)\n",
    "        # train_loss = criterion(tmpoutputtrain_prob, tmpmytrain_output)\n",
    "        train_loss = trainprior*criterion(tmpoutputtrain_prob[class1ID_train,:,0], tmpmytrain_output[class1ID_train,:,0]) + (1-trainprior)*criterion(tmpoutputtrain_prob[class2ID_train,:,1], tmpmytrain_output[class2ID_train,:,1])\n",
    "        history_train_valid['TrainLoss'].append(train_loss.item())\n",
    "        history_train_valid['TrainAcc'].append(train_acc)\n",
    "        history_train_valid['TrainAUC'].append(train_auc)\n",
    "        \n",
    "\n",
    "        #--------------       Validate model      --------------#\n",
    "        outputvalid, hidden = model(myvalid_input, device)\n",
    "        outputvalid = outputvalid.reshape((myvalid_output.shape))\n",
    "        outputvalid_prob = nn.functional.softmax(outputvalid, dim=2)\n",
    "        # validation loss\n",
    "        # valid_loss = criterion(outputvalid_prob, myvalid_output)\n",
    "        valid_loss = validprior*criterion(outputvalid_prob[class1ID_valid,:,0], myvalid_output[class1ID_valid,:,0]) + (1-validprior)*criterion(outputvalid_prob[class2ID_valid,:,1], myvalid_output[class2ID_valid,:,1])\n",
    "        # Number correct prediction on trainning set collection\n",
    "        tmppredprob = outputvalid_prob[:,finalperiod-1,0].cpu().detach().numpy()\n",
    "        tmppred = 1*(tmppredprob > 0.5)\n",
    "        tmpobs = myvalid_output[:,finalperiod-1,0].cpu().detach().numpy()\n",
    "        valid_num_correct = sum(1*(tmppred == tmpobs))\n",
    "        valid_acc = (float(valid_num_correct) / len(myvalid_output))*100\n",
    "        valid_auc = metrics.roc_auc_score(tmpobs, tmppredprob)\n",
    "        \n",
    "        history_train_valid['ValidLoss'].append(valid_loss.item())\n",
    "        history_train_valid['ValidAcc'].append(valid_acc)\n",
    "        history_train_valid['ValidAUC'].append(valid_auc)\n",
    "        \n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[E {epoch + 1}/{max_epochs}]'\n",
    "                  f\" T.Loss: {history_train_valid['TrainLoss'][-1]:.4f}, T.Acc: {history_train_valid['TrainAcc'][-1]:2.2f}, T.AUC: {history_train_valid['TrainAUC'][-1]:.4f}\"\n",
    "                  f\" V.Loss: {history_train_valid['ValidLoss'][-1]:.4f}, V.Acc: {history_train_valid['ValidAcc'][-1]:2.2f}, V.AUC: {history_train_valid['ValidAUC'][-1]:.4f};\")\n",
    "        \n",
    "        valid_auces.append(valid_auc.item())\n",
    "        valid_losses.append(valid_loss.item())\n",
    "        \n",
    "        if earlystop == \"auc\":\n",
    "            current_valid_auc = valid_auc\n",
    "            if current_valid_auc < last_valid_auc:\n",
    "                trigger_times += 1\n",
    "                print('AUC Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping by AUC!.')\n",
    "                    break\n",
    "            else:\n",
    "                print('trigger times: 0')\n",
    "                trigger_times = 0\n",
    "            last_valid_auc = np.mean(valid_auces[-10:])\n",
    "        elif earlystop == \"loss\":\n",
    "            current_valid_loss = valid_loss\n",
    "            if current_valid_loss > last_valid_loss:\n",
    "                trigger_times += 1\n",
    "                print('Loss Trigger Times:', trigger_times)\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping by LOSS!.')\n",
    "                    break\n",
    "            else:\n",
    "                print('Trigger times >= patience: 0')\n",
    "                trigger_times = 0\n",
    "            last_valid_loss = np.mean(valid_losses[-10:])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # if earlystop == \"auc\":\n",
    "        #     # start to considering early-stop after 20 epoch\n",
    "        #     if epoch > 20:\n",
    "        #        if np.mean(valid_auces) < valid_auc_max:\n",
    "        #            print(\"Stopped here by AUC!\")\n",
    "        #            break\n",
    "        #        valid_auc_max = np.mean(valid_auces)\n",
    "        # elif earlystop == \"loss\":\n",
    "        #    # start to considering early-stop after 20 epoch\n",
    "        #    if epoch > 20:\n",
    "        #        if np.mean(valid_losses) > valid_loss_min:\n",
    "        #            print(\"Stopped here by LOSS!\")\n",
    "        #            break\n",
    "        #        # valid_loss_min = np.mean(valid_losses[-20:])\n",
    "        #        valid_loss_min = np.mean(valid_losses)\n",
    "        \n",
    "    return history_train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53778d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Mtd(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_Mtd, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=output_size, bias=False)\n",
    "\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "            \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "\n",
    "        outp = self.fc_1(outp)\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daed6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_Mtd(mytrain_input_mtd, mytrain_output, myvalid_input_mtd, myvalid_output, mytest_input_mtd, mytest_output, finalperiod):\n",
    "    \n",
    "    # 7 -> lstm -> 16 -> 8\n",
    "    \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_mtd.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 8\n",
    "    fc_size     = [16]\n",
    "    n_layers    = 1\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 200\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_Mtd = Model_Mtd(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, \n",
    "                          n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_Mtd.parameters(), lr=lr) \n",
    "    \n",
    "    print(\"################ Mtd LSTM training...\")\n",
    "    Mtd_hist = LSTMtrain(model_Mtd, device, criterion, optimizer, mytrain_input_mtd, mytrain_output, \n",
    "                         myvalid_input_mtd, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    Mtd_obs, Mtd_pred, Mtd_prob = evaluate(model_Mtd, device, mytest_input_mtd, mytest_output, finalperiod, cutoff=0.5)\n",
    "    Mtdtest_auc = metrics.roc_auc_score(Mtd_obs, Mtd_prob[:,0])\n",
    "    Mtdtest_acc = metrics.accuracy_score(Mtd_obs, Mtd_pred)\n",
    "    Mtdtest_conf = metrics.confusion_matrix(Mtd_obs, Mtd_pred)\n",
    "\n",
    "    return model_Mtd, Mtd_hist, Mtd_obs, Mtd_pred, Mtd_prob, Mtdtest_auc, Mtdtest_acc, Mtdtest_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5816b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_pty(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_pty, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=fc_size[2], bias=False)\n",
    "        self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        \n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_4(outp)   # 4th Ouuput\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910645cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_pty(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod):\n",
    "   \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_pty.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 128\n",
    "    n_layers    = 1\n",
    "    fc_size     = [256, 128, 64]\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.0001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 20\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_pty = Model_pty(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, \n",
    "                          n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_pty.parameters(), lr=lr) \n",
    "    \n",
    "    #---- training lstm ----#\n",
    "    print(\"################ pty LSTM training...\")\n",
    "    pty_hist = LSTMtrain(model_pty, device, criterion, optimizer, mytrain_input_pty, mytrain_output, \n",
    "                         myvalid_input_pty, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    pty_obs, pty_pred, pty_prob = evaluate(model_pty, device, mytest_input_pty, mytest_output, finalperiod, cutoff=0.5)\n",
    "    ptytest_auc = metrics.roc_auc_score(pty_obs, pty_prob[:,0])\n",
    "    ptytest_acc = metrics.accuracy_score(pty_obs, pty_pred)\n",
    "    ptytest_conf = metrics.confusion_matrix(pty_obs, pty_pred)\n",
    "\n",
    "    return model_pty, pty_hist, pty_obs, pty_pred, pty_prob, ptytest_auc, ptytest_acc, ptytest_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73926e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_txy(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_txy, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=fc_size[2], bias=False)\n",
    "        self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x, device):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size, device)\n",
    "        #------------ RNN  ------------#\n",
    "        # outp, hidden = self.rnn(x, h0)\n",
    "        #------------ LSTM ------------#\n",
    "        # c0 = self.init_hidden(batch_size, device)\n",
    "        # outp, hidden = self.lstm(x, (h0, c0))\n",
    "        #------------ GRU  ------------#\n",
    "        outp, hidden = self.gru(x, h0)\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_4(outp)   # 4th Ouuput\n",
    "        outp = self.tanh(outp)   # relu\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "426a2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstStage_txy(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod):\n",
    "    \n",
    "    #---- Hyper-parameter set-up ----#\n",
    "    input_size  = mytrain_input_txy.shape[2]\n",
    "    output_size = mytrain_output.shape[2]*finalperiod\n",
    "    seq_len     = finalperiod\n",
    "    hidden_dim  = 128\n",
    "    n_layers    = 1\n",
    "    fc_size     = [256, 128, 64]\n",
    "    \n",
    "    dropoutrate = 0.1\n",
    "    lr          = 0.0001\n",
    "    max_epochs  = 2000\n",
    "    batch_size  = 200\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_txy = Model_txy(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, \n",
    "                          seq_len=seq_len, n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model_txy.parameters(), lr=lr) \n",
    "    \n",
    "    print(\"################ txy LSTM training...\")\n",
    "    txy_hist = LSTMtrain(model_txy, device, criterion, optimizer, mytrain_input_txy, mytrain_output, \n",
    "                         myvalid_input_txy, myvalid_output, max_epochs, batch_size, finalperiod, patience=4, earlystop=\"loss\", verbose=True)\n",
    "    \n",
    "    #---- testing set evaluation ----#\n",
    "    txy_obs, txy_pred, txy_prob = evaluate(model_txy, device, mytest_input_txy, mytest_output, finalperiod, cutoff=0.5)\n",
    "    txytest_auc = metrics.roc_auc_score(txy_obs, txy_prob[:,0])\n",
    "    txytest_acc = metrics.accuracy_score(txy_obs, txy_pred)\n",
    "    txytest_conf = metrics.confusion_matrix(txy_obs, txy_pred)\n",
    "\n",
    "    return model_txy, txy_hist, txy_obs, txy_pred, txy_prob, txytest_auc, txytest_acc, txytest_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0661a",
   "metadata": {},
   "source": [
    "# Main script start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15b65d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Output: train/valid/test (before reshape)\n",
      "(862, 12)\n",
      "(433, 12)\n",
      "(130, 12)\n",
      "################ Output: train/valid/test (after reshape)\n",
      "(695, 4, 2)\n",
      "(346, 4, 2)\n",
      "(114, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "# data directory\n",
    "# meta_dir      = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/metadata/metadata.csv'\n",
    "meta_dir      = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/metadata_imputed1.csv'\n",
    "alpha_dir     = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/alpha_diversity/alpha_diversity.csv'\n",
    "cst_dir       = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/community_state_types/cst_valencia.csv'\n",
    "\n",
    "txy_dir_fam = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.family.csv'\n",
    "txy_dir_gen = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.genus.csv'\n",
    "txy_dir_spe = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.species.csv'\n",
    "\n",
    "pty_dir_1dot = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.1e0.csv'\n",
    "pty_dir_dot5 = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.5e_1.csv'\n",
    "pty_dir_dot1 = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.1e_1.csv'\n",
    "\n",
    "# krdwide_dir   = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/pairwise_distance/krd_distance_wide.csv'\n",
    "\n",
    "\n",
    "txy_dir = txy_dir_gen\n",
    "txy_feature_dir = \"/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/selectedfeature/txyfeature_was_early_preterm_gen.csv\"\n",
    "\n",
    "pty_dir = pty_dir_dot5\n",
    "pty_feature_dir = \"/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/selectedfeature/ptyfeature_was_early_preterm_dot5.csv\"\n",
    "\n",
    "# task = \"was_preterm\"\n",
    "# finalperiod = 5\n",
    "task = \"was_early_preterm\"\n",
    "finalperiod = 4\n",
    "\n",
    "myprop = [0.6, 0.3, 0.1]\n",
    "myseed = 0\n",
    "\n",
    "\n",
    "#-------------------------------------------#\n",
    "#---- Data Preparation                  ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "meta_data = metadata_loader(meta_dir, alpha_dir, cst_dir, task, finalperiod)\n",
    "\n",
    "#---- data set splitter ----#\n",
    "trainID, validID, testID = dataset_splitID(meta_data=meta_data, prop=myprop, myseed=myseed)\n",
    "\n",
    "#---- output loader ----#\n",
    "mytrain_output, myvalid_output, mytest_output = OutputLoader(meta_data, trainID, validID, testID, task, finalperiod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17876d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ meta:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(862, 9)\n",
      "(433, 9)\n",
      "(130, 9)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(695, 4, 7)\n",
      "(346, 4, 7)\n",
      "(114, 4, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ meta:\")\n",
    "mytrain_input_mtd, myvalid_input_mtd, mytest_input_mtd = InputLoaderMtd(meta_data, trainID, validID, testID, task, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803ac03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ pty:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(862, 258)\n",
      "(433, 258)\n",
      "(130, 258)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(695, 4, 256)\n",
      "(346, 4, 256)\n",
      "(114, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ pty:\")\n",
    "mytrain_input_pty, myvalid_input_pty, mytest_input_pty = InputLoader(pty_dir, pty_feature_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "815b14aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ txy:\n",
      "## Input: train/valid/test (before reshape)\n",
      "(862, 258)\n",
      "(433, 258)\n",
      "(130, 258)\n",
      "## Input: train/valid/test (after reshape)\n",
      "(695, 4, 256)\n",
      "(346, 4, 256)\n",
      "(114, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"################ txy:\")\n",
    "mytrain_input_txy, myvalid_input_txy, mytest_input_txy = InputLoader(txy_dir, txy_feature_dir, meta_data, trainID, validID, testID, myprop, myseed, finalperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06738bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ Mtd LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0966, T.Acc: 47.34, T.AUC: 0.5601 V.Loss: 0.0968, V.Acc: 85.55, V.AUC: 0.5876;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0930, T.Acc: 81.01, T.AUC: 0.5110 V.Loss: 0.0934, V.Acc: 84.97, V.AUC: 0.5909;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0896, T.Acc: 86.47, T.AUC: 0.5694 V.Loss: 0.0901, V.Acc: 85.84, V.AUC: 0.6043;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0862, T.Acc: 86.47, T.AUC: 0.5558 V.Loss: 0.0868, V.Acc: 85.84, V.AUC: 0.6111;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0826, T.Acc: 86.47, T.AUC: 0.4844 V.Loss: 0.0833, V.Acc: 85.84, V.AUC: 0.6138;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0787, T.Acc: 86.47, T.AUC: 0.5406 V.Loss: 0.0796, V.Acc: 85.84, V.AUC: 0.6135;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0746, T.Acc: 86.47, T.AUC: 0.5255 V.Loss: 0.0755, V.Acc: 85.84, V.AUC: 0.6107;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0700, T.Acc: 86.47, T.AUC: 0.4781 V.Loss: 0.0711, V.Acc: 85.84, V.AUC: 0.6094;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0654, T.Acc: 86.47, T.AUC: 0.5524 V.Loss: 0.0666, V.Acc: 85.84, V.AUC: 0.6066;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0607, T.Acc: 86.47, T.AUC: 0.4922 V.Loss: 0.0620, V.Acc: 85.84, V.AUC: 0.6049;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0563, T.Acc: 86.47, T.AUC: 0.5346 V.Loss: 0.0577, V.Acc: 85.84, V.AUC: 0.6017;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0526, T.Acc: 86.47, T.AUC: 0.5810 V.Loss: 0.0541, V.Acc: 85.84, V.AUC: 0.5973;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0497, T.Acc: 86.47, T.AUC: 0.5120 V.Loss: 0.0513, V.Acc: 85.84, V.AUC: 0.5833;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0477, T.Acc: 86.47, T.AUC: 0.5549 V.Loss: 0.0493, V.Acc: 85.84, V.AUC: 0.5784;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0465, T.Acc: 86.47, T.AUC: 0.5518 V.Loss: 0.0481, V.Acc: 85.84, V.AUC: 0.5780;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0458, T.Acc: 86.47, T.AUC: 0.5491 V.Loss: 0.0475, V.Acc: 85.84, V.AUC: 0.5778;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5976 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.5782;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5414 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.5808;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5671 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.5830;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5532 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.5891;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5468 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.5955;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5378 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6002;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.5011 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6077;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.5730 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6162;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5640 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6226;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5960 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6298;\n",
      "Trigger times >= patience: 0\n",
      "[E 27/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5528 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6357;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5373 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6406;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.6096 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6443;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.5647 V.Loss: 0.0464, V.Acc: 85.84, V.AUC: 0.6483;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.6285 V.Loss: 0.0464, V.Acc: 85.84, V.AUC: 0.6516;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.5438 V.Loss: 0.0464, V.Acc: 85.84, V.AUC: 0.6577;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.5321 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6621;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.5658 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6656;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.5889 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6693;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.5728 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6728;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.5943 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6748;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.5645 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6778;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.5676 V.Loss: 0.0461, V.Acc: 85.84, V.AUC: 0.6805;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.5188 V.Loss: 0.0461, V.Acc: 85.84, V.AUC: 0.6823;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.5580 V.Loss: 0.0461, V.Acc: 85.84, V.AUC: 0.6858;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.5711 V.Loss: 0.0460, V.Acc: 85.84, V.AUC: 0.6880;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.5696 V.Loss: 0.0460, V.Acc: 85.84, V.AUC: 0.6897;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.6513 V.Loss: 0.0459, V.Acc: 85.84, V.AUC: 0.6913;\n",
      "Trigger times >= patience: 0\n",
      "[E 45/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.6085 V.Loss: 0.0459, V.Acc: 85.84, V.AUC: 0.6926;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.6325 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6939;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.5959 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6947;\n",
      "Trigger times >= patience: 0\n",
      "[E 48/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.6322 V.Loss: 0.0457, V.Acc: 85.84, V.AUC: 0.6964;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.5789 V.Loss: 0.0457, V.Acc: 85.84, V.AUC: 0.6983;\n",
      "Trigger times >= patience: 0\n",
      "[E 50/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.5827 V.Loss: 0.0456, V.Acc: 85.84, V.AUC: 0.7000;\n",
      "Trigger times >= patience: 0\n",
      "[E 51/2000] T.Loss: 0.0443, T.Acc: 86.47, T.AUC: 0.6254 V.Loss: 0.0456, V.Acc: 85.84, V.AUC: 0.7025;\n",
      "Trigger times >= patience: 0\n",
      "[E 52/2000] T.Loss: 0.0443, T.Acc: 86.47, T.AUC: 0.6137 V.Loss: 0.0455, V.Acc: 85.84, V.AUC: 0.7064;\n",
      "Trigger times >= patience: 0\n",
      "[E 53/2000] T.Loss: 0.0443, T.Acc: 86.47, T.AUC: 0.5947 V.Loss: 0.0455, V.Acc: 85.84, V.AUC: 0.7089;\n",
      "Trigger times >= patience: 0\n",
      "[E 54/2000] T.Loss: 0.0442, T.Acc: 86.47, T.AUC: 0.5989 V.Loss: 0.0454, V.Acc: 85.84, V.AUC: 0.7108;\n",
      "Trigger times >= patience: 0\n",
      "[E 55/2000] T.Loss: 0.0442, T.Acc: 86.47, T.AUC: 0.6321 V.Loss: 0.0454, V.Acc: 85.84, V.AUC: 0.7136;\n",
      "Trigger times >= patience: 0\n",
      "[E 56/2000] T.Loss: 0.0442, T.Acc: 86.47, T.AUC: 0.5832 V.Loss: 0.0453, V.Acc: 85.84, V.AUC: 0.7161;\n",
      "Trigger times >= patience: 0\n",
      "[E 57/2000] T.Loss: 0.0441, T.Acc: 86.47, T.AUC: 0.6516 V.Loss: 0.0452, V.Acc: 85.84, V.AUC: 0.7172;\n",
      "Trigger times >= patience: 0\n",
      "[E 58/2000] T.Loss: 0.0441, T.Acc: 86.47, T.AUC: 0.6411 V.Loss: 0.0452, V.Acc: 85.84, V.AUC: 0.7180;\n",
      "Trigger times >= patience: 0\n",
      "[E 59/2000] T.Loss: 0.0441, T.Acc: 86.47, T.AUC: 0.6356 V.Loss: 0.0451, V.Acc: 85.84, V.AUC: 0.7190;\n",
      "Trigger times >= patience: 0\n",
      "[E 60/2000] T.Loss: 0.0440, T.Acc: 86.47, T.AUC: 0.6312 V.Loss: 0.0450, V.Acc: 85.84, V.AUC: 0.7196;\n",
      "Trigger times >= patience: 0\n",
      "[E 61/2000] T.Loss: 0.0440, T.Acc: 86.47, T.AUC: 0.6268 V.Loss: 0.0450, V.Acc: 85.84, V.AUC: 0.7201;\n",
      "Trigger times >= patience: 0\n",
      "[E 62/2000] T.Loss: 0.0439, T.Acc: 86.47, T.AUC: 0.6340 V.Loss: 0.0449, V.Acc: 85.84, V.AUC: 0.7200;\n",
      "Trigger times >= patience: 0\n",
      "[E 63/2000] T.Loss: 0.0439, T.Acc: 86.47, T.AUC: 0.6482 V.Loss: 0.0449, V.Acc: 85.84, V.AUC: 0.7203;\n",
      "Trigger times >= patience: 0\n",
      "[E 64/2000] T.Loss: 0.0439, T.Acc: 86.47, T.AUC: 0.6288 V.Loss: 0.0448, V.Acc: 85.84, V.AUC: 0.7208;\n",
      "Trigger times >= patience: 0\n",
      "[E 65/2000] T.Loss: 0.0438, T.Acc: 86.47, T.AUC: 0.6320 V.Loss: 0.0448, V.Acc: 85.84, V.AUC: 0.7220;\n",
      "Trigger times >= patience: 0\n",
      "[E 66/2000] T.Loss: 0.0438, T.Acc: 86.47, T.AUC: 0.6357 V.Loss: 0.0447, V.Acc: 85.84, V.AUC: 0.7227;\n",
      "Trigger times >= patience: 0\n",
      "[E 67/2000] T.Loss: 0.0438, T.Acc: 86.47, T.AUC: 0.6462 V.Loss: 0.0446, V.Acc: 85.84, V.AUC: 0.7227;\n",
      "Trigger times >= patience: 0\n",
      "[E 68/2000] T.Loss: 0.0437, T.Acc: 86.47, T.AUC: 0.6265 V.Loss: 0.0446, V.Acc: 85.84, V.AUC: 0.7245;\n",
      "Trigger times >= patience: 0\n",
      "[E 69/2000] T.Loss: 0.0437, T.Acc: 86.47, T.AUC: 0.6616 V.Loss: 0.0445, V.Acc: 85.84, V.AUC: 0.7253;\n",
      "Trigger times >= patience: 0\n",
      "[E 70/2000] T.Loss: 0.0436, T.Acc: 86.47, T.AUC: 0.6340 V.Loss: 0.0444, V.Acc: 85.84, V.AUC: 0.7251;\n",
      "Trigger times >= patience: 0\n",
      "[E 71/2000] T.Loss: 0.0436, T.Acc: 86.47, T.AUC: 0.6628 V.Loss: 0.0443, V.Acc: 85.84, V.AUC: 0.7247;\n",
      "Trigger times >= patience: 0\n",
      "[E 72/2000] T.Loss: 0.0436, T.Acc: 86.47, T.AUC: 0.6369 V.Loss: 0.0443, V.Acc: 85.84, V.AUC: 0.7242;\n",
      "Trigger times >= patience: 0\n",
      "[E 73/2000] T.Loss: 0.0435, T.Acc: 86.47, T.AUC: 0.6382 V.Loss: 0.0442, V.Acc: 85.84, V.AUC: 0.7243;\n",
      "Trigger times >= patience: 0\n",
      "[E 74/2000] T.Loss: 0.0435, T.Acc: 86.47, T.AUC: 0.6443 V.Loss: 0.0441, V.Acc: 85.84, V.AUC: 0.7244;\n",
      "Trigger times >= patience: 0\n",
      "[E 75/2000] T.Loss: 0.0434, T.Acc: 86.47, T.AUC: 0.6619 V.Loss: 0.0440, V.Acc: 85.84, V.AUC: 0.7240;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 76/2000] T.Loss: 0.0434, T.Acc: 86.47, T.AUC: 0.6440 V.Loss: 0.0440, V.Acc: 85.84, V.AUC: 0.7243;\n",
      "Trigger times >= patience: 0\n",
      "[E 77/2000] T.Loss: 0.0433, T.Acc: 86.47, T.AUC: 0.6501 V.Loss: 0.0439, V.Acc: 85.84, V.AUC: 0.7243;\n",
      "Trigger times >= patience: 0\n",
      "[E 78/2000] T.Loss: 0.0433, T.Acc: 86.47, T.AUC: 0.6755 V.Loss: 0.0438, V.Acc: 85.84, V.AUC: 0.7249;\n",
      "Trigger times >= patience: 0\n",
      "[E 79/2000] T.Loss: 0.0432, T.Acc: 86.47, T.AUC: 0.6516 V.Loss: 0.0437, V.Acc: 85.84, V.AUC: 0.7250;\n",
      "Trigger times >= patience: 0\n",
      "[E 80/2000] T.Loss: 0.0432, T.Acc: 86.47, T.AUC: 0.6378 V.Loss: 0.0436, V.Acc: 85.84, V.AUC: 0.7246;\n",
      "Trigger times >= patience: 0\n",
      "[E 81/2000] T.Loss: 0.0431, T.Acc: 86.47, T.AUC: 0.6445 V.Loss: 0.0436, V.Acc: 85.84, V.AUC: 0.7249;\n",
      "Trigger times >= patience: 0\n",
      "[E 82/2000] T.Loss: 0.0431, T.Acc: 86.47, T.AUC: 0.6597 V.Loss: 0.0435, V.Acc: 85.84, V.AUC: 0.7245;\n",
      "Trigger times >= patience: 0\n",
      "[E 83/2000] T.Loss: 0.0430, T.Acc: 86.47, T.AUC: 0.6744 V.Loss: 0.0434, V.Acc: 85.84, V.AUC: 0.7247;\n",
      "Trigger times >= patience: 0\n",
      "[E 84/2000] T.Loss: 0.0430, T.Acc: 86.47, T.AUC: 0.6599 V.Loss: 0.0433, V.Acc: 85.84, V.AUC: 0.7247;\n",
      "Trigger times >= patience: 0\n",
      "[E 85/2000] T.Loss: 0.0429, T.Acc: 86.47, T.AUC: 0.6776 V.Loss: 0.0433, V.Acc: 85.84, V.AUC: 0.7249;\n",
      "Trigger times >= patience: 0\n",
      "[E 86/2000] T.Loss: 0.0429, T.Acc: 86.47, T.AUC: 0.6420 V.Loss: 0.0432, V.Acc: 85.84, V.AUC: 0.7245;\n",
      "Trigger times >= patience: 0\n",
      "[E 87/2000] T.Loss: 0.0428, T.Acc: 86.47, T.AUC: 0.6753 V.Loss: 0.0431, V.Acc: 85.84, V.AUC: 0.7249;\n",
      "Trigger times >= patience: 0\n",
      "[E 88/2000] T.Loss: 0.0428, T.Acc: 86.47, T.AUC: 0.6527 V.Loss: 0.0430, V.Acc: 85.84, V.AUC: 0.7248;\n",
      "Trigger times >= patience: 0\n",
      "[E 89/2000] T.Loss: 0.0427, T.Acc: 86.47, T.AUC: 0.6485 V.Loss: 0.0429, V.Acc: 85.84, V.AUC: 0.7248;\n",
      "Trigger times >= patience: 0\n",
      "[E 90/2000] T.Loss: 0.0427, T.Acc: 86.47, T.AUC: 0.6636 V.Loss: 0.0428, V.Acc: 85.84, V.AUC: 0.7249;\n",
      "Trigger times >= patience: 0\n",
      "[E 91/2000] T.Loss: 0.0426, T.Acc: 86.47, T.AUC: 0.6583 V.Loss: 0.0427, V.Acc: 85.84, V.AUC: 0.7252;\n",
      "Trigger times >= patience: 0\n",
      "[E 92/2000] T.Loss: 0.0426, T.Acc: 86.47, T.AUC: 0.6476 V.Loss: 0.0426, V.Acc: 85.84, V.AUC: 0.7251;\n",
      "Trigger times >= patience: 0\n",
      "[E 93/2000] T.Loss: 0.0425, T.Acc: 86.47, T.AUC: 0.6561 V.Loss: 0.0425, V.Acc: 85.84, V.AUC: 0.7242;\n",
      "Trigger times >= patience: 0\n",
      "[E 94/2000] T.Loss: 0.0425, T.Acc: 86.47, T.AUC: 0.6489 V.Loss: 0.0425, V.Acc: 85.84, V.AUC: 0.7229;\n",
      "Trigger times >= patience: 0\n",
      "[E 95/2000] T.Loss: 0.0424, T.Acc: 86.47, T.AUC: 0.6552 V.Loss: 0.0424, V.Acc: 85.84, V.AUC: 0.7225;\n",
      "Trigger times >= patience: 0\n",
      "[E 96/2000] T.Loss: 0.0424, T.Acc: 86.47, T.AUC: 0.6633 V.Loss: 0.0423, V.Acc: 85.84, V.AUC: 0.7227;\n",
      "Trigger times >= patience: 0\n",
      "[E 97/2000] T.Loss: 0.0423, T.Acc: 86.47, T.AUC: 0.6619 V.Loss: 0.0422, V.Acc: 85.84, V.AUC: 0.7223;\n",
      "Trigger times >= patience: 0\n",
      "[E 98/2000] T.Loss: 0.0423, T.Acc: 86.47, T.AUC: 0.6827 V.Loss: 0.0422, V.Acc: 85.84, V.AUC: 0.7219;\n",
      "Trigger times >= patience: 0\n",
      "[E 99/2000] T.Loss: 0.0422, T.Acc: 86.47, T.AUC: 0.6474 V.Loss: 0.0421, V.Acc: 85.84, V.AUC: 0.7224;\n",
      "Trigger times >= patience: 0\n",
      "[E 100/2000] T.Loss: 0.0422, T.Acc: 86.47, T.AUC: 0.6983 V.Loss: 0.0420, V.Acc: 85.84, V.AUC: 0.7228;\n",
      "Trigger times >= patience: 0\n",
      "[E 101/2000] T.Loss: 0.0421, T.Acc: 86.47, T.AUC: 0.6690 V.Loss: 0.0419, V.Acc: 85.84, V.AUC: 0.7229;\n",
      "Trigger times >= patience: 0\n",
      "[E 102/2000] T.Loss: 0.0421, T.Acc: 86.47, T.AUC: 0.6760 V.Loss: 0.0418, V.Acc: 85.84, V.AUC: 0.7227;\n",
      "Trigger times >= patience: 0\n",
      "[E 103/2000] T.Loss: 0.0420, T.Acc: 86.47, T.AUC: 0.6447 V.Loss: 0.0418, V.Acc: 85.84, V.AUC: 0.7226;\n",
      "Trigger times >= patience: 0\n",
      "[E 104/2000] T.Loss: 0.0420, T.Acc: 86.47, T.AUC: 0.6674 V.Loss: 0.0417, V.Acc: 85.84, V.AUC: 0.7225;\n",
      "Trigger times >= patience: 0\n",
      "[E 105/2000] T.Loss: 0.0420, T.Acc: 86.47, T.AUC: 0.6662 V.Loss: 0.0417, V.Acc: 85.84, V.AUC: 0.7223;\n",
      "Trigger times >= patience: 0\n",
      "[E 106/2000] T.Loss: 0.0419, T.Acc: 86.47, T.AUC: 0.6719 V.Loss: 0.0416, V.Acc: 85.84, V.AUC: 0.7221;\n",
      "Trigger times >= patience: 0\n",
      "[E 107/2000] T.Loss: 0.0419, T.Acc: 86.62, T.AUC: 0.6866 V.Loss: 0.0415, V.Acc: 85.84, V.AUC: 0.7219;\n",
      "Trigger times >= patience: 0\n",
      "[E 108/2000] T.Loss: 0.0418, T.Acc: 86.62, T.AUC: 0.6656 V.Loss: 0.0415, V.Acc: 85.84, V.AUC: 0.7213;\n",
      "Trigger times >= patience: 0\n",
      "[E 109/2000] T.Loss: 0.0418, T.Acc: 86.62, T.AUC: 0.6630 V.Loss: 0.0414, V.Acc: 85.84, V.AUC: 0.7210;\n",
      "Trigger times >= patience: 0\n",
      "[E 110/2000] T.Loss: 0.0417, T.Acc: 86.62, T.AUC: 0.6774 V.Loss: 0.0414, V.Acc: 85.84, V.AUC: 0.7210;\n",
      "Trigger times >= patience: 0\n",
      "[E 111/2000] T.Loss: 0.0417, T.Acc: 86.62, T.AUC: 0.6610 V.Loss: 0.0413, V.Acc: 85.84, V.AUC: 0.7212;\n",
      "Trigger times >= patience: 0\n",
      "[E 112/2000] T.Loss: 0.0417, T.Acc: 86.47, T.AUC: 0.6534 V.Loss: 0.0413, V.Acc: 85.84, V.AUC: 0.7207;\n",
      "Trigger times >= patience: 0\n",
      "[E 113/2000] T.Loss: 0.0417, T.Acc: 86.62, T.AUC: 0.6710 V.Loss: 0.0412, V.Acc: 86.13, V.AUC: 0.7205;\n",
      "Trigger times >= patience: 0\n",
      "[E 114/2000] T.Loss: 0.0416, T.Acc: 86.47, T.AUC: 0.6875 V.Loss: 0.0412, V.Acc: 86.13, V.AUC: 0.7210;\n",
      "Trigger times >= patience: 0\n",
      "[E 115/2000] T.Loss: 0.0416, T.Acc: 86.62, T.AUC: 0.6951 V.Loss: 0.0412, V.Acc: 86.71, V.AUC: 0.7213;\n",
      "Trigger times >= patience: 0\n",
      "[E 116/2000] T.Loss: 0.0416, T.Acc: 86.62, T.AUC: 0.6653 V.Loss: 0.0412, V.Acc: 86.99, V.AUC: 0.7213;\n",
      "Trigger times >= patience: 0\n",
      "[E 117/2000] T.Loss: 0.0415, T.Acc: 86.76, T.AUC: 0.6579 V.Loss: 0.0411, V.Acc: 86.99, V.AUC: 0.7214;\n",
      "Trigger times >= patience: 0\n",
      "[E 118/2000] T.Loss: 0.0415, T.Acc: 86.76, T.AUC: 0.6712 V.Loss: 0.0411, V.Acc: 86.71, V.AUC: 0.7214;\n",
      "Trigger times >= patience: 0\n",
      "[E 119/2000] T.Loss: 0.0415, T.Acc: 86.76, T.AUC: 0.6815 V.Loss: 0.0410, V.Acc: 86.71, V.AUC: 0.7218;\n",
      "Trigger times >= patience: 0\n",
      "[E 120/2000] T.Loss: 0.0414, T.Acc: 86.91, T.AUC: 0.6706 V.Loss: 0.0410, V.Acc: 86.71, V.AUC: 0.7218;\n",
      "Trigger times >= patience: 0\n",
      "[E 121/2000] T.Loss: 0.0414, T.Acc: 87.05, T.AUC: 0.6641 V.Loss: 0.0409, V.Acc: 86.99, V.AUC: 0.7223;\n",
      "Trigger times >= patience: 0\n",
      "[E 122/2000] T.Loss: 0.0414, T.Acc: 87.05, T.AUC: 0.6848 V.Loss: 0.0409, V.Acc: 86.99, V.AUC: 0.7227;\n",
      "Trigger times >= patience: 0\n",
      "[E 123/2000] T.Loss: 0.0413, T.Acc: 87.19, T.AUC: 0.6765 V.Loss: 0.0409, V.Acc: 86.99, V.AUC: 0.7230;\n",
      "Trigger times >= patience: 0\n",
      "[E 124/2000] T.Loss: 0.0413, T.Acc: 87.05, T.AUC: 0.6817 V.Loss: 0.0409, V.Acc: 86.71, V.AUC: 0.7234;\n",
      "Trigger times >= patience: 0\n",
      "[E 125/2000] T.Loss: 0.0413, T.Acc: 86.76, T.AUC: 0.6780 V.Loss: 0.0408, V.Acc: 86.99, V.AUC: 0.7239;\n",
      "Trigger times >= patience: 0\n",
      "[E 126/2000] T.Loss: 0.0413, T.Acc: 87.48, T.AUC: 0.6631 V.Loss: 0.0408, V.Acc: 87.28, V.AUC: 0.7235;\n",
      "Trigger times >= patience: 0\n",
      "[E 127/2000] T.Loss: 0.0412, T.Acc: 87.19, T.AUC: 0.6852 V.Loss: 0.0408, V.Acc: 87.28, V.AUC: 0.7240;\n",
      "Trigger times >= patience: 0\n",
      "[E 128/2000] T.Loss: 0.0412, T.Acc: 87.34, T.AUC: 0.6824 V.Loss: 0.0407, V.Acc: 86.99, V.AUC: 0.7250;\n",
      "Trigger times >= patience: 0\n",
      "[E 129/2000] T.Loss: 0.0412, T.Acc: 87.19, T.AUC: 0.6782 V.Loss: 0.0407, V.Acc: 87.28, V.AUC: 0.7258;\n",
      "Trigger times >= patience: 0\n",
      "[E 130/2000] T.Loss: 0.0412, T.Acc: 87.19, T.AUC: 0.6681 V.Loss: 0.0407, V.Acc: 86.99, V.AUC: 0.7255;\n",
      "Trigger times >= patience: 0\n",
      "[E 131/2000] T.Loss: 0.0412, T.Acc: 87.19, T.AUC: 0.6775 V.Loss: 0.0407, V.Acc: 86.99, V.AUC: 0.7256;\n",
      "Trigger times >= patience: 0\n",
      "[E 132/2000] T.Loss: 0.0412, T.Acc: 87.34, T.AUC: 0.6679 V.Loss: 0.0407, V.Acc: 87.28, V.AUC: 0.7258;\n",
      "Trigger times >= patience: 0\n",
      "[E 133/2000] T.Loss: 0.0411, T.Acc: 86.91, T.AUC: 0.6885 V.Loss: 0.0406, V.Acc: 86.99, V.AUC: 0.7262;\n",
      "Trigger times >= patience: 0\n",
      "[E 134/2000] T.Loss: 0.0411, T.Acc: 87.34, T.AUC: 0.6521 V.Loss: 0.0406, V.Acc: 87.28, V.AUC: 0.7264;\n",
      "Trigger times >= patience: 0\n",
      "[E 135/2000] T.Loss: 0.0411, T.Acc: 87.05, T.AUC: 0.6628 V.Loss: 0.0407, V.Acc: 87.57, V.AUC: 0.7272;\n",
      "Trigger times >= patience: 0\n",
      "[E 136/2000] T.Loss: 0.0411, T.Acc: 87.34, T.AUC: 0.6656 V.Loss: 0.0406, V.Acc: 87.57, V.AUC: 0.7273;\n",
      "Trigger times >= patience: 0\n",
      "[E 137/2000] T.Loss: 0.0410, T.Acc: 87.48, T.AUC: 0.6494 V.Loss: 0.0406, V.Acc: 87.57, V.AUC: 0.7277;\n",
      "Trigger times >= patience: 0\n",
      "[E 138/2000] T.Loss: 0.0410, T.Acc: 87.77, T.AUC: 0.6830 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7277;\n",
      "Trigger times >= patience: 0\n",
      "[E 139/2000] T.Loss: 0.0410, T.Acc: 87.63, T.AUC: 0.7030 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7282;\n",
      "Trigger times >= patience: 0\n",
      "[E 140/2000] T.Loss: 0.0410, T.Acc: 87.48, T.AUC: 0.7000 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7282;\n",
      "Trigger times >= patience: 0\n",
      "[E 141/2000] T.Loss: 0.0410, T.Acc: 87.48, T.AUC: 0.6588 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7279;\n",
      "Trigger times >= patience: 0\n",
      "[E 142/2000] T.Loss: 0.0409, T.Acc: 87.34, T.AUC: 0.6929 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7278;\n",
      "Trigger times >= patience: 0\n",
      "[E 143/2000] T.Loss: 0.0409, T.Acc: 87.34, T.AUC: 0.6790 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7275;\n",
      "Trigger times >= patience: 0\n",
      "[E 144/2000] T.Loss: 0.0409, T.Acc: 87.34, T.AUC: 0.6721 V.Loss: 0.0405, V.Acc: 87.86, V.AUC: 0.7275;\n",
      "Trigger times >= patience: 0\n",
      "[E 145/2000] T.Loss: 0.0409, T.Acc: 87.34, T.AUC: 0.6601 V.Loss: 0.0405, V.Acc: 87.86, V.AUC: 0.7266;\n",
      "Trigger times >= patience: 0\n",
      "[E 146/2000] T.Loss: 0.0409, T.Acc: 87.34, T.AUC: 0.6832 V.Loss: 0.0405, V.Acc: 87.86, V.AUC: 0.7267;\n",
      "Trigger times >= patience: 0\n",
      "[E 147/2000] T.Loss: 0.0409, T.Acc: 87.77, T.AUC: 0.6775 V.Loss: 0.0405, V.Acc: 87.86, V.AUC: 0.7276;\n",
      "Trigger times >= patience: 0\n",
      "[E 148/2000] T.Loss: 0.0408, T.Acc: 87.63, T.AUC: 0.6746 V.Loss: 0.0405, V.Acc: 87.86, V.AUC: 0.7280;\n",
      "Trigger times >= patience: 0\n",
      "[E 149/2000] T.Loss: 0.0408, T.Acc: 87.63, T.AUC: 0.6712 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7277;\n",
      "Trigger times >= patience: 0\n",
      "[E 150/2000] T.Loss: 0.0408, T.Acc: 87.48, T.AUC: 0.6529 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7278;\n",
      "Loss Trigger Times: 1\n",
      "[E 151/2000] T.Loss: 0.0408, T.Acc: 87.63, T.AUC: 0.6819 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7277;\n",
      "Loss Trigger Times: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 152/2000] T.Loss: 0.0408, T.Acc: 87.19, T.AUC: 0.6606 V.Loss: 0.0405, V.Acc: 87.86, V.AUC: 0.7281;\n",
      "Loss Trigger Times: 3\n",
      "[E 153/2000] T.Loss: 0.0408, T.Acc: 87.48, T.AUC: 0.6695 V.Loss: 0.0405, V.Acc: 87.57, V.AUC: 0.7285;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.8421052631578947\n",
      "0.8526595744680852\n",
      "[[94  0]\n",
      " [18  2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/Mtd_wasearlypreterm.save']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: Metadata             ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_Mtd, Mtd_hist, Mtdtest_obs, Mtdtest_pred, Mtdtest_prob, Mtdtest_auc, Mtdtest_acc, Mtdtest_conf = FirstStage_Mtd(mytrain_input_mtd, mytrain_output, myvalid_input_mtd, myvalid_output, mytest_input_mtd, mytest_output, finalperiod)\n",
    "print(Mtdtest_acc)\n",
    "print(Mtdtest_auc)\n",
    "print(Mtdtest_conf)\n",
    "dump(model_Mtd, '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/Mtd_wasearlypreterm.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d9b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ pty LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0645, T.Acc: 86.04, T.AUC: 0.4682 V.Loss: 0.0652, V.Acc: 85.84, V.AUC: 0.4553;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0458, T.Acc: 86.47, T.AUC: 0.5579 V.Loss: 0.0476, V.Acc: 85.84, V.AUC: 0.5105;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5072 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5459;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4517 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.5792;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4932 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6194;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4768 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6332;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5164 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6210;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4990 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6217;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4932 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6204;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5484 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6238;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5163 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6261;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5599 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6253;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5447 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6252;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5279 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6239;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5172 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6229;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5586 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6226;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5120 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6228;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5678 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6220;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.4534 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6219;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.5600 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6205;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5698 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6206;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5671 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6199;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.5420 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6204;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.5819 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6205;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.5463 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6210;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.5608 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6212;\n",
      "Trigger times >= patience: 0\n",
      "[E 27/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.5916 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6188;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.5961 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6189;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.5820 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6184;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.6768 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6167;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0442, T.Acc: 86.47, T.AUC: 0.5967 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6184;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0441, T.Acc: 86.47, T.AUC: 0.6438 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6190;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0440, T.Acc: 86.47, T.AUC: 0.6206 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6178;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0439, T.Acc: 86.47, T.AUC: 0.6934 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6193;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0438, T.Acc: 86.47, T.AUC: 0.6224 V.Loss: 0.0464, V.Acc: 85.84, V.AUC: 0.6195;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0435, T.Acc: 86.47, T.AUC: 0.6346 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6198;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0435, T.Acc: 86.33, T.AUC: 0.6369 V.Loss: 0.0464, V.Acc: 85.84, V.AUC: 0.6205;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0432, T.Acc: 86.33, T.AUC: 0.7292 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6217;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0430, T.Acc: 86.33, T.AUC: 0.6758 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6216;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0428, T.Acc: 86.33, T.AUC: 0.6559 V.Loss: 0.0462, V.Acc: 85.55, V.AUC: 0.6235;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0423, T.Acc: 86.33, T.AUC: 0.6415 V.Loss: 0.0460, V.Acc: 85.55, V.AUC: 0.6237;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0421, T.Acc: 86.19, T.AUC: 0.6809 V.Loss: 0.0459, V.Acc: 85.55, V.AUC: 0.6233;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0427, T.Acc: 86.47, T.AUC: 0.7053 V.Loss: 0.0464, V.Acc: 85.55, V.AUC: 0.6237;\n",
      "Loss Trigger Times: 1\n",
      "[E 44/2000] T.Loss: 0.0418, T.Acc: 86.62, T.AUC: 0.6903 V.Loss: 0.0463, V.Acc: 85.55, V.AUC: 0.6240;\n",
      "Loss Trigger Times: 2\n",
      "[E 45/2000] T.Loss: 0.0428, T.Acc: 86.47, T.AUC: 0.7078 V.Loss: 0.0480, V.Acc: 83.24, V.AUC: 0.6226;\n",
      "Loss Trigger Times: 3\n",
      "[E 46/2000] T.Loss: 0.0410, T.Acc: 86.62, T.AUC: 0.6709 V.Loss: 0.0458, V.Acc: 85.55, V.AUC: 0.6254;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0414, T.Acc: 86.62, T.AUC: 0.6890 V.Loss: 0.0470, V.Acc: 83.82, V.AUC: 0.6254;\n",
      "Loss Trigger Times: 1\n",
      "[E 48/2000] T.Loss: 0.0406, T.Acc: 87.19, T.AUC: 0.6575 V.Loss: 0.0461, V.Acc: 84.39, V.AUC: 0.6299;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0405, T.Acc: 86.91, T.AUC: 0.6858 V.Loss: 0.0463, V.Acc: 84.39, V.AUC: 0.6316;\n",
      "Trigger times >= patience: 0\n",
      "[E 50/2000] T.Loss: 0.0409, T.Acc: 86.76, T.AUC: 0.6678 V.Loss: 0.0468, V.Acc: 84.10, V.AUC: 0.6361;\n",
      "Loss Trigger Times: 1\n",
      "[E 51/2000] T.Loss: 0.0400, T.Acc: 86.91, T.AUC: 0.7079 V.Loss: 0.0459, V.Acc: 85.26, V.AUC: 0.6346;\n",
      "Trigger times >= patience: 0\n",
      "[E 52/2000] T.Loss: 0.0408, T.Acc: 86.62, T.AUC: 0.7290 V.Loss: 0.0463, V.Acc: 85.55, V.AUC: 0.6325;\n",
      "Trigger times >= patience: 0\n",
      "[E 53/2000] T.Loss: 0.0401, T.Acc: 87.19, T.AUC: 0.7040 V.Loss: 0.0460, V.Acc: 85.55, V.AUC: 0.6296;\n",
      "Trigger times >= patience: 0\n",
      "[E 54/2000] T.Loss: 0.0394, T.Acc: 87.19, T.AUC: 0.7020 V.Loss: 0.0460, V.Acc: 84.97, V.AUC: 0.6285;\n",
      "Trigger times >= patience: 0\n",
      "[E 55/2000] T.Loss: 0.0401, T.Acc: 87.19, T.AUC: 0.6878 V.Loss: 0.0469, V.Acc: 83.82, V.AUC: 0.6292;\n",
      "Loss Trigger Times: 1\n",
      "[E 56/2000] T.Loss: 0.0392, T.Acc: 87.34, T.AUC: 0.7240 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6282;\n",
      "Trigger times >= patience: 0\n",
      "[E 57/2000] T.Loss: 0.0404, T.Acc: 87.19, T.AUC: 0.7374 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6306;\n",
      "Loss Trigger Times: 1\n",
      "[E 58/2000] T.Loss: 0.0391, T.Acc: 87.77, T.AUC: 0.6862 V.Loss: 0.0460, V.Acc: 85.55, V.AUC: 0.6338;\n",
      "Trigger times >= patience: 0\n",
      "[E 59/2000] T.Loss: 0.0394, T.Acc: 87.05, T.AUC: 0.7284 V.Loss: 0.0461, V.Acc: 85.55, V.AUC: 0.6338;\n",
      "Trigger times >= patience: 0\n",
      "[E 60/2000] T.Loss: 0.0389, T.Acc: 86.76, T.AUC: 0.7182 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6354;\n",
      "Trigger times >= patience: 0\n",
      "[E 61/2000] T.Loss: 0.0388, T.Acc: 87.77, T.AUC: 0.7053 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6353;\n",
      "Trigger times >= patience: 0\n",
      "[E 62/2000] T.Loss: 0.0399, T.Acc: 87.91, T.AUC: 0.7371 V.Loss: 0.0474, V.Acc: 84.10, V.AUC: 0.6335;\n",
      "Loss Trigger Times: 1\n",
      "[E 63/2000] T.Loss: 0.0386, T.Acc: 87.63, T.AUC: 0.7070 V.Loss: 0.0459, V.Acc: 85.84, V.AUC: 0.6368;\n",
      "Trigger times >= patience: 0\n",
      "[E 64/2000] T.Loss: 0.0389, T.Acc: 87.19, T.AUC: 0.6983 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6353;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 65/2000] T.Loss: 0.0387, T.Acc: 87.91, T.AUC: 0.7434 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6336;\n",
      "Trigger times >= patience: 0\n",
      "[E 66/2000] T.Loss: 0.0392, T.Acc: 87.63, T.AUC: 0.7342 V.Loss: 0.0461, V.Acc: 85.55, V.AUC: 0.6309;\n",
      "Loss Trigger Times: 1\n",
      "[E 67/2000] T.Loss: 0.0384, T.Acc: 87.63, T.AUC: 0.7215 V.Loss: 0.0458, V.Acc: 86.13, V.AUC: 0.6346;\n",
      "Trigger times >= patience: 0\n",
      "[E 68/2000] T.Loss: 0.0384, T.Acc: 87.77, T.AUC: 0.7236 V.Loss: 0.0460, V.Acc: 86.13, V.AUC: 0.6350;\n",
      "Trigger times >= patience: 0\n",
      "[E 69/2000] T.Loss: 0.0384, T.Acc: 87.63, T.AUC: 0.7507 V.Loss: 0.0458, V.Acc: 86.13, V.AUC: 0.6359;\n",
      "Trigger times >= patience: 0\n",
      "[E 70/2000] T.Loss: 0.0383, T.Acc: 87.63, T.AUC: 0.7395 V.Loss: 0.0460, V.Acc: 85.84, V.AUC: 0.6333;\n",
      "Loss Trigger Times: 1\n",
      "[E 71/2000] T.Loss: 0.0384, T.Acc: 87.77, T.AUC: 0.7259 V.Loss: 0.0460, V.Acc: 86.13, V.AUC: 0.6337;\n",
      "Trigger times >= patience: 0\n",
      "[E 72/2000] T.Loss: 0.0384, T.Acc: 87.91, T.AUC: 0.7466 V.Loss: 0.0461, V.Acc: 86.13, V.AUC: 0.6343;\n",
      "Loss Trigger Times: 1\n",
      "[E 73/2000] T.Loss: 0.0381, T.Acc: 87.19, T.AUC: 0.7211 V.Loss: 0.0459, V.Acc: 86.13, V.AUC: 0.6358;\n",
      "Loss Trigger Times: 2\n",
      "[E 74/2000] T.Loss: 0.0380, T.Acc: 87.63, T.AUC: 0.7403 V.Loss: 0.0460, V.Acc: 86.13, V.AUC: 0.6347;\n",
      "Loss Trigger Times: 3\n",
      "[E 75/2000] T.Loss: 0.0379, T.Acc: 87.63, T.AUC: 0.7618 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6342;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.8421052631578947\n",
      "0.5784574468085106\n",
      "[[94  0]\n",
      " [18  2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/pty_wasearlypreterm.save']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: phylotype data       ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_pty, pty_hist, ptytest_obs, ptytest_pred, ptytest_prob, ptytest_auc, ptytest_acc, ptytest_conf = FirstStage_pty(mytrain_input_pty, mytrain_output, myvalid_input_pty, myvalid_output, mytest_input_pty, mytest_output, finalperiod)\n",
    "print(ptytest_acc)\n",
    "print(ptytest_auc)\n",
    "print(ptytest_conf)\n",
    "dump(model_pty, '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/pty_wasearlypreterm.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b67692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ txy LSTM training...\n",
      "[E 1/2000] T.Loss: 0.0955, T.Acc: 54.10, T.AUC: 0.5437 V.Loss: 0.0955, V.Acc: 85.84, V.AUC: 0.5265;\n",
      "Trigger times >= patience: 0\n",
      "[E 2/2000] T.Loss: 0.0931, T.Acc: 86.47, T.AUC: 0.5375 V.Loss: 0.0932, V.Acc: 85.84, V.AUC: 0.5026;\n",
      "Trigger times >= patience: 0\n",
      "[E 3/2000] T.Loss: 0.0905, T.Acc: 86.47, T.AUC: 0.4978 V.Loss: 0.0906, V.Acc: 85.84, V.AUC: 0.5112;\n",
      "Trigger times >= patience: 0\n",
      "[E 4/2000] T.Loss: 0.0874, T.Acc: 86.47, T.AUC: 0.5544 V.Loss: 0.0876, V.Acc: 85.84, V.AUC: 0.5205;\n",
      "Trigger times >= patience: 0\n",
      "[E 5/2000] T.Loss: 0.0837, T.Acc: 86.47, T.AUC: 0.4904 V.Loss: 0.0839, V.Acc: 85.84, V.AUC: 0.5309;\n",
      "Trigger times >= patience: 0\n",
      "[E 6/2000] T.Loss: 0.0793, T.Acc: 86.47, T.AUC: 0.5467 V.Loss: 0.0796, V.Acc: 85.84, V.AUC: 0.5397;\n",
      "Trigger times >= patience: 0\n",
      "[E 7/2000] T.Loss: 0.0742, T.Acc: 86.47, T.AUC: 0.5511 V.Loss: 0.0746, V.Acc: 85.84, V.AUC: 0.5533;\n",
      "Trigger times >= patience: 0\n",
      "[E 8/2000] T.Loss: 0.0685, T.Acc: 86.47, T.AUC: 0.5121 V.Loss: 0.0692, V.Acc: 85.84, V.AUC: 0.5616;\n",
      "Trigger times >= patience: 0\n",
      "[E 9/2000] T.Loss: 0.0628, T.Acc: 86.47, T.AUC: 0.5510 V.Loss: 0.0636, V.Acc: 85.84, V.AUC: 0.5633;\n",
      "Trigger times >= patience: 0\n",
      "[E 10/2000] T.Loss: 0.0575, T.Acc: 86.47, T.AUC: 0.5335 V.Loss: 0.0584, V.Acc: 85.84, V.AUC: 0.5669;\n",
      "Trigger times >= patience: 0\n",
      "[E 11/2000] T.Loss: 0.0531, T.Acc: 86.47, T.AUC: 0.5207 V.Loss: 0.0542, V.Acc: 85.84, V.AUC: 0.5717;\n",
      "Trigger times >= patience: 0\n",
      "[E 12/2000] T.Loss: 0.0498, T.Acc: 86.47, T.AUC: 0.5225 V.Loss: 0.0511, V.Acc: 85.84, V.AUC: 0.5741;\n",
      "Trigger times >= patience: 0\n",
      "[E 13/2000] T.Loss: 0.0478, T.Acc: 86.47, T.AUC: 0.5205 V.Loss: 0.0493, V.Acc: 85.84, V.AUC: 0.5808;\n",
      "Trigger times >= patience: 0\n",
      "[E 14/2000] T.Loss: 0.0468, T.Acc: 86.47, T.AUC: 0.5016 V.Loss: 0.0483, V.Acc: 85.84, V.AUC: 0.5858;\n",
      "Trigger times >= patience: 0\n",
      "[E 15/2000] T.Loss: 0.0464, T.Acc: 86.47, T.AUC: 0.4883 V.Loss: 0.0480, V.Acc: 85.84, V.AUC: 0.5921;\n",
      "Trigger times >= patience: 0\n",
      "[E 16/2000] T.Loss: 0.0462, T.Acc: 86.47, T.AUC: 0.5340 V.Loss: 0.0479, V.Acc: 85.84, V.AUC: 0.5945;\n",
      "Trigger times >= patience: 0\n",
      "[E 17/2000] T.Loss: 0.0461, T.Acc: 86.47, T.AUC: 0.4891 V.Loss: 0.0478, V.Acc: 85.84, V.AUC: 0.5963;\n",
      "Trigger times >= patience: 0\n",
      "[E 18/2000] T.Loss: 0.0460, T.Acc: 86.47, T.AUC: 0.4936 V.Loss: 0.0477, V.Acc: 85.84, V.AUC: 0.5978;\n",
      "Trigger times >= patience: 0\n",
      "[E 19/2000] T.Loss: 0.0458, T.Acc: 86.47, T.AUC: 0.4900 V.Loss: 0.0476, V.Acc: 85.84, V.AUC: 0.6036;\n",
      "Trigger times >= patience: 0\n",
      "[E 20/2000] T.Loss: 0.0457, T.Acc: 86.47, T.AUC: 0.5174 V.Loss: 0.0475, V.Acc: 85.84, V.AUC: 0.6095;\n",
      "Trigger times >= patience: 0\n",
      "[E 21/2000] T.Loss: 0.0457, T.Acc: 86.47, T.AUC: 0.5100 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.6142;\n",
      "Trigger times >= patience: 0\n",
      "[E 22/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.5578 V.Loss: 0.0474, V.Acc: 85.84, V.AUC: 0.6175;\n",
      "Trigger times >= patience: 0\n",
      "[E 23/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.4620 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6217;\n",
      "Trigger times >= patience: 0\n",
      "[E 24/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.5267 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6230;\n",
      "Trigger times >= patience: 0\n",
      "[E 25/2000] T.Loss: 0.0456, T.Acc: 86.47, T.AUC: 0.5021 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6235;\n",
      "Trigger times >= patience: 0\n",
      "[E 26/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5302 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6229;\n",
      "Trigger times >= patience: 0\n",
      "[E 27/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5267 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6248;\n",
      "Trigger times >= patience: 0\n",
      "[E 28/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5362 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6259;\n",
      "Trigger times >= patience: 0\n",
      "[E 29/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4925 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6289;\n",
      "Trigger times >= patience: 0\n",
      "[E 30/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4833 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6305;\n",
      "Trigger times >= patience: 0\n",
      "[E 31/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.4784 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6317;\n",
      "Trigger times >= patience: 0\n",
      "[E 32/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5359 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6330;\n",
      "Trigger times >= patience: 0\n",
      "[E 33/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5817 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6341;\n",
      "Trigger times >= patience: 0\n",
      "[E 34/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5236 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6358;\n",
      "Trigger times >= patience: 0\n",
      "[E 35/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5119 V.Loss: 0.0473, V.Acc: 85.84, V.AUC: 0.6405;\n",
      "Trigger times >= patience: 0\n",
      "[E 36/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5189 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6417;\n",
      "Trigger times >= patience: 0\n",
      "[E 37/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5759 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6431;\n",
      "Trigger times >= patience: 0\n",
      "[E 38/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5248 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6444;\n",
      "Trigger times >= patience: 0\n",
      "[E 39/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5223 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6462;\n",
      "Trigger times >= patience: 0\n",
      "[E 40/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5029 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6474;\n",
      "Trigger times >= patience: 0\n",
      "[E 41/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5153 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6490;\n",
      "Trigger times >= patience: 0\n",
      "[E 42/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5379 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6506;\n",
      "Trigger times >= patience: 0\n",
      "[E 43/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5106 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6543;\n",
      "Trigger times >= patience: 0\n",
      "[E 44/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5098 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6556;\n",
      "Trigger times >= patience: 0\n",
      "[E 45/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5327 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6558;\n",
      "Trigger times >= patience: 0\n",
      "[E 46/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5538 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6566;\n",
      "Trigger times >= patience: 0\n",
      "[E 47/2000] T.Loss: 0.0455, T.Acc: 86.47, T.AUC: 0.5363 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6571;\n",
      "Trigger times >= patience: 0\n",
      "[E 48/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.6017 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6580;\n",
      "Trigger times >= patience: 0\n",
      "[E 49/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5495 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6579;\n",
      "Trigger times >= patience: 0\n",
      "[E 50/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5133 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6581;\n",
      "Trigger times >= patience: 0\n",
      "[E 51/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4897 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6592;\n",
      "Trigger times >= patience: 0\n",
      "[E 52/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5483 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6607;\n",
      "Trigger times >= patience: 0\n",
      "[E 53/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5223 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6608;\n",
      "Trigger times >= patience: 0\n",
      "[E 54/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5254 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6610;\n",
      "Trigger times >= patience: 0\n",
      "[E 55/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4504 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6618;\n",
      "Trigger times >= patience: 0\n",
      "[E 56/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5404 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6620;\n",
      "Trigger times >= patience: 0\n",
      "[E 57/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5029 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6623;\n",
      "Trigger times >= patience: 0\n",
      "[E 58/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5348 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6629;\n",
      "Trigger times >= patience: 0\n",
      "[E 59/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4969 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6632;\n",
      "Trigger times >= patience: 0\n",
      "[E 60/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5342 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6637;\n",
      "Trigger times >= patience: 0\n",
      "[E 61/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4973 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6640;\n",
      "Trigger times >= patience: 0\n",
      "[E 62/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5138 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6645;\n",
      "Trigger times >= patience: 0\n",
      "[E 63/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5529 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6653;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 64/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5174 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6654;\n",
      "Trigger times >= patience: 0\n",
      "[E 65/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4799 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6655;\n",
      "Trigger times >= patience: 0\n",
      "[E 66/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5194 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6663;\n",
      "Trigger times >= patience: 0\n",
      "[E 67/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5369 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6673;\n",
      "Trigger times >= patience: 0\n",
      "[E 68/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4920 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6679;\n",
      "Trigger times >= patience: 0\n",
      "[E 69/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4594 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6683;\n",
      "Trigger times >= patience: 0\n",
      "[E 70/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5482 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6686;\n",
      "Trigger times >= patience: 0\n",
      "[E 71/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.4725 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6693;\n",
      "Trigger times >= patience: 0\n",
      "[E 72/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5669 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6690;\n",
      "Trigger times >= patience: 0\n",
      "[E 73/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5596 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6691;\n",
      "Trigger times >= patience: 0\n",
      "[E 74/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5450 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6696;\n",
      "Trigger times >= patience: 0\n",
      "[E 75/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5564 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6693;\n",
      "Trigger times >= patience: 0\n",
      "[E 76/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5325 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6697;\n",
      "Trigger times >= patience: 0\n",
      "[E 77/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5116 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6698;\n",
      "Trigger times >= patience: 0\n",
      "[E 78/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5407 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6701;\n",
      "Trigger times >= patience: 0\n",
      "[E 79/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5503 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6707;\n",
      "Trigger times >= patience: 0\n",
      "[E 80/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5217 V.Loss: 0.0472, V.Acc: 85.84, V.AUC: 0.6711;\n",
      "Trigger times >= patience: 0\n",
      "[E 81/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5823 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6719;\n",
      "Trigger times >= patience: 0\n",
      "[E 82/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5897 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6724;\n",
      "Trigger times >= patience: 0\n",
      "[E 83/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5603 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6726;\n",
      "Trigger times >= patience: 0\n",
      "[E 84/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5277 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6733;\n",
      "Trigger times >= patience: 0\n",
      "[E 85/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5644 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6740;\n",
      "Trigger times >= patience: 0\n",
      "[E 86/2000] T.Loss: 0.0454, T.Acc: 86.47, T.AUC: 0.5317 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6743;\n",
      "Trigger times >= patience: 0\n",
      "[E 87/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5076 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6744;\n",
      "Trigger times >= patience: 0\n",
      "[E 88/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5397 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6744;\n",
      "Trigger times >= patience: 0\n",
      "[E 89/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5179 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6745;\n",
      "Trigger times >= patience: 0\n",
      "[E 90/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5572 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6745;\n",
      "Trigger times >= patience: 0\n",
      "[E 91/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5299 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6748;\n",
      "Trigger times >= patience: 0\n",
      "[E 92/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.4924 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6748;\n",
      "Trigger times >= patience: 0\n",
      "[E 93/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.6291 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6745;\n",
      "Trigger times >= patience: 0\n",
      "[E 94/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.6042 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6749;\n",
      "Trigger times >= patience: 0\n",
      "[E 95/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.6165 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6750;\n",
      "Trigger times >= patience: 0\n",
      "[E 96/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5956 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6751;\n",
      "Trigger times >= patience: 0\n",
      "[E 97/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5680 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6755;\n",
      "Trigger times >= patience: 0\n",
      "[E 98/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5612 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6757;\n",
      "Trigger times >= patience: 0\n",
      "[E 99/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5875 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6754;\n",
      "Trigger times >= patience: 0\n",
      "[E 100/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5627 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6756;\n",
      "Trigger times >= patience: 0\n",
      "[E 101/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5459 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6764;\n",
      "Trigger times >= patience: 0\n",
      "[E 102/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5383 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6767;\n",
      "Loss Trigger Times: 1\n",
      "[E 103/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.4849 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6769;\n",
      "Trigger times >= patience: 0\n",
      "[E 104/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5141 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6774;\n",
      "Trigger times >= patience: 0\n",
      "[E 105/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5490 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 106/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5554 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6791;\n",
      "Trigger times >= patience: 0\n",
      "[E 107/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5625 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6799;\n",
      "Trigger times >= patience: 0\n",
      "[E 108/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5923 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6799;\n",
      "Trigger times >= patience: 0\n",
      "[E 109/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5245 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6801;\n",
      "Trigger times >= patience: 0\n",
      "[E 110/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5752 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6802;\n",
      "Trigger times >= patience: 0\n",
      "[E 111/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5921 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6801;\n",
      "Trigger times >= patience: 0\n",
      "[E 112/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5987 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6800;\n",
      "Trigger times >= patience: 0\n",
      "[E 113/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5503 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6799;\n",
      "Trigger times >= patience: 0\n",
      "[E 114/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5264 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6799;\n",
      "Trigger times >= patience: 0\n",
      "[E 115/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.6433 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6799;\n",
      "Trigger times >= patience: 0\n",
      "[E 116/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5947 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6797;\n",
      "Trigger times >= patience: 0\n",
      "[E 117/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5583 V.Loss: 0.0471, V.Acc: 85.84, V.AUC: 0.6796;\n",
      "Trigger times >= patience: 0\n",
      "[E 118/2000] T.Loss: 0.0453, T.Acc: 86.47, T.AUC: 0.5775 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6794;\n",
      "Trigger times >= patience: 0\n",
      "[E 119/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5667 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6793;\n",
      "Trigger times >= patience: 0\n",
      "[E 120/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5940 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6792;\n",
      "Trigger times >= patience: 0\n",
      "[E 121/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5614 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6792;\n",
      "Trigger times >= patience: 0\n",
      "[E 122/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.6159 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6790;\n",
      "Trigger times >= patience: 0\n",
      "[E 123/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5251 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6789;\n",
      "Trigger times >= patience: 0\n",
      "[E 124/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5745 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6789;\n",
      "Trigger times >= patience: 0\n",
      "[E 125/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5482 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6790;\n",
      "Trigger times >= patience: 0\n",
      "[E 126/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.6126 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6790;\n",
      "Trigger times >= patience: 0\n",
      "[E 127/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5411 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6790;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 128/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5640 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6791;\n",
      "Trigger times >= patience: 0\n",
      "[E 129/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5609 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6791;\n",
      "Trigger times >= patience: 0\n",
      "[E 130/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.6006 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6789;\n",
      "Trigger times >= patience: 0\n",
      "[E 131/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5705 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6789;\n",
      "Trigger times >= patience: 0\n",
      "[E 132/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5755 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6788;\n",
      "Trigger times >= patience: 0\n",
      "[E 133/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.6237 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 134/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5879 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 135/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5478 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 136/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5388 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6783;\n",
      "Trigger times >= patience: 0\n",
      "[E 137/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5815 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6784;\n",
      "Trigger times >= patience: 0\n",
      "[E 138/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5850 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6784;\n",
      "Trigger times >= patience: 0\n",
      "[E 139/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.5962 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 140/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.6061 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 141/2000] T.Loss: 0.0452, T.Acc: 86.47, T.AUC: 0.6110 V.Loss: 0.0470, V.Acc: 85.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 142/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5800 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 143/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5752 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 144/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5861 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 145/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.6437 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6781;\n",
      "Trigger times >= patience: 0\n",
      "[E 146/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5646 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 147/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.6177 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 148/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.6206 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6783;\n",
      "Trigger times >= patience: 0\n",
      "[E 149/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5990 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 150/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.6151 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6781;\n",
      "Trigger times >= patience: 0\n",
      "[E 151/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.6153 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 152/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.6191 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 153/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5715 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 154/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.5947 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6787;\n",
      "Trigger times >= patience: 0\n",
      "[E 155/2000] T.Loss: 0.0451, T.Acc: 86.47, T.AUC: 0.6060 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 156/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.5980 V.Loss: 0.0469, V.Acc: 85.84, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 157/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.6029 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6787;\n",
      "Trigger times >= patience: 0\n",
      "[E 158/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.5852 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6787;\n",
      "Trigger times >= patience: 0\n",
      "[E 159/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.6040 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 160/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.6423 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6785;\n",
      "Trigger times >= patience: 0\n",
      "[E 161/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.6380 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6781;\n",
      "Trigger times >= patience: 0\n",
      "[E 162/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.6059 V.Loss: 0.0468, V.Acc: 85.84, V.AUC: 0.6781;\n",
      "Trigger times >= patience: 0\n",
      "[E 163/2000] T.Loss: 0.0450, T.Acc: 86.47, T.AUC: 0.6309 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 164/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.6531 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6779;\n",
      "Trigger times >= patience: 0\n",
      "[E 165/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.6121 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 166/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.6092 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6781;\n",
      "Trigger times >= patience: 0\n",
      "[E 167/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.6083 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 168/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.6514 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 169/2000] T.Loss: 0.0449, T.Acc: 86.47, T.AUC: 0.6115 V.Loss: 0.0467, V.Acc: 85.84, V.AUC: 0.6778;\n",
      "Trigger times >= patience: 0\n",
      "[E 170/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.6521 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6777;\n",
      "Trigger times >= patience: 0\n",
      "[E 171/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.6291 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6779;\n",
      "Trigger times >= patience: 0\n",
      "[E 172/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.6324 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6781;\n",
      "Trigger times >= patience: 0\n",
      "[E 173/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.6220 V.Loss: 0.0466, V.Acc: 85.84, V.AUC: 0.6778;\n",
      "Trigger times >= patience: 0\n",
      "[E 174/2000] T.Loss: 0.0448, T.Acc: 86.47, T.AUC: 0.6515 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6778;\n",
      "Trigger times >= patience: 0\n",
      "[E 175/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.6565 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 176/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.6587 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6779;\n",
      "Trigger times >= patience: 0\n",
      "[E 177/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.6191 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6778;\n",
      "Trigger times >= patience: 0\n",
      "[E 178/2000] T.Loss: 0.0447, T.Acc: 86.47, T.AUC: 0.6266 V.Loss: 0.0465, V.Acc: 85.84, V.AUC: 0.6780;\n",
      "Trigger times >= patience: 0\n",
      "[E 179/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.6584 V.Loss: 0.0464, V.Acc: 85.84, V.AUC: 0.6778;\n",
      "Trigger times >= patience: 0\n",
      "[E 180/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.6370 V.Loss: 0.0464, V.Acc: 85.84, V.AUC: 0.6781;\n",
      "Trigger times >= patience: 0\n",
      "[E 181/2000] T.Loss: 0.0446, T.Acc: 86.47, T.AUC: 0.6889 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6781;\n",
      "Trigger times >= patience: 0\n",
      "[E 182/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.6551 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 183/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.6670 V.Loss: 0.0463, V.Acc: 85.84, V.AUC: 0.6782;\n",
      "Trigger times >= patience: 0\n",
      "[E 184/2000] T.Loss: 0.0445, T.Acc: 86.47, T.AUC: 0.6133 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 185/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.6138 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6788;\n",
      "Trigger times >= patience: 0\n",
      "[E 186/2000] T.Loss: 0.0444, T.Acc: 86.47, T.AUC: 0.6707 V.Loss: 0.0462, V.Acc: 85.84, V.AUC: 0.6787;\n",
      "Trigger times >= patience: 0\n",
      "[E 187/2000] T.Loss: 0.0443, T.Acc: 86.47, T.AUC: 0.7042 V.Loss: 0.0461, V.Acc: 85.84, V.AUC: 0.6790;\n",
      "Trigger times >= patience: 0\n",
      "[E 188/2000] T.Loss: 0.0443, T.Acc: 86.47, T.AUC: 0.6401 V.Loss: 0.0460, V.Acc: 85.84, V.AUC: 0.6792;\n",
      "Trigger times >= patience: 0\n",
      "[E 189/2000] T.Loss: 0.0442, T.Acc: 86.47, T.AUC: 0.6369 V.Loss: 0.0460, V.Acc: 85.84, V.AUC: 0.6794;\n",
      "Trigger times >= patience: 0\n",
      "[E 190/2000] T.Loss: 0.0441, T.Acc: 86.47, T.AUC: 0.6476 V.Loss: 0.0459, V.Acc: 85.84, V.AUC: 0.6792;\n",
      "Trigger times >= patience: 0\n",
      "[E 191/2000] T.Loss: 0.0441, T.Acc: 86.47, T.AUC: 0.6813 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6793;\n",
      "Trigger times >= patience: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 192/2000] T.Loss: 0.0440, T.Acc: 86.47, T.AUC: 0.6473 V.Loss: 0.0458, V.Acc: 85.84, V.AUC: 0.6793;\n",
      "Trigger times >= patience: 0\n",
      "[E 193/2000] T.Loss: 0.0439, T.Acc: 86.47, T.AUC: 0.6611 V.Loss: 0.0457, V.Acc: 85.84, V.AUC: 0.6793;\n",
      "Trigger times >= patience: 0\n",
      "[E 194/2000] T.Loss: 0.0438, T.Acc: 86.47, T.AUC: 0.6771 V.Loss: 0.0456, V.Acc: 85.84, V.AUC: 0.6794;\n",
      "Trigger times >= patience: 0\n",
      "[E 195/2000] T.Loss: 0.0437, T.Acc: 86.47, T.AUC: 0.6717 V.Loss: 0.0455, V.Acc: 85.84, V.AUC: 0.6795;\n",
      "Trigger times >= patience: 0\n",
      "[E 196/2000] T.Loss: 0.0436, T.Acc: 86.47, T.AUC: 0.6650 V.Loss: 0.0454, V.Acc: 85.84, V.AUC: 0.6797;\n",
      "Trigger times >= patience: 0\n",
      "[E 197/2000] T.Loss: 0.0435, T.Acc: 86.47, T.AUC: 0.6639 V.Loss: 0.0453, V.Acc: 85.84, V.AUC: 0.6793;\n",
      "Trigger times >= patience: 0\n",
      "[E 198/2000] T.Loss: 0.0434, T.Acc: 86.47, T.AUC: 0.6490 V.Loss: 0.0452, V.Acc: 85.84, V.AUC: 0.6793;\n",
      "Trigger times >= patience: 0\n",
      "[E 199/2000] T.Loss: 0.0432, T.Acc: 86.47, T.AUC: 0.6639 V.Loss: 0.0451, V.Acc: 85.84, V.AUC: 0.6794;\n",
      "Trigger times >= patience: 0\n",
      "[E 200/2000] T.Loss: 0.0431, T.Acc: 86.76, T.AUC: 0.6931 V.Loss: 0.0450, V.Acc: 85.55, V.AUC: 0.6796;\n",
      "Trigger times >= patience: 0\n",
      "[E 201/2000] T.Loss: 0.0429, T.Acc: 86.62, T.AUC: 0.6938 V.Loss: 0.0449, V.Acc: 85.84, V.AUC: 0.6797;\n",
      "Trigger times >= patience: 0\n",
      "[E 202/2000] T.Loss: 0.0428, T.Acc: 86.76, T.AUC: 0.6756 V.Loss: 0.0448, V.Acc: 85.84, V.AUC: 0.6797;\n",
      "Trigger times >= patience: 0\n",
      "[E 203/2000] T.Loss: 0.0426, T.Acc: 86.91, T.AUC: 0.6901 V.Loss: 0.0447, V.Acc: 85.84, V.AUC: 0.6802;\n",
      "Trigger times >= patience: 0\n",
      "[E 204/2000] T.Loss: 0.0425, T.Acc: 86.91, T.AUC: 0.6755 V.Loss: 0.0446, V.Acc: 86.13, V.AUC: 0.6809;\n",
      "Trigger times >= patience: 0\n",
      "[E 205/2000] T.Loss: 0.0423, T.Acc: 87.19, T.AUC: 0.6833 V.Loss: 0.0445, V.Acc: 86.42, V.AUC: 0.6810;\n",
      "Trigger times >= patience: 0\n",
      "[E 206/2000] T.Loss: 0.0422, T.Acc: 87.05, T.AUC: 0.6845 V.Loss: 0.0444, V.Acc: 86.71, V.AUC: 0.6812;\n",
      "Trigger times >= patience: 0\n",
      "[E 207/2000] T.Loss: 0.0420, T.Acc: 87.05, T.AUC: 0.6896 V.Loss: 0.0444, V.Acc: 86.42, V.AUC: 0.6814;\n",
      "Trigger times >= patience: 0\n",
      "[E 208/2000] T.Loss: 0.0419, T.Acc: 87.05, T.AUC: 0.6913 V.Loss: 0.0443, V.Acc: 86.42, V.AUC: 0.6817;\n",
      "Trigger times >= patience: 0\n",
      "[E 209/2000] T.Loss: 0.0418, T.Acc: 87.05, T.AUC: 0.6724 V.Loss: 0.0443, V.Acc: 86.71, V.AUC: 0.6819;\n",
      "Trigger times >= patience: 0\n",
      "[E 210/2000] T.Loss: 0.0417, T.Acc: 87.05, T.AUC: 0.7021 V.Loss: 0.0442, V.Acc: 86.71, V.AUC: 0.6821;\n",
      "Trigger times >= patience: 0\n",
      "[E 211/2000] T.Loss: 0.0416, T.Acc: 87.05, T.AUC: 0.6949 V.Loss: 0.0442, V.Acc: 86.71, V.AUC: 0.6823;\n",
      "Trigger times >= patience: 0\n",
      "[E 212/2000] T.Loss: 0.0416, T.Acc: 87.63, T.AUC: 0.6839 V.Loss: 0.0443, V.Acc: 86.42, V.AUC: 0.6823;\n",
      "Trigger times >= patience: 0\n",
      "[E 213/2000] T.Loss: 0.0415, T.Acc: 87.34, T.AUC: 0.6893 V.Loss: 0.0442, V.Acc: 86.42, V.AUC: 0.6825;\n",
      "Trigger times >= patience: 0\n",
      "[E 214/2000] T.Loss: 0.0414, T.Acc: 87.63, T.AUC: 0.6840 V.Loss: 0.0441, V.Acc: 86.71, V.AUC: 0.6830;\n",
      "Trigger times >= patience: 0\n",
      "[E 215/2000] T.Loss: 0.0413, T.Acc: 87.77, T.AUC: 0.6941 V.Loss: 0.0442, V.Acc: 86.42, V.AUC: 0.6829;\n",
      "Trigger times >= patience: 0\n",
      "[E 216/2000] T.Loss: 0.0413, T.Acc: 87.48, T.AUC: 0.6986 V.Loss: 0.0441, V.Acc: 86.42, V.AUC: 0.6830;\n",
      "Trigger times >= patience: 0\n",
      "[E 217/2000] T.Loss: 0.0412, T.Acc: 87.63, T.AUC: 0.6917 V.Loss: 0.0440, V.Acc: 86.42, V.AUC: 0.6831;\n",
      "Trigger times >= patience: 0\n",
      "[E 218/2000] T.Loss: 0.0411, T.Acc: 87.91, T.AUC: 0.6868 V.Loss: 0.0440, V.Acc: 86.71, V.AUC: 0.6834;\n",
      "Trigger times >= patience: 0\n",
      "[E 219/2000] T.Loss: 0.0411, T.Acc: 87.48, T.AUC: 0.7284 V.Loss: 0.0439, V.Acc: 86.71, V.AUC: 0.6832;\n",
      "Trigger times >= patience: 0\n",
      "[E 220/2000] T.Loss: 0.0410, T.Acc: 87.63, T.AUC: 0.7140 V.Loss: 0.0439, V.Acc: 86.71, V.AUC: 0.6832;\n",
      "Trigger times >= patience: 0\n",
      "[E 221/2000] T.Loss: 0.0410, T.Acc: 87.63, T.AUC: 0.6824 V.Loss: 0.0440, V.Acc: 86.42, V.AUC: 0.6831;\n",
      "Trigger times >= patience: 0\n",
      "[E 222/2000] T.Loss: 0.0409, T.Acc: 88.06, T.AUC: 0.7167 V.Loss: 0.0438, V.Acc: 86.71, V.AUC: 0.6827;\n",
      "Trigger times >= patience: 0\n",
      "[E 223/2000] T.Loss: 0.0409, T.Acc: 87.91, T.AUC: 0.7045 V.Loss: 0.0438, V.Acc: 86.71, V.AUC: 0.6827;\n",
      "Trigger times >= patience: 0\n",
      "[E 224/2000] T.Loss: 0.0408, T.Acc: 87.91, T.AUC: 0.6976 V.Loss: 0.0438, V.Acc: 86.71, V.AUC: 0.6833;\n",
      "Trigger times >= patience: 0\n",
      "[E 225/2000] T.Loss: 0.0408, T.Acc: 88.06, T.AUC: 0.7030 V.Loss: 0.0438, V.Acc: 86.42, V.AUC: 0.6836;\n",
      "Trigger times >= patience: 0\n",
      "[E 226/2000] T.Loss: 0.0407, T.Acc: 88.06, T.AUC: 0.7113 V.Loss: 0.0438, V.Acc: 86.42, V.AUC: 0.6839;\n",
      "Trigger times >= patience: 0\n",
      "[E 227/2000] T.Loss: 0.0407, T.Acc: 87.63, T.AUC: 0.7114 V.Loss: 0.0437, V.Acc: 86.71, V.AUC: 0.6829;\n",
      "Trigger times >= patience: 0\n",
      "[E 228/2000] T.Loss: 0.0406, T.Acc: 87.91, T.AUC: 0.7160 V.Loss: 0.0437, V.Acc: 86.71, V.AUC: 0.6823;\n",
      "Trigger times >= patience: 0\n",
      "[E 229/2000] T.Loss: 0.0406, T.Acc: 87.91, T.AUC: 0.7124 V.Loss: 0.0437, V.Acc: 86.42, V.AUC: 0.6816;\n",
      "Trigger times >= patience: 0\n",
      "[E 230/2000] T.Loss: 0.0406, T.Acc: 88.06, T.AUC: 0.7340 V.Loss: 0.0436, V.Acc: 86.71, V.AUC: 0.6804;\n",
      "Trigger times >= patience: 0\n",
      "[E 231/2000] T.Loss: 0.0405, T.Acc: 88.06, T.AUC: 0.7175 V.Loss: 0.0436, V.Acc: 86.99, V.AUC: 0.6794;\n",
      "Trigger times >= patience: 0\n",
      "[E 232/2000] T.Loss: 0.0405, T.Acc: 87.91, T.AUC: 0.6921 V.Loss: 0.0436, V.Acc: 86.71, V.AUC: 0.6794;\n",
      "Trigger times >= patience: 0\n",
      "[E 233/2000] T.Loss: 0.0404, T.Acc: 88.06, T.AUC: 0.7090 V.Loss: 0.0436, V.Acc: 86.99, V.AUC: 0.6786;\n",
      "Trigger times >= patience: 0\n",
      "[E 234/2000] T.Loss: 0.0404, T.Acc: 87.77, T.AUC: 0.7049 V.Loss: 0.0436, V.Acc: 86.71, V.AUC: 0.6788;\n",
      "Trigger times >= patience: 0\n",
      "[E 235/2000] T.Loss: 0.0403, T.Acc: 87.91, T.AUC: 0.7322 V.Loss: 0.0436, V.Acc: 86.71, V.AUC: 0.6790;\n",
      "Trigger times >= patience: 0\n",
      "[E 236/2000] T.Loss: 0.0403, T.Acc: 87.91, T.AUC: 0.7252 V.Loss: 0.0435, V.Acc: 86.99, V.AUC: 0.6789;\n",
      "Trigger times >= patience: 0\n",
      "[E 237/2000] T.Loss: 0.0403, T.Acc: 87.77, T.AUC: 0.7192 V.Loss: 0.0435, V.Acc: 86.99, V.AUC: 0.6779;\n",
      "Trigger times >= patience: 0\n",
      "[E 238/2000] T.Loss: 0.0403, T.Acc: 87.63, T.AUC: 0.7309 V.Loss: 0.0435, V.Acc: 86.99, V.AUC: 0.6757;\n",
      "Trigger times >= patience: 0\n",
      "[E 239/2000] T.Loss: 0.0402, T.Acc: 87.77, T.AUC: 0.7269 V.Loss: 0.0435, V.Acc: 86.71, V.AUC: 0.6726;\n",
      "Trigger times >= patience: 0\n",
      "[E 240/2000] T.Loss: 0.0402, T.Acc: 87.63, T.AUC: 0.7247 V.Loss: 0.0435, V.Acc: 87.28, V.AUC: 0.6722;\n",
      "Trigger times >= patience: 0\n",
      "[E 241/2000] T.Loss: 0.0401, T.Acc: 88.20, T.AUC: 0.7285 V.Loss: 0.0435, V.Acc: 86.71, V.AUC: 0.6715;\n",
      "Trigger times >= patience: 0\n",
      "[E 242/2000] T.Loss: 0.0401, T.Acc: 87.63, T.AUC: 0.7305 V.Loss: 0.0435, V.Acc: 86.71, V.AUC: 0.6710;\n",
      "Trigger times >= patience: 0\n",
      "[E 243/2000] T.Loss: 0.0401, T.Acc: 87.91, T.AUC: 0.7208 V.Loss: 0.0435, V.Acc: 86.71, V.AUC: 0.6707;\n",
      "Trigger times >= patience: 0\n",
      "[E 244/2000] T.Loss: 0.0401, T.Acc: 88.06, T.AUC: 0.7215 V.Loss: 0.0435, V.Acc: 86.71, V.AUC: 0.6709;\n",
      "Trigger times >= patience: 0\n",
      "[E 245/2000] T.Loss: 0.0400, T.Acc: 88.20, T.AUC: 0.7122 V.Loss: 0.0434, V.Acc: 87.28, V.AUC: 0.6711;\n",
      "Trigger times >= patience: 0\n",
      "[E 246/2000] T.Loss: 0.0400, T.Acc: 88.06, T.AUC: 0.7162 V.Loss: 0.0434, V.Acc: 87.28, V.AUC: 0.6708;\n",
      "Trigger times >= patience: 0\n",
      "[E 247/2000] T.Loss: 0.0400, T.Acc: 87.77, T.AUC: 0.7358 V.Loss: 0.0435, V.Acc: 86.71, V.AUC: 0.6714;\n",
      "Trigger times >= patience: 0\n",
      "[E 248/2000] T.Loss: 0.0399, T.Acc: 88.06, T.AUC: 0.7376 V.Loss: 0.0435, V.Acc: 86.71, V.AUC: 0.6720;\n",
      "Loss Trigger Times: 1\n",
      "[E 249/2000] T.Loss: 0.0399, T.Acc: 88.06, T.AUC: 0.7261 V.Loss: 0.0435, V.Acc: 86.71, V.AUC: 0.6707;\n",
      "Loss Trigger Times: 2\n",
      "[E 250/2000] T.Loss: 0.0399, T.Acc: 88.06, T.AUC: 0.7354 V.Loss: 0.0435, V.Acc: 86.71, V.AUC: 0.6696;\n",
      "Loss Trigger Times: 3\n",
      "[E 251/2000] T.Loss: 0.0399, T.Acc: 87.91, T.AUC: 0.7253 V.Loss: 0.0436, V.Acc: 86.71, V.AUC: 0.6687;\n",
      "Loss Trigger Times: 4\n",
      "Early stopping by LOSS!.\n",
      "0.8245614035087719\n",
      "0.6718085106382979\n",
      "[[93  1]\n",
      " [19  1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/txy_wasearlypreterm.save']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- First stage: taxonomy data        ----#\n",
    "#-------------------------------------------#\n",
    "\n",
    "model_txy, txy_hist, txytest_obs, txytest_pred, txytest_prob, txytest_auc, txytest_acc, txytest_conf = FirstStage_txy(mytrain_input_txy, mytrain_output, myvalid_input_txy, myvalid_output, mytest_input_txy, mytest_output, finalperiod)\n",
    "print(txytest_acc)\n",
    "print(txytest_auc)\n",
    "print(txytest_conf)\n",
    "dump(model_txy, '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/txy_wasearlypreterm.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b6192b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17543859649122806"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mytest_output[:,finalperiod-1,0])/mytest_output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d415957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1416184971098266\n",
      "0.8421052631578947\n",
      "0.7968085106382978\n",
      "[[94  0]\n",
      " [18  2]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use validation set only without class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "Mtdvalid_obs, Mtdvalid_pred, Mtdvalid_prob = evaluate(model_Mtd, device, myvalid_input_mtd, myvalid_output, finalperiod, cutoff=0.5)\n",
    "ptyvalid_obs, ptyvalid_pred, ptyvalid_prob = evaluate(model_pty, device, myvalid_input_pty, myvalid_output, finalperiod, cutoff=0.5)\n",
    "txyvalid_obs, txyvalid_pred, txyvalid_prob = evaluate(model_txy, device, myvalid_input_txy, myvalid_output, finalperiod, cutoff=0.5)\n",
    "\n",
    "x_valid = np.array(np.column_stack([Mtdvalid_prob, ptyvalid_prob, txyvalid_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(Mtdvalid_obs)/len(Mtdvalid_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag')\n",
    "L2Logistic_model.fit(x_valid, Mtdvalid_obs)\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ebac487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1416184971098266\n",
      "0.8508771929824561\n",
      "0.7840425531914893\n",
      "[[88  6]\n",
      " [11  9]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use validation set only with class weights (Best)\n",
    "#-------------------------------------------#\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "Mtdvalid_obs, Mtdvalid_pred, Mtdvalid_prob = evaluate(model_Mtd, device, myvalid_input_mtd, myvalid_output, finalperiod, cutoff=0.5)\n",
    "ptyvalid_obs, ptyvalid_pred, ptyvalid_prob = evaluate(model_pty, device, myvalid_input_pty, myvalid_output, finalperiod, cutoff=0.5)\n",
    "txyvalid_obs, txyvalid_pred, txyvalid_prob = evaluate(model_txy, device, myvalid_input_txy, myvalid_output, finalperiod, cutoff=0.5)\n",
    "\n",
    "x_valid = np.array(np.column_stack([Mtdvalid_prob, ptyvalid_prob, txyvalid_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(Mtdvalid_obs)/len(Mtdvalid_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag', class_weight=wt)\n",
    "L2Logistic_model.fit(x_valid, Mtdvalid_obs)\n",
    "dump(L2Logistic_model, '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/RNN_py/submission/trainedmodels/L2logistic_wasearlypreterm.save')\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd20ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1373679154658982\n",
      "0.8421052631578947\n",
      "0.7606382978723404\n",
      "[[94  0]\n",
      " [18  2]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use trianing+validation set without class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "MtdS2train_input = np.concatenate((mytrain_input_mtd, myvalid_input_mtd), axis=0)\n",
    "ptyS2train_input = np.concatenate((mytrain_input_pty, myvalid_input_pty), axis=0)\n",
    "txyS2train_input = np.concatenate((mytrain_input_txy, myvalid_input_txy), axis=0)\n",
    "\n",
    "S2train_output = np.concatenate((mytrain_output, myvalid_output), axis=0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "\n",
    "MtdS2_obs, MtdS2_pred, MtdS2_prob = evaluate(model_Mtd, device, MtdS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "ptyS2_obs, ptyS2_pred, ptyS2_prob = evaluate(model_pty, device, ptyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "txyS2_obs, txyS2_pred, txyS2_prob = evaluate(model_txy, device, txyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "x_valid = np.array(np.column_stack([MtdS2_prob, ptyS2_prob, txyS2_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(MtdS2_obs)/len(MtdS2_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag')\n",
    "L2Logistic_model.fit(x_valid, MtdS2_obs)\n",
    "\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "# x_test = np.array(np.transpose([Mtdtest_prob, ptytest_prob, txytest_prob, krdtest_prob])).reshape(-1, 3*2)\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55178019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1373679154658982\n",
      "0.8421052631578947\n",
      "0.7537234042553193\n",
      "[[87  7]\n",
      " [11  9]]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------#\n",
    "#---- Second stage: Logistic Regression ----# # use trianing + validation set with class weights\n",
    "#-------------------------------------------#\n",
    "\n",
    "MtdS2train_input = np.concatenate((mytrain_input_mtd, myvalid_input_mtd), axis=0)\n",
    "ptyS2train_input = np.concatenate((mytrain_input_pty, myvalid_input_pty), axis=0)\n",
    "txyS2train_input = np.concatenate((mytrain_input_txy, myvalid_input_txy), axis=0)\n",
    "\n",
    "S2train_output = np.concatenate((mytrain_output, myvalid_output), axis=0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#---- validation set training ----#\n",
    "\n",
    "MtdS2_obs, MtdS2_pred, MtdS2_prob = evaluate(model_Mtd, device, MtdS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "ptyS2_obs, ptyS2_pred, ptyS2_prob = evaluate(model_pty, device, ptyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "txyS2_obs, txyS2_pred, txyS2_prob = evaluate(model_txy, device, txyS2train_input, S2train_output, finalperiod, cutoff=0.5)\n",
    "x_valid = np.array(np.column_stack([MtdS2_prob, ptyS2_prob, txyS2_prob])).reshape(-1, 3*2)\n",
    "\n",
    "S2prior = sum(MtdS2_obs)/len(MtdS2_obs)\n",
    "print(S2prior)\n",
    "wt = {0:S2prior, 1:1-S2prior} # use the prior class prob as class weights\n",
    "\n",
    "L2Logistic_model = LogisticRegression(penalty='l2', solver='sag', class_weight=wt)\n",
    "L2Logistic_model.fit(x_valid, MtdS2_obs)\n",
    "\n",
    "\n",
    "#---- testing set evaluation ----#\n",
    "# x_test = np.array(np.transpose([Mtdtest_prob, ptytest_prob, txytest_prob, krdtest_prob])).reshape(-1, 3*2)\n",
    "x_test = np.array(np.column_stack([Mtdtest_prob, ptytest_prob, txytest_prob])).reshape(-1, 3*2)\n",
    "final_obs  = Mtdtest_obs\n",
    "final_prob = L2Logistic_model.predict_proba(x_test)[:,1]\n",
    "final_pred = L2Logistic_model.predict(x_test)\n",
    "\n",
    "final_acc  = metrics.accuracy_score(final_obs, final_pred)\n",
    "final_auc  = metrics.roc_auc_score(final_obs, final_prob)\n",
    "final_conf = metrics.confusion_matrix(final_obs, final_pred)\n",
    "\n",
    "print(final_acc)\n",
    "print(final_auc)\n",
    "print(final_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300086cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3fc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
