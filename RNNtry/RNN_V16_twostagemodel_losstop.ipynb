{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df84dbc",
   "metadata": {},
   "source": [
    "# Preterm Birth Prediction Microbiome Model Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e813d",
   "metadata": {},
   "source": [
    "Challenge website:\n",
    "https://www.synapse.org/#!Synapse:syn26133770/wiki/618018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27987cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from collections import Counter,defaultdict, OrderedDict\n",
    "from itertools import islice\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e0581",
   "metadata": {},
   "source": [
    "# 1. Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf15a13",
   "metadata": {},
   "source": [
    "The model framework is based on several different datasets and can be separated into two different stages.\n",
    "\n",
    "## First stage model:\n",
    "\n",
    "The **first stage model** contain four different models:\n",
    "- LSTM: alpha diversity + CST categories + meta data;\n",
    "- LSTM: phylotype data;\n",
    "- LSTM: taxonomy data;\n",
    "- KNN:  krdwide data.\n",
    "\n",
    "The outputs of each above sub-model are two columns of class probabilities of \"*preterm*\" or \"*not preterm*\".\n",
    "\n",
    "## Second stage model:\n",
    "\n",
    "The **second stage model** simply uses a logistic regression model to summarize the prediction probabilities from first stage model. Since the prediction probabilities of binary classes for each observation are sum to one and hence coupled, we only use the prediction probability of class \"*preterm*\" in the second stage model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c369b4",
   "metadata": {},
   "source": [
    "# 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e72258",
   "metadata": {},
   "source": [
    "Each data set has been divided into the training, validation and testing set with the ratio of 0.6, 0.3 and 0.1. The training set is used to train model, the validation set is used to select the best model from the trained models and the testing set is used for checking the constructed model performance.\n",
    "\n",
    "Since we have a two-stages model framework, the training set will be used twice in both building the first stage models and the second stage logistic regression model. \n",
    "\n",
    "(**Questions to check**: Maybe we can construct the logistic regression model with both the training and validation sets?*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ce470",
   "metadata": {},
   "source": [
    "## 2.1. Read-in & clean data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd386f87",
   "metadata": {},
   "source": [
    "Here, we mainly have two criterion for data cleaning and filtering.\n",
    "- 1). Average within each collection period;\n",
    "\n",
    "    There are many cases of having multiple specimens collected from the same week and simply deleting them will be a huge sample lost. Average within each collection period can not only help save these specimens' information, but also it could help reduce the numer of zeros when we perform zero padding to construct the input of LSTM models. (Generally, the none zero values (observed) usually have at least 10% out of all observations to have a better performance.) The new variable, **collect_period**, is constructed based on the collected week of each specimen and both the time varying and not varying are averaged within each collection period. Rules of **collect_period** construction are:\n",
    "    \n",
    "    - **collect_period 1** is labeled if specimen collected before week 13;\n",
    "    - **collect_period 2** is labeled if specimen collected between week 13 and week 24;\n",
    "    - **collect_period 3** is labeled if specimen collected between week 25 and week 28;\n",
    "    - **collect_period 4** is labeled if specimen collected between week 29 and week 32;\n",
    "    - **collect_period 5** is labeled if specimen collected between after week 32;\n",
    "    \n",
    "    \n",
    "    \n",
    "- 2). Filtered out observations with collect_wk<=32;\n",
    "\n",
    "    The training and Validation data sets will be limited to through 32 weeks for the preterm prediction and through 28 weeks for the early-preterm prediction.\n",
    "    \n",
    "**Questions**: Need to involve collect_wk<=28 as one end of collection period !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73af12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "meta_dir      = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/metadata_imputed1.csv'\n",
    "alpha_dir     = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/alpha_diversity/alpha_diversity.csv'\n",
    "cst_dir       = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/community_state_types/cst_valencia.csv'\n",
    "krdwide_dir   = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/pairwise_distance/krd_distance_wide.csv'\n",
    "phylotype_dir = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/phylotypes/phylotype_relabd.1e0.csv'\n",
    "taxonomy_dir  = '/Users/mli171/Desktop/JHU/3Summer2022_JHU/DREAM/training_data_2022-05-27/taxonomy/taxonomy_relabd.family.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ddaa2",
   "metadata": {},
   "source": [
    "**Questions**: The meta data imputation need to be involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28906fe5",
   "metadata": {},
   "source": [
    "### 2.1.1 Meta + alpha + cst datadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66213a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3578, 8)\n",
      "(1661, 11)\n",
      "2    769\n",
      "3    370\n",
      "1    299\n",
      "4    223\n",
      "Name: collect_period, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/ccg9zpjj4sq_l6d6fypc_5740000gn/T/ipykernel_52612/202948611.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meta_data['collect_period'][(meta_data['collect_wk']>=17)  & (meta_data['collect_wk']<=20)] = 2\n",
      "/var/folders/xy/ccg9zpjj4sq_l6d6fypc_5740000gn/T/ipykernel_52612/202948611.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meta_data['collect_period'][(meta_data['collect_wk']>=21)  & (meta_data['collect_wk']<=28)] = 3\n",
      "/var/folders/xy/ccg9zpjj4sq_l6d6fypc_5740000gn/T/ipykernel_52612/202948611.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meta_data['collect_period'][(meta_data['collect_wk']>=29)  & (meta_data['collect_wk']<=32)] = 4\n",
      "/var/folders/xy/ccg9zpjj4sq_l6d6fypc_5740000gn/T/ipykernel_52612/202948611.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  meta_data['collect_period'][(meta_data['collect_wk']>32)]                                   = 5\n"
     ]
    }
   ],
   "source": [
    "meta_data = pd.DataFrame(pd.read_csv(meta_dir, delimiter=','))\n",
    "meta_data = meta_data[['participant_id', 'project', 'delivery_wk', 'collect_wk', 'age_imp', 'race_imp']]\n",
    "alpha_data = pd.DataFrame(pd.read_csv(alpha_dir, delimiter=','))\n",
    "cst_data = pd.DataFrame(pd.read_csv(cst_dir, delimiter=','))\n",
    "\n",
    "meta_data = pd.concat([meta_data, alpha_data['shannon'], cst_data['CST']], axis=1)\n",
    "print(meta_data.shape)\n",
    "\n",
    "for i in range(1,meta_data.shape[1]):\n",
    "    if meta_data.iloc[:,i].dtypes == object:\n",
    "        meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('category').cat.codes + 1\n",
    "        meta_data.iloc[:,i] = meta_data.iloc[:,i].astype('float64')\n",
    "\n",
    "# create new variable 'collect_period'\n",
    "meta_data['collect_period'] = 1\n",
    "meta_data['collect_period'][(meta_data['collect_wk']>=17)  & (meta_data['collect_wk']<=20)] = 2\n",
    "meta_data['collect_period'][(meta_data['collect_wk']>=21)  & (meta_data['collect_wk']<=28)] = 3\n",
    "meta_data['collect_period'][(meta_data['collect_wk']>=29)  & (meta_data['collect_wk']<=32)] = 4\n",
    "meta_data['collect_period'][(meta_data['collect_wk']>32)]                                   = 5\n",
    "\n",
    "collect_period = meta_data['collect_period']\n",
    "participant_id = meta_data['participant_id']\n",
    "\n",
    "# create class label\n",
    "meta_data['was_preterm'] = 1*(meta_data['delivery_wk'] < 37)\n",
    "meta_data['was_early_preterm'] = 1*(meta_data['delivery_wk'] < 32)\n",
    "\n",
    "# Filtered out observations with \"collect_wk<=32\" == \"collect_period<=4\" \n",
    "meta_data = meta_data[meta_data['collect_period']<=4]\n",
    "# Average within each collection period\n",
    "meta_data = meta_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "print(meta_data.shape)\n",
    "print(meta_data['collect_period'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb689c3f",
   "metadata": {},
   "source": [
    "### 2.1.2 Taxonomy OTU (RA) data (Family level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad50ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1661, 527)\n"
     ]
    }
   ],
   "source": [
    "taxonomy_data = pd.DataFrame(pd.read_csv(taxonomy_dir, delimiter=','))\n",
    "taxonomy_data = pd.concat([participant_id, collect_period, taxonomy_data], axis=1)\n",
    "\n",
    "# Filtered out observations with \"collect_wk<=32\" == \"collect_period<=4\" \n",
    "taxonomy_data = taxonomy_data[taxonomy_data['collect_period']<=4]\n",
    "# Average within each collection period\n",
    "taxonomy_data = taxonomy_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "print(taxonomy_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac94ab",
   "metadata": {},
   "source": [
    "### 2.1.3 Phylotype data  (.1e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11988e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1661, 1846)\n"
     ]
    }
   ],
   "source": [
    "phylotype_data = pd.DataFrame(pd.read_csv(phylotype_dir, delimiter=','))\n",
    "phylotype_data = pd.concat([participant_id, collect_period, phylotype_data], axis=1)\n",
    "\n",
    "# Filtered out observations with \"collect_wk<=32\" == \"collect_period<=4\" \n",
    "phylotype_data = phylotype_data[phylotype_data['collect_period']<=4]\n",
    "# Average within each collection period\n",
    "phylotype_data = phylotype_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "print(phylotype_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ce3ed",
   "metadata": {},
   "source": [
    "### 2.1.4 KRD pairwise distance data (wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50c22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1661, 1663)\n"
     ]
    }
   ],
   "source": [
    "krd_data = pd.DataFrame(pd.read_csv(krdwide_dir, delimiter=','))\n",
    "\n",
    "krd_data = pd.concat([participant_id, collect_period, krd_data], axis=1)\n",
    "# Filtered out observations with \"collect_wk<=32\" == \"collect_period<=4\" \n",
    "krd_data = krd_data[krd_data['collect_period']<=4]\n",
    "# Average within each collection period\n",
    "krd_data = krd_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "krd_data = krd_data.drop(['participant_id', 'collect_period'], axis=1).T\n",
    "# set the index same in order to concate columns\n",
    "participant_id.index = krd_data.index\n",
    "collect_period.index = krd_data.index\n",
    "krd_data = pd.concat([participant_id, collect_period, krd_data], axis=1)\n",
    "# Filtered out observations with \"collect_wk<=32\" == \"collect_period<=4\" \n",
    "krd_data = krd_data[krd_data['collect_period']<=4]\n",
    "krd_data = krd_data.groupby(['participant_id', 'collect_period'], as_index = False).mean()\n",
    "print(krd_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6688e",
   "metadata": {},
   "source": [
    "### 2.1.5 Dimension summary of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9895e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "# of subjects =  1214\n",
      "# of samples  =  1661\n",
      "##################################\n",
      "# of meta data features    =  11\n",
      "# of taxonnomy features    =  527\n",
      "# of phylotype features    =  1846\n",
      "# of modified krd features =  1663\n"
     ]
    }
   ],
   "source": [
    "uniquenames, counts = np.unique(meta_data[\"participant_id\"], return_counts=True)\n",
    "subjects = list(uniquenames)\n",
    "print(\"##################################\")\n",
    "print(\"# of subjects = \", len(subjects))\n",
    "print(\"# of samples  = \", meta_data.shape[0])\n",
    "print(\"##################################\")\n",
    "print(\"# of meta data features    = \", meta_data.shape[1])\n",
    "print(\"# of taxonnomy features    = \", len(list(taxonomy_data)))\n",
    "print(\"# of phylotype features    = \", len(list(phylotype_data)))\n",
    "print(\"# of modified krd features = \", len(list(krd_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb9950",
   "metadata": {},
   "source": [
    "## 2.2. Data sets splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ab12f",
   "metadata": {},
   "source": [
    "Since we have multiple datasets, we will use the index of subjects to guide training, validation and testing set spliter. Also noted, since there are different number of records for each patient, the dimension of train and testing data sets are not follow the proportion 0.6, but the patients will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092f5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_splitID(meta_data, subjects, prop, myseed):\n",
    "    \n",
    "    if myseed != None:\n",
    "        random.seed(myseed)\n",
    "        \n",
    "    numsubjects = len(subjects)\n",
    "\n",
    "    subjects_shuffle = random.sample(subjects, numsubjects)\n",
    "    \n",
    "    train_subjects = subjects_shuffle[0:(int(len(subjects)*prop[0])+1)] \n",
    "    valid_subjects = subjects_shuffle[(int(len(subjects)*prop[0])+2):(int(len(subjects)*(prop[0]+prop[1]))+1)]\n",
    "    test_subjects = subjects_shuffle[(int(len(subjects)*(prop[0]+prop[1]))+2):numsubjects]\n",
    "    \n",
    "    splitID_train = meta_data['participant_id'].isin(train_subjects)\n",
    "    splitID_valid = meta_data['participant_id'].isin(valid_subjects)\n",
    "    splitID_test = meta_data['participant_id'].isin(test_subjects)\n",
    "    \n",
    "    return splitID_train, splitID_valid, splitID_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e30b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ meta_data\n",
      "(1006, 11)\n",
      "(495, 11)\n",
      "(158, 11)\n",
      "################ taxonomy_data\n",
      "(1006, 527)\n",
      "(495, 527)\n",
      "(158, 527)\n",
      "################ phylotype_data\n",
      "(1006, 1846)\n",
      "(495, 1846)\n",
      "(158, 1846)\n",
      "################ krd_data\n",
      "(1006, 1663)\n",
      "(495, 1663)\n",
      "(158, 1663)\n"
     ]
    }
   ],
   "source": [
    "# set myseed=None to have complete random state\n",
    "splitID_train, splitID_valid, splitID_test = dataset_splitID(meta_data, subjects, prop = [0.6, 0.3, 0.1], myseed=0)\n",
    "\n",
    "# apply to each data sets\n",
    "meta_data_train = meta_data[splitID_train]\n",
    "meta_data_valid = meta_data[splitID_valid]\n",
    "meta_data_test  = meta_data[splitID_test]\n",
    "print(\"################ meta_data\")\n",
    "print(meta_data_train.shape)\n",
    "print(meta_data_valid.shape)\n",
    "print(meta_data_test.shape)\n",
    "\n",
    "taxonomy_data_train = taxonomy_data[splitID_train]\n",
    "taxonomy_data_valid = taxonomy_data[splitID_valid]\n",
    "taxonomy_data_test  = taxonomy_data[splitID_test]\n",
    "print(\"################ taxonomy_data\")\n",
    "print(taxonomy_data_train.shape)\n",
    "print(taxonomy_data_valid.shape)\n",
    "print(taxonomy_data_test.shape)\n",
    "\n",
    "phylotype_data_train = phylotype_data[splitID_train]\n",
    "phylotype_data_valid = phylotype_data[splitID_valid]\n",
    "phylotype_data_test  = phylotype_data[splitID_test]\n",
    "print(\"################ phylotype_data\")\n",
    "print(phylotype_data_train.shape)\n",
    "print(phylotype_data_valid.shape)\n",
    "print(phylotype_data_test.shape)\n",
    "\n",
    "krd_data_train = krd_data[splitID_train]\n",
    "krd_data_valid = krd_data[splitID_valid]\n",
    "krd_data_test  = krd_data[splitID_test]\n",
    "print(\"################ krd_data\")\n",
    "print(krd_data_train.shape)\n",
    "print(krd_data_valid.shape)\n",
    "print(krd_data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafba1fe",
   "metadata": {},
   "source": [
    "## 2.3. Data Reshaper for LSTM models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e13104",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f837d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Reshaper_Input(data, seq_length):\n",
    "    \n",
    "    numsubjects = len(np.unique(data['participant_id']))\n",
    "    myvary = list(data.columns.values)[2:data.shape[1]]\n",
    "    num_covariates = len(myvary)\n",
    "    \n",
    "    myinput = np.zeros((numsubjects, seq_length, num_covariates), dtype=np.float32)\n",
    "    for i in range(num_covariates):\n",
    "        data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=myvary[i])\n",
    "        data_wide = data_wide.sort_index(axis=1)\n",
    "        data_wide = data_wide.fillna(0)\n",
    "        tmpindex = data_wide._get_numeric_data().columns.values - 1\n",
    "        tmpindex = tmpindex.tolist()\n",
    "        # time varying variables need to impute all and no records are denoted as 0\n",
    "        for j in range(numsubjects):\n",
    "                myinput[j,tmpindex,i] = data_wide.iloc[[j]]\n",
    "    return myinput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a165dd37",
   "metadata": {},
   "source": [
    "**Warning**: *Longer running time*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693f8c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(729, 4, 9)\n",
      "(363, 4, 9)\n",
      "(120, 4, 9)\n",
      "(729, 4, 525)\n",
      "(363, 4, 525)\n",
      "(120, 4, 525)\n",
      "(729, 4, 1844)\n",
      "(363, 4, 1844)\n",
      "(120, 4, 1844)\n"
     ]
    }
   ],
   "source": [
    "metatrain_input = Data_Reshaper_Input(data=meta_data_train, seq_length=4)\n",
    "metavalid_input = Data_Reshaper_Input(data=meta_data_valid, seq_length=4)\n",
    "metatest_input  = Data_Reshaper_Input(data=meta_data_test, seq_length=4)\n",
    "print(metatrain_input.shape)\n",
    "print(metavalid_input.shape)\n",
    "print(metatest_input.shape)\n",
    "\n",
    "taxonomytrain_input = Data_Reshaper_Input(data=taxonomy_data_train, seq_length=4)\n",
    "taxonomyvalid_input = Data_Reshaper_Input(data=taxonomy_data_valid, seq_length=4)\n",
    "taxonomytest_input  = Data_Reshaper_Input(data=taxonomy_data_test, seq_length=4)\n",
    "print(taxonomytrain_input.shape)\n",
    "print(taxonomyvalid_input.shape)\n",
    "print(taxonomytest_input.shape)\n",
    "\n",
    "phylotypetrain_input = Data_Reshaper_Input(data=phylotype_data_train, seq_length=4)\n",
    "phylotypevalid_input = Data_Reshaper_Input(data=phylotype_data_valid, seq_length=4)\n",
    "phylotypetest_input = Data_Reshaper_Input(data=phylotype_data_test, seq_length=4)\n",
    "print(phylotypetrain_input.shape)\n",
    "print(phylotypevalid_input.shape)\n",
    "print(phylotypetest_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83751be2",
   "metadata": {},
   "source": [
    "### Output\n",
    "##### Data_Reshaper_Output_ManytoMany_0\n",
    "- reshape patients class labels from long to wide form;\n",
    "- output array formulation, **two** columns;\n",
    "- Label smoothing on 1stColumn;\n",
    "    - was_preterm: 0.5, 0.67, 0.83, 1;\n",
    "    - not was_preterm: 0.5, 0.33, 0.17, 0;\n",
    "    - no missing values indication;\n",
    "- Label smoothing on 2ndColumn: 1 - 1stColumn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "794f3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Reshaper_Output_ManytoMany_0(data, seq_length, classlabel):\n",
    "\n",
    "    num_samples = len(np.unique(data['participant_id']))\n",
    "    \n",
    "    data_wide = data.pivot_table(index=['participant_id'], columns='collect_period', values=classlabel)\n",
    "    data_wide = data_wide.sort_index(axis=1)\n",
    "    \n",
    "    myoutput = np.zeros((num_samples, seq_length, 2), dtype=np.float32)\n",
    "    for i in range(num_samples):\n",
    "        tmp = data_wide.iloc[i,:]\n",
    "        \n",
    "        if np.nanmax(tmp) == 1:\n",
    "            # label linear smoonthing from 0.5 to 1\n",
    "            # fill all position 1 to have final labels equal to 1\n",
    "            myoutput[i,:,0].fill(1)\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=1, num=seq_length)\n",
    "        else:\n",
    "            # label linear smoonthing from 0.5 to 0\n",
    "            # fill all position 0 to have final labels equal to 0 \n",
    "            #     but array alrady initialize as 0\n",
    "            myoutput[i,:,0] = np.linspace(start=0.5, stop=0, num=seq_length)\n",
    "            \n",
    "        myoutput[i,:,1] = 1 - myoutput[i,:,0]\n",
    "    return myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73240d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(729, 4, 2)\n",
      "[[0.5        0.5       ]\n",
      " [0.33333334 0.6666666 ]\n",
      " [0.16666667 0.8333333 ]\n",
      " [0.         1.        ]]\n",
      "(363, 4, 2)\n",
      "[[0.5        0.5       ]\n",
      " [0.33333334 0.6666666 ]\n",
      " [0.16666667 0.8333333 ]\n",
      " [0.         1.        ]]\n",
      "(120, 4, 2)\n",
      "[[0.5        0.5       ]\n",
      " [0.6666667  0.3333333 ]\n",
      " [0.8333333  0.16666669]\n",
      " [1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "mytrain_output_0 = Data_Reshaper_Output_ManytoMany_0(data=meta_data_train, seq_length=4, classlabel=\"was_preterm\")\n",
    "print(mytrain_output_0.shape)\n",
    "print(mytrain_output_0[2])\n",
    "\n",
    "myvalid_output_0 = Data_Reshaper_Output_ManytoMany_0(data=meta_data_valid, seq_length=4, classlabel=\"was_preterm\")\n",
    "print(myvalid_output_0.shape)\n",
    "print(myvalid_output_0[4])\n",
    "\n",
    "mytest_output_0 = Data_Reshaper_Output_ManytoMany_0(data=meta_data_test, seq_length=4, classlabel=\"was_preterm\")\n",
    "print(mytest_output_0.shape)\n",
    "print(mytest_output_0[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b596a",
   "metadata": {},
   "source": [
    "## 2.4. Phylotype LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4968ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_phylotype(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_phylotype, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        # (1) RNN layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=fc_size[2], bias=False)\n",
    "        self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        # (5) ReLU layer (Fix Vanishing Gradients ???)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size)\n",
    "        c0 = self.init_hidden(batch_size)\n",
    "        # Propagate input through LSTM/RNN\n",
    "        # outp, hidden = self.rnn(x, h0)     # lstm/rnn with input, hidden, and internal state\n",
    "        outp, hidden = self.lstm(x, (h0, c0))   # lstm/rnn with input, hidden, and internal state\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_4(outp)   # 4th Output\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe825570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNtrain_phylotype(model, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, verbose=True):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Track the value of the loss function and model accuracy across epochs\n",
    "    history_train_valid = {'TrainLoss': [], 'TrainAcc': [], 'TrainAUC': [],\n",
    "                           'ValidLoss': [], 'ValidAcc': [], 'ValidAUC': []}\n",
    "    \n",
    "    # Same reshaped Validation set for each epoch\n",
    "    myvalid_input  = torch.from_numpy(myvalid_input).float().to(device)\n",
    "    myvalid_output = torch.from_numpy(myvalid_output).float().to(device)\n",
    "    \n",
    "    valid_loss_min = np.inf\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        #-------------- Batch-wise training model --------------#\n",
    "        model.train()\n",
    "        # train_loss = 0.0\n",
    "        train_num_correct = 0\n",
    "        train_prob = []\n",
    "        for batch_idx in range(0, mytrain_input.shape[0], batch_size):\n",
    "            # subset a batch of sequences and class labels\n",
    "            tmpindex = list(range(batch_idx, min(batch_idx+batch_size, mytrain_input.shape[0])))\n",
    "            mytrain_input_batch  = mytrain_input[tmpindex,:]\n",
    "            mytrain_output_batch = mytrain_output[tmpindex,:]\n",
    "            mytrain_input_batch  = torch.from_numpy(mytrain_input_batch).float().to(device)\n",
    "            mytrain_output_batch = torch.from_numpy(mytrain_output_batch).float().to(device)\n",
    "            # forward pass of RNN model\n",
    "            output, hidden = model(mytrain_input_batch)\n",
    "            output = output.reshape((mytrain_output_batch.shape))\n",
    "            output_prob = nn.functional.softmax(output, dim=2)\n",
    "            loss = criterion(output_prob, mytrain_output_batch)\n",
    "            # Clear existing gradients from previous epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Does backpropagation and calculates gradients\n",
    "            loss.backward()\n",
    "            # Updates the weights accordingly\n",
    "            optimizer.step()\n",
    "            # Number correct prediction on trainning set collection\n",
    "            tmppred = 1*(output_prob[:,3,0] > 0.5)\n",
    "            train_num_correct += sum(1*(tmppred == mytrain_output_batch[:,3,0]))\n",
    "            # Training function loss collection\n",
    "            # train_loss += loss.item()\n",
    "            train_prob = np.concatenate((train_prob, output_prob[:,3,0].cpu().detach().numpy()), axis=None)\n",
    "            \n",
    "        train_acc = (float(train_num_correct) / len(mytrain_output))*100\n",
    "        train_auc = metrics.roc_auc_score(mytrain_output[:,3,0], train_prob)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Training loss calculation\n",
    "        tmpmytrain_input  = torch.from_numpy(mytrain_input).float().to(device)\n",
    "        tmpmytrain_output = torch.from_numpy(mytrain_output).float().to(device)\n",
    "        tmpoutputtrain, tmphidden = model(tmpmytrain_input)\n",
    "        tmpoutputtrain = tmpoutputtrain.reshape((tmpmytrain_output.shape))\n",
    "        tmpoutputtrain_prob = nn.functional.softmax(tmpoutputtrain, dim=2)\n",
    "        train_loss = criterion(tmpoutputtrain_prob, tmpmytrain_output)\n",
    "        \n",
    "        history_train_valid['TrainLoss'].append(train_loss.item())\n",
    "        history_train_valid['TrainAcc'].append(train_acc)\n",
    "        history_train_valid['TrainAUC'].append(train_auc)\n",
    "        \n",
    "\n",
    "        #--------------       Validate model      --------------#\n",
    "        outputvalid, hidden = model(myvalid_input)\n",
    "        outputvalid = outputvalid.reshape((myvalid_output.shape))\n",
    "        outputvalid_prob = nn.functional.softmax(outputvalid, dim=2)\n",
    "        # validation loss\n",
    "        valid_loss = criterion(outputvalid_prob, myvalid_output)\n",
    "        # Number correct prediction on trainning set collection\n",
    "        tmppredprob = outputvalid_prob[:,3,0].cpu().detach().numpy()\n",
    "        tmppred = 1*(tmppredprob > 0.5)\n",
    "        tmpobs = myvalid_output[:,3,0].cpu().detach().numpy()\n",
    "        valid_num_correct = sum(1*(tmppred == tmpobs))\n",
    "        valid_acc = (float(valid_num_correct) / len(myvalid_output))*100\n",
    "        valid_auc = metrics.roc_auc_score(tmpobs, tmppredprob)\n",
    "        \n",
    "        history_train_valid['ValidLoss'].append(valid_loss.item())\n",
    "        history_train_valid['ValidAcc'].append(valid_acc)\n",
    "        history_train_valid['ValidAUC'].append(valid_auc)\n",
    "        \n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[E {epoch + 1}/{max_epochs}]'\n",
    "                  f\" T.Loss: {history_train_valid['TrainLoss'][-1]:.4f}, T.Acc: {history_train_valid['TrainAcc'][-1]:2.2f}, T.AUC: {history_train_valid['TrainAUC'][-1]:.4f}\"\n",
    "                  f\" V.Loss: {history_train_valid['ValidLoss'][-1]:.4f}, V.Acc: {history_train_valid['ValidAcc'][-1]:2.2f}, V.AUC: {history_train_valid['ValidAUC'][-1]:.4f};\")\n",
    "        \n",
    "        valid_losses.append(valid_loss.item())\n",
    "        # start to considering early-stop after 10 epoch\n",
    "        if epoch > 10:\n",
    "            if np.mean(valid_losses) > valid_loss_min:\n",
    "                print(\"Stopping here!\")\n",
    "                break\n",
    "                # torch.save(model.state_dict(), './state_dict.pt')\n",
    "                # print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "            valid_loss_min = np.mean(valid_losses)\n",
    "            \n",
    "    return history_train_valid, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb2763",
   "metadata": {},
   "source": [
    "**Early-stopping prevent overfitting training set**:\n",
    "\n",
    "- After 10 epoches, if the average validation AUC value of the latest 10 epoches from $i^{th}$ iteration is smaller than the average of the latest 10 epoches from $(i-1)^{th}$ iteration, the LSTM training will stop.\n",
    "\n",
    "- dropout rate could also help prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62758f06",
   "metadata": {},
   "source": [
    "#### Phylotype LSTM hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e94eaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 1/100] T.Loss: 0.0967, T.Acc: 62.00, T.AUC: 0.6050 V.Loss: 0.0968, V.Acc: 65.84, V.AUC: 0.5772;\n",
      "[E 2/100] T.Loss: 0.0877, T.Acc: 69.27, T.AUC: 0.6423 V.Loss: 0.0899, V.Acc: 65.84, V.AUC: 0.5790;\n",
      "[E 3/100] T.Loss: 0.0847, T.Acc: 69.27, T.AUC: 0.6004 V.Loss: 0.0923, V.Acc: 65.84, V.AUC: 0.5938;\n",
      "[E 4/100] T.Loss: 0.0829, T.Acc: 69.27, T.AUC: 0.3734 V.Loss: 0.0871, V.Acc: 65.84, V.AUC: 0.6116;\n",
      "[E 5/100] T.Loss: 0.0818, T.Acc: 69.27, T.AUC: 0.5846 V.Loss: 0.0873, V.Acc: 65.84, V.AUC: 0.6218;\n",
      "[E 6/100] T.Loss: 0.0815, T.Acc: 69.27, T.AUC: 0.5112 V.Loss: 0.0876, V.Acc: 65.84, V.AUC: 0.6276;\n",
      "[E 7/100] T.Loss: 0.0813, T.Acc: 69.27, T.AUC: 0.4771 V.Loss: 0.0870, V.Acc: 65.84, V.AUC: 0.6320;\n",
      "[E 8/100] T.Loss: 0.0811, T.Acc: 69.27, T.AUC: 0.5098 V.Loss: 0.0869, V.Acc: 65.84, V.AUC: 0.6353;\n",
      "[E 9/100] T.Loss: 0.0808, T.Acc: 69.27, T.AUC: 0.5186 V.Loss: 0.0870, V.Acc: 65.84, V.AUC: 0.6364;\n",
      "[E 10/100] T.Loss: 0.0805, T.Acc: 69.27, T.AUC: 0.5154 V.Loss: 0.0866, V.Acc: 65.84, V.AUC: 0.6371;\n",
      "[E 11/100] T.Loss: 0.0803, T.Acc: 69.27, T.AUC: 0.5519 V.Loss: 0.0865, V.Acc: 65.84, V.AUC: 0.6401;\n",
      "[E 12/100] T.Loss: 0.0800, T.Acc: 69.27, T.AUC: 0.5465 V.Loss: 0.0863, V.Acc: 65.84, V.AUC: 0.6431;\n",
      "[E 13/100] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.5886 V.Loss: 0.0862, V.Acc: 65.84, V.AUC: 0.6424;\n",
      "[E 14/100] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.6202 V.Loss: 0.0861, V.Acc: 65.84, V.AUC: 0.6426;\n",
      "[E 15/100] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.5910 V.Loss: 0.0857, V.Acc: 65.84, V.AUC: 0.6428;\n",
      "[E 16/100] T.Loss: 0.0779, T.Acc: 69.27, T.AUC: 0.5831 V.Loss: 0.0853, V.Acc: 65.84, V.AUC: 0.6421;\n",
      "[E 17/100] T.Loss: 0.0771, T.Acc: 69.27, T.AUC: 0.6271 V.Loss: 0.0849, V.Acc: 65.84, V.AUC: 0.6461;\n",
      "[E 18/100] T.Loss: 0.0763, T.Acc: 69.27, T.AUC: 0.6042 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6447;\n",
      "[E 19/100] T.Loss: 0.0756, T.Acc: 69.27, T.AUC: 0.6208 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6431;\n",
      "[E 20/100] T.Loss: 0.0749, T.Acc: 69.27, T.AUC: 0.6446 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6437;\n",
      "[E 21/100] T.Loss: 0.0742, T.Acc: 69.27, T.AUC: 0.6430 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6431;\n",
      "[E 22/100] T.Loss: 0.0736, T.Acc: 69.27, T.AUC: 0.6641 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6415;\n",
      "[E 23/100] T.Loss: 0.0729, T.Acc: 69.27, T.AUC: 0.6682 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6404;\n",
      "[E 24/100] T.Loss: 0.0723, T.Acc: 69.27, T.AUC: 0.6642 V.Loss: 0.0823, V.Acc: 65.84, V.AUC: 0.6417;\n",
      "[E 25/100] T.Loss: 0.0717, T.Acc: 69.27, T.AUC: 0.7079 V.Loss: 0.0827, V.Acc: 65.84, V.AUC: 0.6412;\n",
      "[E 26/100] T.Loss: 0.0713, T.Acc: 69.27, T.AUC: 0.6933 V.Loss: 0.0829, V.Acc: 65.84, V.AUC: 0.6398;\n",
      "[E 27/100] T.Loss: 0.0708, T.Acc: 69.27, T.AUC: 0.6921 V.Loss: 0.0818, V.Acc: 65.84, V.AUC: 0.6421;\n",
      "[E 28/100] T.Loss: 0.0702, T.Acc: 69.27, T.AUC: 0.7157 V.Loss: 0.0822, V.Acc: 65.84, V.AUC: 0.6458;\n",
      "[E 29/100] T.Loss: 0.0698, T.Acc: 69.27, T.AUC: 0.7166 V.Loss: 0.0829, V.Acc: 65.84, V.AUC: 0.6480;\n",
      "[E 30/100] T.Loss: 0.0694, T.Acc: 69.27, T.AUC: 0.7273 V.Loss: 0.0823, V.Acc: 65.84, V.AUC: 0.6480;\n",
      "[E 31/100] T.Loss: 0.0693, T.Acc: 69.27, T.AUC: 0.7259 V.Loss: 0.0818, V.Acc: 65.84, V.AUC: 0.6478;\n",
      "[E 32/100] T.Loss: 0.0688, T.Acc: 69.27, T.AUC: 0.7488 V.Loss: 0.0830, V.Acc: 65.84, V.AUC: 0.6439;\n",
      "[E 33/100] T.Loss: 0.0685, T.Acc: 69.27, T.AUC: 0.7313 V.Loss: 0.0829, V.Acc: 65.84, V.AUC: 0.6429;\n",
      "[E 34/100] T.Loss: 0.0683, T.Acc: 69.27, T.AUC: 0.7479 V.Loss: 0.0822, V.Acc: 65.84, V.AUC: 0.6425;\n",
      "[E 35/100] T.Loss: 0.0677, T.Acc: 69.27, T.AUC: 0.7472 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6440;\n",
      "[E 36/100] T.Loss: 0.0675, T.Acc: 69.27, T.AUC: 0.7564 V.Loss: 0.0832, V.Acc: 65.84, V.AUC: 0.6483;\n",
      "[E 37/100] T.Loss: 0.0673, T.Acc: 69.27, T.AUC: 0.7707 V.Loss: 0.0825, V.Acc: 65.84, V.AUC: 0.6455;\n",
      "[E 38/100] T.Loss: 0.0668, T.Acc: 69.27, T.AUC: 0.7531 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6482;\n",
      "[E 39/100] T.Loss: 0.0665, T.Acc: 69.27, T.AUC: 0.7583 V.Loss: 0.0832, V.Acc: 65.84, V.AUC: 0.6489;\n",
      "[E 40/100] T.Loss: 0.0667, T.Acc: 69.27, T.AUC: 0.7641 V.Loss: 0.0829, V.Acc: 65.84, V.AUC: 0.6469;\n",
      "[E 41/100] T.Loss: 0.0657, T.Acc: 69.27, T.AUC: 0.7645 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6385;\n",
      "[E 42/100] T.Loss: 0.0667, T.Acc: 69.27, T.AUC: 0.7905 V.Loss: 0.0866, V.Acc: 65.84, V.AUC: 0.6428;\n",
      "Stopping here!\n"
     ]
    }
   ],
   "source": [
    "mytrain_input  = phylotypetrain_input\n",
    "mytrain_output = mytrain_output_0\n",
    "\n",
    "myvalid_input  = phylotypevalid_input\n",
    "myvalid_output = myvalid_output_0\n",
    "\n",
    "input_size  = mytrain_input.shape[2]    # number of features\n",
    "output_size = mytrain_output.shape[2]*4 # depend on the model\n",
    "seq_len     = 4\n",
    "hidden_dim  = 128\n",
    "n_layers    = 1\n",
    "fc_size     = [128, 64, 32]\n",
    "\n",
    "dropoutrate = 0.15\n",
    "lr         = 0.001\n",
    "max_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "# Automatically determine the device that PyTorch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_phylotype = Model_phylotype(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "criterion = nn.MSELoss() # mean-squared error multiple values (not 0 or 1)\n",
    "optimizer = torch.optim.Adam(model_phylotype.parameters(), lr=lr) \n",
    "\n",
    "TrainHistory, stop_epoch = RNNtrain_phylotype(model_phylotype, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f49bae8",
   "metadata": {},
   "source": [
    "## 2.5. Taxonomy LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "580c0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_taxonomy(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_taxonomy, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=fc_size[1], bias=False)\n",
    "        self.fc_3 = nn.Linear(in_features=fc_size[1], out_features=fc_size[2], bias=False)\n",
    "        self.fc_4 = nn.Linear(in_features=fc_size[2], out_features=output_size, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size)\n",
    "        c0 = self.init_hidden(batch_size)\n",
    "        # Propagate input through LSTM/RNN\n",
    "        # outp, hidden = self.rnn(x, h0)     # lstm/rnn with input, hidden, and internal state\n",
    "        outp, hidden = self.lstm(x, (h0, c0))   # lstm/rnn with input, hidden, and internal state\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_3(outp)   # 3rd Output\n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_4(outp)   # 4th Ouuput\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "299d9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNtrain_taxonomy(model, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, verbose=True):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Track the value of the loss function and model accuracy across epochs\n",
    "    history_train_valid = {'TrainLoss': [], 'TrainAcc': [], 'TrainAUC': [],\n",
    "                           'ValidLoss': [], 'ValidAcc': [], 'ValidAUC': []}\n",
    "    \n",
    "    # Same reshaped Validation set for each epoch\n",
    "    myvalid_input  = torch.from_numpy(myvalid_input).float().to(device)\n",
    "    myvalid_output = torch.from_numpy(myvalid_output).float().to(device)\n",
    "    \n",
    "    valid_loss_min = np.inf\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        #-------------- Batch-wise training model --------------#\n",
    "        model.train()\n",
    "        # train_loss = 0.0\n",
    "        train_num_correct = 0\n",
    "        train_prob = []\n",
    "        for batch_idx in range(0, mytrain_input.shape[0], batch_size):\n",
    "            # subset a batch of sequences and class labels\n",
    "            tmpindex = list(range(batch_idx, min(batch_idx+batch_size, mytrain_input.shape[0])))\n",
    "            mytrain_input_batch  = mytrain_input[tmpindex,:]\n",
    "            mytrain_output_batch = mytrain_output[tmpindex,:]\n",
    "            mytrain_input_batch  = torch.from_numpy(mytrain_input_batch).float().to(device)\n",
    "            mytrain_output_batch = torch.from_numpy(mytrain_output_batch).float().to(device)\n",
    "            # forward pass of RNN model\n",
    "            output, hidden = model(mytrain_input_batch)\n",
    "            output = output.reshape((mytrain_output_batch.shape))\n",
    "            output_prob = nn.functional.softmax(output, dim=2)\n",
    "            loss = criterion(output_prob, mytrain_output_batch)\n",
    "            # Clear existing gradients from previous epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Does backpropagation and calculates gradients\n",
    "            loss.backward()\n",
    "            # Updates the weights accordingly\n",
    "            optimizer.step()\n",
    "            # Number correct prediction on trainning set collection\n",
    "            tmppred = 1*(output_prob[:,3,0] > 0.5)\n",
    "            train_num_correct += sum(1*(tmppred == mytrain_output_batch[:,3,0]))\n",
    "            # Training function loss collection\n",
    "            # train_loss += loss.item()\n",
    "            train_prob = np.concatenate((train_prob, output_prob[:,3,0].cpu().detach().numpy()), axis=None)\n",
    "            \n",
    "        train_acc = (float(train_num_correct) / len(mytrain_output))*100\n",
    "        train_auc = metrics.roc_auc_score(mytrain_output[:,3,0], train_prob)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Training loss calculation\n",
    "        tmpmytrain_input  = torch.from_numpy(mytrain_input).float().to(device)\n",
    "        tmpmytrain_output = torch.from_numpy(mytrain_output).float().to(device)\n",
    "        tmpoutputtrain, tmphidden = model(tmpmytrain_input)\n",
    "        tmpoutputtrain = tmpoutputtrain.reshape((tmpmytrain_output.shape))\n",
    "        tmpoutputtrain_prob = nn.functional.softmax(tmpoutputtrain, dim=2)\n",
    "        train_loss = criterion(tmpoutputtrain_prob, tmpmytrain_output)\n",
    "        \n",
    "        history_train_valid['TrainLoss'].append(train_loss.item())\n",
    "        history_train_valid['TrainAcc'].append(train_acc)\n",
    "        history_train_valid['TrainAUC'].append(train_auc)\n",
    "        \n",
    "\n",
    "        #--------------       Validate model      --------------#\n",
    "        outputvalid, hidden = model(myvalid_input)\n",
    "        outputvalid = outputvalid.reshape((myvalid_output.shape))\n",
    "        outputvalid_prob = nn.functional.softmax(outputvalid, dim=2)\n",
    "        # validation loss\n",
    "        valid_loss = criterion(outputvalid_prob, myvalid_output)\n",
    "        # Number correct prediction on trainning set collection\n",
    "        tmppredprob = outputvalid_prob[:,3,0].cpu().detach().numpy()\n",
    "        tmppred = 1*(tmppredprob > 0.5)\n",
    "        tmpobs = myvalid_output[:,3,0].cpu().detach().numpy()\n",
    "        valid_num_correct = sum(1*(tmppred == tmpobs))\n",
    "        valid_acc = (float(valid_num_correct) / len(myvalid_output))*100\n",
    "        valid_auc = metrics.roc_auc_score(tmpobs, tmppredprob)\n",
    "        \n",
    "        history_train_valid['ValidLoss'].append(valid_loss.item())\n",
    "        history_train_valid['ValidAcc'].append(valid_acc)\n",
    "        history_train_valid['ValidAUC'].append(valid_auc)\n",
    "        \n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[E {epoch + 1}/{max_epochs}]'\n",
    "                  f\" T.Loss: {history_train_valid['TrainLoss'][-1]:.4f}, T.Acc: {history_train_valid['TrainAcc'][-1]:2.2f}, T.AUC: {history_train_valid['TrainAUC'][-1]:.4f}\"\n",
    "                  f\" V.Loss: {history_train_valid['ValidLoss'][-1]:.4f}, V.Acc: {history_train_valid['ValidAcc'][-1]:2.2f}, V.AUC: {history_train_valid['ValidAUC'][-1]:.4f};\")\n",
    "        \n",
    "        valid_losses.append(valid_loss.item())\n",
    "        # start to considering early-stop after 10 epoch\n",
    "        if epoch > 10:\n",
    "            if np.mean(valid_losses) > valid_loss_min:\n",
    "                print(\"Stopping here!\")\n",
    "                break\n",
    "                # torch.save(model.state_dict(), './state_dict.pt')\n",
    "                # print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "            valid_loss_min = np.mean(valid_losses)\n",
    "            \n",
    "    return history_train_valid, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c55b7fb",
   "metadata": {},
   "source": [
    "#### Taxonomy LSTM hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da1b20bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 1/100] T.Loss: 0.0967, T.Acc: 58.44, T.AUC: 0.5960 V.Loss: 0.0968, V.Acc: 65.84, V.AUC: 0.5028;\n",
      "[E 2/100] T.Loss: 0.0882, T.Acc: 69.27, T.AUC: 0.6499 V.Loss: 0.0903, V.Acc: 65.84, V.AUC: 0.5708;\n",
      "[E 3/100] T.Loss: 0.0855, T.Acc: 69.27, T.AUC: 0.6136 V.Loss: 0.0933, V.Acc: 65.84, V.AUC: 0.5957;\n",
      "[E 4/100] T.Loss: 0.0831, T.Acc: 69.27, T.AUC: 0.3566 V.Loss: 0.0871, V.Acc: 65.84, V.AUC: 0.6184;\n",
      "[E 5/100] T.Loss: 0.0820, T.Acc: 69.27, T.AUC: 0.5531 V.Loss: 0.0871, V.Acc: 65.84, V.AUC: 0.6237;\n",
      "[E 6/100] T.Loss: 0.0819, T.Acc: 69.27, T.AUC: 0.5130 V.Loss: 0.0876, V.Acc: 65.84, V.AUC: 0.6295;\n",
      "[E 7/100] T.Loss: 0.0816, T.Acc: 69.27, T.AUC: 0.4607 V.Loss: 0.0869, V.Acc: 65.84, V.AUC: 0.6351;\n",
      "[E 8/100] T.Loss: 0.0815, T.Acc: 69.27, T.AUC: 0.4761 V.Loss: 0.0867, V.Acc: 65.84, V.AUC: 0.6379;\n",
      "[E 9/100] T.Loss: 0.0813, T.Acc: 69.27, T.AUC: 0.5189 V.Loss: 0.0868, V.Acc: 65.84, V.AUC: 0.6389;\n",
      "[E 10/100] T.Loss: 0.0811, T.Acc: 69.27, T.AUC: 0.5255 V.Loss: 0.0866, V.Acc: 65.84, V.AUC: 0.6428;\n",
      "[E 11/100] T.Loss: 0.0809, T.Acc: 69.27, T.AUC: 0.5105 V.Loss: 0.0862, V.Acc: 65.84, V.AUC: 0.6438;\n",
      "[E 12/100] T.Loss: 0.0806, T.Acc: 69.27, T.AUC: 0.5239 V.Loss: 0.0860, V.Acc: 65.84, V.AUC: 0.6469;\n",
      "[E 13/100] T.Loss: 0.0802, T.Acc: 69.27, T.AUC: 0.5409 V.Loss: 0.0857, V.Acc: 65.84, V.AUC: 0.6514;\n",
      "[E 14/100] T.Loss: 0.0798, T.Acc: 69.27, T.AUC: 0.5689 V.Loss: 0.0853, V.Acc: 65.84, V.AUC: 0.6532;\n",
      "[E 15/100] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.5838 V.Loss: 0.0848, V.Acc: 65.84, V.AUC: 0.6540;\n",
      "[E 16/100] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.5704 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6557;\n",
      "[E 17/100] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.6117 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6567;\n",
      "[E 18/100] T.Loss: 0.0775, T.Acc: 69.27, T.AUC: 0.6058 V.Loss: 0.0832, V.Acc: 65.84, V.AUC: 0.6599;\n",
      "[E 19/100] T.Loss: 0.0769, T.Acc: 69.27, T.AUC: 0.6056 V.Loss: 0.0828, V.Acc: 65.84, V.AUC: 0.6642;\n",
      "[E 20/100] T.Loss: 0.0763, T.Acc: 69.27, T.AUC: 0.6213 V.Loss: 0.0825, V.Acc: 66.94, V.AUC: 0.6679;\n",
      "[E 21/100] T.Loss: 0.0757, T.Acc: 70.51, T.AUC: 0.6380 V.Loss: 0.0818, V.Acc: 67.22, V.AUC: 0.6699;\n",
      "[E 22/100] T.Loss: 0.0751, T.Acc: 71.60, T.AUC: 0.6638 V.Loss: 0.0816, V.Acc: 68.32, V.AUC: 0.6729;\n",
      "[E 23/100] T.Loss: 0.0738, T.Acc: 72.57, T.AUC: 0.6744 V.Loss: 0.0810, V.Acc: 68.04, V.AUC: 0.6737;\n",
      "[E 24/100] T.Loss: 0.0715, T.Acc: 72.57, T.AUC: 0.6558 V.Loss: 0.0803, V.Acc: 68.87, V.AUC: 0.6782;\n",
      "[E 25/100] T.Loss: 0.0718, T.Acc: 72.98, T.AUC: 0.6640 V.Loss: 0.0806, V.Acc: 68.32, V.AUC: 0.6840;\n",
      "[E 26/100] T.Loss: 0.0692, T.Acc: 73.80, T.AUC: 0.6831 V.Loss: 0.0798, V.Acc: 69.15, V.AUC: 0.6902;\n",
      "[E 27/100] T.Loss: 0.0707, T.Acc: 74.35, T.AUC: 0.6945 V.Loss: 0.0812, V.Acc: 68.87, V.AUC: 0.6907;\n",
      "[E 28/100] T.Loss: 0.0677, T.Acc: 74.35, T.AUC: 0.6986 V.Loss: 0.0797, V.Acc: 69.42, V.AUC: 0.6892;\n",
      "[E 29/100] T.Loss: 0.0674, T.Acc: 75.45, T.AUC: 0.6733 V.Loss: 0.0795, V.Acc: 69.15, V.AUC: 0.6891;\n",
      "[E 30/100] T.Loss: 0.0660, T.Acc: 76.41, T.AUC: 0.7036 V.Loss: 0.0811, V.Acc: 69.97, V.AUC: 0.6873;\n",
      "[E 31/100] T.Loss: 0.0652, T.Acc: 75.99, T.AUC: 0.6953 V.Loss: 0.0812, V.Acc: 69.97, V.AUC: 0.6846;\n",
      "[E 32/100] T.Loss: 0.0647, T.Acc: 76.27, T.AUC: 0.6777 V.Loss: 0.0816, V.Acc: 70.52, V.AUC: 0.6843;\n",
      "[E 33/100] T.Loss: 0.0672, T.Acc: 75.86, T.AUC: 0.6778 V.Loss: 0.0834, V.Acc: 69.42, V.AUC: 0.6848;\n",
      "[E 34/100] T.Loss: 0.0708, T.Acc: 76.95, T.AUC: 0.7119 V.Loss: 0.0830, V.Acc: 68.04, V.AUC: 0.6835;\n",
      "[E 35/100] T.Loss: 0.0652, T.Acc: 73.94, T.AUC: 0.6790 V.Loss: 0.0820, V.Acc: 68.87, V.AUC: 0.6843;\n",
      "[E 36/100] T.Loss: 0.0648, T.Acc: 77.09, T.AUC: 0.7162 V.Loss: 0.0846, V.Acc: 68.04, V.AUC: 0.6788;\n",
      "Stopping here!\n"
     ]
    }
   ],
   "source": [
    "mytrain_input  = taxonomytrain_input\n",
    "mytrain_output = mytrain_output_0\n",
    "\n",
    "myvalid_input  = taxonomyvalid_input\n",
    "myvalid_output = myvalid_output_0\n",
    "\n",
    "input_size  = mytrain_input.shape[2]    # number of features\n",
    "output_size = mytrain_output.shape[2]*4 # depend on the model\n",
    "seq_len     = 4\n",
    "hidden_dim  = 128\n",
    "n_layers    = 1\n",
    "fc_size     = [128, 64, 32]\n",
    "\n",
    "dropoutrate = 0.15\n",
    "lr         = 0.001\n",
    "max_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "# Automatically determine the device that PyTorch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_taxonomy = Model_taxonomy(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "criterion = nn.MSELoss() # mean-squared error multiple values (not 0 or 1)\n",
    "optimizer = torch.optim.Adam(model_taxonomy.parameters(), lr=lr) \n",
    "\n",
    "TrainHistory, stop_epoch = RNNtrain_taxonomy(model_taxonomy, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c76b42",
   "metadata": {},
   "source": [
    "## 2.6. Metadata LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24043705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(729, 4, 5)\n",
      "(363, 4, 5)\n",
      "(120, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "meta_data_LSTM_train = meta_data_train.drop(['project', 'delivery_wk', 'was_preterm', 'was_early_preterm'], axis=1)\n",
    "meta_data_LSTM_valid = meta_data_valid.drop(['project', 'delivery_wk', 'was_preterm', 'was_early_preterm'], axis=1)\n",
    "meta_data_LSTM_test  = meta_data_test.drop(['project', 'delivery_wk', 'was_preterm', 'was_early_preterm'], axis=1)\n",
    "\n",
    "columns = ['collect_wk', 'age_imp', 'race_imp', 'shannon', 'CST']\n",
    "for col in columns:\n",
    "    meta_data_LSTM_train[col] = MinMaxScaler().fit_transform(np.array(meta_data_LSTM_train[col]).reshape(-1,1))\n",
    "    meta_data_LSTM_valid[col] = MinMaxScaler().fit_transform(np.array(meta_data_LSTM_valid[col]).reshape(-1,1))\n",
    "    meta_data_LSTM_test[col]  = MinMaxScaler().fit_transform(np.array(meta_data_LSTM_test[col]).reshape(-1,1))\n",
    "    \n",
    "metatrain_input = Data_Reshaper_Input(data=meta_data_LSTM_train, seq_length=4)\n",
    "metavalid_input = Data_Reshaper_Input(data=meta_data_LSTM_valid, seq_length=4)\n",
    "metatest_input  = Data_Reshaper_Input(data=meta_data_LSTM_test,  seq_length=4)\n",
    "print(metatrain_input.shape)\n",
    "print(metavalid_input.shape)\n",
    "print(metatest_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c1e6642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_meta(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, seq_len, n_layers, fc_size, dropoutrate):\n",
    "        super(Model_meta, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size  = input_size      # number of input node\n",
    "        self.output_size = output_size     # number of output node\n",
    "        self.seq_len     = seq_len         # seq_len: number of timepoints (collection period)\n",
    "        self.fc_size     = fc_size         # size of the fully connected net\n",
    "        self.n_layers    = n_layers        # number of LSTM/RNN layers\n",
    "        self.hidden_dim  = hidden_dim      # hidden size of LSTM/RNN, also the size of fully connected NN 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(in_features=hidden_dim*seq_len, out_features=fc_size[0], bias=False)\n",
    "        self.fc_2 = nn.Linear(in_features=fc_size[0], out_features=output_size, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # define dropout proportion to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropoutrate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        batch_size = x.size(0)\n",
    "        h0 = self.init_hidden(batch_size)\n",
    "        c0 = self.init_hidden(batch_size)\n",
    "        # Propagate input through LSTM/RNN\n",
    "        # outp, hidden = self.rnn(x, h0)     # lstm/rnn with input, hidden, and internal state\n",
    "        outp, hidden = self.lstm(x, (h0, c0))   # lstm/rnn with input, hidden, and internal state\n",
    "        \n",
    "        outp = outp.reshape(outp.shape[0], -1)  # reshaping the data for Dense layer next\n",
    "        \n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.dropout(outp)# dropout\n",
    "        outp = self.fc_1(outp)   # first Dense\n",
    "        outp = self.relu(outp)   # relu\n",
    "        outp = self.fc_2(outp)   # 2nd Dense\n",
    "        \n",
    "        return outp, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "411a847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNtrain_meta(model, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, verbose=True):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Track the value of the loss function and model accuracy across epochs\n",
    "    history_train_valid = {'TrainLoss': [], 'TrainAcc': [], 'TrainAUC': [],\n",
    "                           'ValidLoss': [], 'ValidAcc': [], 'ValidAUC': []}\n",
    "    \n",
    "    # Same reshaped Validation set for each epoch\n",
    "    myvalid_input  = torch.from_numpy(myvalid_input).float().to(device)\n",
    "    myvalid_output = torch.from_numpy(myvalid_output).float().to(device)\n",
    "    \n",
    "    valid_loss_min = np.inf\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        #-------------- Batch-wise training model --------------#\n",
    "        model.train()\n",
    "        # train_loss = 0.0\n",
    "        train_num_correct = 0\n",
    "        train_prob = []\n",
    "        for batch_idx in range(0, mytrain_input.shape[0], batch_size):\n",
    "            # subset a batch of sequences and class labels\n",
    "            tmpindex = list(range(batch_idx, min(batch_idx+batch_size, mytrain_input.shape[0])))\n",
    "            mytrain_input_batch  = mytrain_input[tmpindex,:]\n",
    "            mytrain_output_batch = mytrain_output[tmpindex,:]\n",
    "            mytrain_input_batch  = torch.from_numpy(mytrain_input_batch).float().to(device)\n",
    "            mytrain_output_batch = torch.from_numpy(mytrain_output_batch).float().to(device)\n",
    "            # forward pass of RNN model\n",
    "            output, hidden = model(mytrain_input_batch)\n",
    "            output = output.reshape((mytrain_output_batch.shape))\n",
    "            output_prob = nn.functional.softmax(output, dim=2)\n",
    "            loss = criterion(output_prob, mytrain_output_batch)\n",
    "            # Clear existing gradients from previous epoch\n",
    "            optimizer.zero_grad()\n",
    "            # Does backpropagation and calculates gradients\n",
    "            loss.backward()\n",
    "            # Updates the weights accordingly\n",
    "            optimizer.step()\n",
    "            # Number correct prediction on trainning set collection\n",
    "            tmppred = 1*(output_prob[:,3,0] > 0.5)\n",
    "            train_num_correct += sum(1*(tmppred == mytrain_output_batch[:,3,0]))\n",
    "            # Training function loss collection\n",
    "            # train_loss += loss.item()\n",
    "            train_prob = np.concatenate((train_prob, output_prob[:,3,0].cpu().detach().numpy()), axis=None)\n",
    "            \n",
    "        train_acc = (float(train_num_correct) / len(mytrain_output))*100\n",
    "        train_auc = metrics.roc_auc_score(mytrain_output[:,3,0], train_prob)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Training loss calculation\n",
    "        tmpmytrain_input  = torch.from_numpy(mytrain_input).float().to(device)\n",
    "        tmpmytrain_output = torch.from_numpy(mytrain_output).float().to(device)\n",
    "        tmpoutputtrain, tmphidden = model(tmpmytrain_input)\n",
    "        tmpoutputtrain = tmpoutputtrain.reshape((tmpmytrain_output.shape))\n",
    "        tmpoutputtrain_prob = nn.functional.softmax(tmpoutputtrain, dim=2)\n",
    "        train_loss = criterion(tmpoutputtrain_prob, tmpmytrain_output)\n",
    "        \n",
    "        history_train_valid['TrainLoss'].append(train_loss.item())\n",
    "        history_train_valid['TrainAcc'].append(train_acc)\n",
    "        history_train_valid['TrainAUC'].append(train_auc)\n",
    "        \n",
    "        #--------------       Validate model      --------------#\n",
    "        outputvalid, hidden = model(myvalid_input)\n",
    "        outputvalid = outputvalid.reshape((myvalid_output.shape))\n",
    "        outputvalid_prob = nn.functional.softmax(outputvalid, dim=2)\n",
    "        # validation loss\n",
    "        valid_loss = criterion(outputvalid_prob, myvalid_output)\n",
    "        # Number correct prediction on trainning set collection\n",
    "        tmppredprob = outputvalid_prob[:,3,0].cpu().detach().numpy()\n",
    "        tmppred = 1*(tmppredprob > 0.5)\n",
    "        tmpobs = myvalid_output[:,3,0].cpu().detach().numpy()\n",
    "        valid_num_correct = sum(1*(tmppred == tmpobs))\n",
    "        valid_acc = (float(valid_num_correct) / len(myvalid_output))*100\n",
    "        valid_auc = metrics.roc_auc_score(tmpobs, tmppredprob)\n",
    "        \n",
    "        history_train_valid['ValidLoss'].append(valid_loss.item())\n",
    "        history_train_valid['ValidAcc'].append(valid_acc)\n",
    "        history_train_valid['ValidAUC'].append(valid_auc)\n",
    "        \n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[E {epoch + 1}/{max_epochs}]'\n",
    "                  f\" T.Loss: {history_train_valid['TrainLoss'][-1]:.4f}, T.Acc: {history_train_valid['TrainAcc'][-1]:2.2f}, T.AUC: {history_train_valid['TrainAUC'][-1]:.4f}\"\n",
    "                  f\" V.Loss: {history_train_valid['ValidLoss'][-1]:.4f}, V.Acc: {history_train_valid['ValidAcc'][-1]:2.2f}, V.AUC: {history_train_valid['ValidAUC'][-1]:.4f};\")\n",
    "        \n",
    "        valid_losses.append(valid_loss.item())\n",
    "        # start to considering early-stop after 10 epoch\n",
    "        if epoch > 10:\n",
    "            if np.mean(valid_losses) > valid_loss_min:\n",
    "                print(\"Stopping here!\")\n",
    "                break\n",
    "                # torch.save(model.state_dict(), './state_dict.pt')\n",
    "                # print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "            valid_loss_min = np.mean(valid_losses)\n",
    "            \n",
    "    return history_train_valid, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d71e23",
   "metadata": {},
   "source": [
    "#### Metadata LSTM hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39802604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 1/2000] T.Loss: 0.0955, T.Acc: 69.00, T.AUC: 0.5874 V.Loss: 0.0959, V.Acc: 65.84, V.AUC: 0.4032;\n",
      "[E 2/2000] T.Loss: 0.0930, T.Acc: 69.27, T.AUC: 0.6154 V.Loss: 0.0939, V.Acc: 65.84, V.AUC: 0.3904;\n",
      "[E 3/2000] T.Loss: 0.0891, T.Acc: 69.27, T.AUC: 0.6266 V.Loss: 0.0910, V.Acc: 65.84, V.AUC: 0.4457;\n",
      "[E 4/2000] T.Loss: 0.0853, T.Acc: 69.27, T.AUC: 0.5896 V.Loss: 0.0886, V.Acc: 65.84, V.AUC: 0.4635;\n",
      "[E 5/2000] T.Loss: 0.0833, T.Acc: 69.27, T.AUC: 0.5677 V.Loss: 0.0879, V.Acc: 65.84, V.AUC: 0.4717;\n",
      "[E 6/2000] T.Loss: 0.0829, T.Acc: 69.27, T.AUC: 0.5063 V.Loss: 0.0883, V.Acc: 65.84, V.AUC: 0.4814;\n",
      "[E 7/2000] T.Loss: 0.0828, T.Acc: 69.27, T.AUC: 0.4862 V.Loss: 0.0885, V.Acc: 65.84, V.AUC: 0.4922;\n",
      "[E 8/2000] T.Loss: 0.0828, T.Acc: 69.27, T.AUC: 0.4892 V.Loss: 0.0884, V.Acc: 65.84, V.AUC: 0.5010;\n",
      "[E 9/2000] T.Loss: 0.0827, T.Acc: 69.27, T.AUC: 0.4304 V.Loss: 0.0883, V.Acc: 65.84, V.AUC: 0.5089;\n",
      "[E 10/2000] T.Loss: 0.0827, T.Acc: 69.27, T.AUC: 0.5089 V.Loss: 0.0882, V.Acc: 65.84, V.AUC: 0.5165;\n",
      "[E 11/2000] T.Loss: 0.0826, T.Acc: 69.27, T.AUC: 0.4998 V.Loss: 0.0882, V.Acc: 65.84, V.AUC: 0.5227;\n",
      "[E 12/2000] T.Loss: 0.0826, T.Acc: 69.27, T.AUC: 0.5069 V.Loss: 0.0881, V.Acc: 65.84, V.AUC: 0.5315;\n",
      "[E 13/2000] T.Loss: 0.0825, T.Acc: 69.27, T.AUC: 0.5178 V.Loss: 0.0881, V.Acc: 65.84, V.AUC: 0.5394;\n",
      "[E 14/2000] T.Loss: 0.0825, T.Acc: 69.27, T.AUC: 0.5281 V.Loss: 0.0880, V.Acc: 65.84, V.AUC: 0.5467;\n",
      "[E 15/2000] T.Loss: 0.0824, T.Acc: 69.27, T.AUC: 0.5391 V.Loss: 0.0880, V.Acc: 65.84, V.AUC: 0.5523;\n",
      "[E 16/2000] T.Loss: 0.0823, T.Acc: 69.27, T.AUC: 0.5173 V.Loss: 0.0880, V.Acc: 65.84, V.AUC: 0.5586;\n",
      "[E 17/2000] T.Loss: 0.0823, T.Acc: 69.27, T.AUC: 0.5045 V.Loss: 0.0879, V.Acc: 65.84, V.AUC: 0.5647;\n",
      "[E 18/2000] T.Loss: 0.0822, T.Acc: 69.27, T.AUC: 0.5181 V.Loss: 0.0878, V.Acc: 65.84, V.AUC: 0.5695;\n",
      "[E 19/2000] T.Loss: 0.0821, T.Acc: 69.27, T.AUC: 0.5324 V.Loss: 0.0877, V.Acc: 65.84, V.AUC: 0.5751;\n",
      "[E 20/2000] T.Loss: 0.0821, T.Acc: 69.27, T.AUC: 0.5574 V.Loss: 0.0877, V.Acc: 65.84, V.AUC: 0.5785;\n",
      "[E 21/2000] T.Loss: 0.0820, T.Acc: 69.27, T.AUC: 0.5393 V.Loss: 0.0877, V.Acc: 65.84, V.AUC: 0.5817;\n",
      "[E 22/2000] T.Loss: 0.0819, T.Acc: 69.27, T.AUC: 0.5678 V.Loss: 0.0876, V.Acc: 65.84, V.AUC: 0.5836;\n",
      "[E 23/2000] T.Loss: 0.0819, T.Acc: 69.27, T.AUC: 0.5187 V.Loss: 0.0875, V.Acc: 65.84, V.AUC: 0.5877;\n",
      "[E 24/2000] T.Loss: 0.0818, T.Acc: 69.27, T.AUC: 0.5714 V.Loss: 0.0874, V.Acc: 65.84, V.AUC: 0.5905;\n",
      "[E 25/2000] T.Loss: 0.0817, T.Acc: 69.27, T.AUC: 0.5621 V.Loss: 0.0873, V.Acc: 65.84, V.AUC: 0.5926;\n",
      "[E 26/2000] T.Loss: 0.0816, T.Acc: 69.27, T.AUC: 0.5457 V.Loss: 0.0873, V.Acc: 65.84, V.AUC: 0.5946;\n",
      "[E 27/2000] T.Loss: 0.0816, T.Acc: 69.27, T.AUC: 0.5772 V.Loss: 0.0872, V.Acc: 65.84, V.AUC: 0.5976;\n",
      "[E 28/2000] T.Loss: 0.0815, T.Acc: 69.27, T.AUC: 0.6107 V.Loss: 0.0872, V.Acc: 65.84, V.AUC: 0.6010;\n",
      "[E 29/2000] T.Loss: 0.0814, T.Acc: 69.27, T.AUC: 0.5831 V.Loss: 0.0871, V.Acc: 65.84, V.AUC: 0.6028;\n",
      "[E 30/2000] T.Loss: 0.0813, T.Acc: 69.27, T.AUC: 0.5486 V.Loss: 0.0870, V.Acc: 65.84, V.AUC: 0.6051;\n",
      "[E 31/2000] T.Loss: 0.0813, T.Acc: 69.27, T.AUC: 0.5883 V.Loss: 0.0869, V.Acc: 65.84, V.AUC: 0.6079;\n",
      "[E 32/2000] T.Loss: 0.0812, T.Acc: 69.27, T.AUC: 0.5782 V.Loss: 0.0868, V.Acc: 65.84, V.AUC: 0.6096;\n",
      "[E 33/2000] T.Loss: 0.0811, T.Acc: 69.27, T.AUC: 0.6127 V.Loss: 0.0868, V.Acc: 65.84, V.AUC: 0.6121;\n",
      "[E 34/2000] T.Loss: 0.0810, T.Acc: 69.27, T.AUC: 0.5852 V.Loss: 0.0867, V.Acc: 65.84, V.AUC: 0.6164;\n",
      "[E 35/2000] T.Loss: 0.0809, T.Acc: 69.27, T.AUC: 0.6120 V.Loss: 0.0866, V.Acc: 65.84, V.AUC: 0.6182;\n",
      "[E 36/2000] T.Loss: 0.0808, T.Acc: 69.27, T.AUC: 0.5812 V.Loss: 0.0865, V.Acc: 65.84, V.AUC: 0.6202;\n",
      "[E 37/2000] T.Loss: 0.0807, T.Acc: 69.27, T.AUC: 0.5869 V.Loss: 0.0865, V.Acc: 65.84, V.AUC: 0.6212;\n",
      "[E 38/2000] T.Loss: 0.0806, T.Acc: 69.27, T.AUC: 0.6130 V.Loss: 0.0863, V.Acc: 65.84, V.AUC: 0.6224;\n",
      "[E 39/2000] T.Loss: 0.0805, T.Acc: 69.27, T.AUC: 0.5935 V.Loss: 0.0862, V.Acc: 65.84, V.AUC: 0.6233;\n",
      "[E 40/2000] T.Loss: 0.0804, T.Acc: 69.27, T.AUC: 0.6045 V.Loss: 0.0861, V.Acc: 65.84, V.AUC: 0.6234;\n",
      "[E 41/2000] T.Loss: 0.0803, T.Acc: 69.27, T.AUC: 0.6204 V.Loss: 0.0860, V.Acc: 65.84, V.AUC: 0.6215;\n",
      "[E 42/2000] T.Loss: 0.0802, T.Acc: 69.27, T.AUC: 0.6275 V.Loss: 0.0860, V.Acc: 65.84, V.AUC: 0.6220;\n",
      "[E 43/2000] T.Loss: 0.0801, T.Acc: 69.27, T.AUC: 0.6027 V.Loss: 0.0859, V.Acc: 65.84, V.AUC: 0.6228;\n",
      "[E 44/2000] T.Loss: 0.0801, T.Acc: 69.27, T.AUC: 0.5856 V.Loss: 0.0857, V.Acc: 65.84, V.AUC: 0.6233;\n",
      "[E 45/2000] T.Loss: 0.0800, T.Acc: 69.27, T.AUC: 0.6062 V.Loss: 0.0857, V.Acc: 65.84, V.AUC: 0.6246;\n",
      "[E 46/2000] T.Loss: 0.0799, T.Acc: 69.27, T.AUC: 0.6220 V.Loss: 0.0856, V.Acc: 65.84, V.AUC: 0.6259;\n",
      "[E 47/2000] T.Loss: 0.0798, T.Acc: 69.27, T.AUC: 0.6264 V.Loss: 0.0855, V.Acc: 65.84, V.AUC: 0.6275;\n",
      "[E 48/2000] T.Loss: 0.0797, T.Acc: 69.27, T.AUC: 0.5939 V.Loss: 0.0855, V.Acc: 65.84, V.AUC: 0.6283;\n",
      "[E 49/2000] T.Loss: 0.0796, T.Acc: 69.27, T.AUC: 0.6311 V.Loss: 0.0854, V.Acc: 65.84, V.AUC: 0.6282;\n",
      "[E 50/2000] T.Loss: 0.0795, T.Acc: 69.27, T.AUC: 0.6274 V.Loss: 0.0852, V.Acc: 65.84, V.AUC: 0.6301;\n",
      "[E 51/2000] T.Loss: 0.0793, T.Acc: 69.27, T.AUC: 0.6282 V.Loss: 0.0851, V.Acc: 65.84, V.AUC: 0.6315;\n",
      "[E 52/2000] T.Loss: 0.0792, T.Acc: 69.27, T.AUC: 0.6266 V.Loss: 0.0850, V.Acc: 65.84, V.AUC: 0.6331;\n",
      "[E 53/2000] T.Loss: 0.0791, T.Acc: 69.27, T.AUC: 0.6222 V.Loss: 0.0849, V.Acc: 65.84, V.AUC: 0.6347;\n",
      "[E 54/2000] T.Loss: 0.0790, T.Acc: 69.27, T.AUC: 0.6221 V.Loss: 0.0848, V.Acc: 65.84, V.AUC: 0.6349;\n",
      "[E 55/2000] T.Loss: 0.0789, T.Acc: 69.27, T.AUC: 0.6395 V.Loss: 0.0847, V.Acc: 65.84, V.AUC: 0.6348;\n",
      "[E 56/2000] T.Loss: 0.0788, T.Acc: 69.27, T.AUC: 0.6279 V.Loss: 0.0846, V.Acc: 65.84, V.AUC: 0.6349;\n",
      "[E 57/2000] T.Loss: 0.0787, T.Acc: 69.27, T.AUC: 0.6417 V.Loss: 0.0845, V.Acc: 65.84, V.AUC: 0.6353;\n",
      "[E 58/2000] T.Loss: 0.0786, T.Acc: 69.27, T.AUC: 0.6420 V.Loss: 0.0844, V.Acc: 65.84, V.AUC: 0.6384;\n",
      "[E 59/2000] T.Loss: 0.0785, T.Acc: 69.14, T.AUC: 0.6450 V.Loss: 0.0843, V.Acc: 65.84, V.AUC: 0.6389;\n",
      "[E 60/2000] T.Loss: 0.0784, T.Acc: 69.27, T.AUC: 0.6454 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6412;\n",
      "[E 61/2000] T.Loss: 0.0783, T.Acc: 69.00, T.AUC: 0.6472 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6409;\n",
      "[E 62/2000] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.6383 V.Loss: 0.0842, V.Acc: 65.84, V.AUC: 0.6409;\n",
      "[E 63/2000] T.Loss: 0.0782, T.Acc: 69.27, T.AUC: 0.6325 V.Loss: 0.0841, V.Acc: 65.84, V.AUC: 0.6442;\n",
      "[E 64/2000] T.Loss: 0.0781, T.Acc: 69.27, T.AUC: 0.6263 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6437;\n",
      "[E 65/2000] T.Loss: 0.0781, T.Acc: 69.55, T.AUC: 0.6246 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6447;\n",
      "[E 66/2000] T.Loss: 0.0780, T.Acc: 69.41, T.AUC: 0.6294 V.Loss: 0.0840, V.Acc: 65.84, V.AUC: 0.6445;\n",
      "[E 67/2000] T.Loss: 0.0779, T.Acc: 69.14, T.AUC: 0.6481 V.Loss: 0.0839, V.Acc: 65.84, V.AUC: 0.6437;\n",
      "[E 68/2000] T.Loss: 0.0779, T.Acc: 68.86, T.AUC: 0.6418 V.Loss: 0.0838, V.Acc: 65.84, V.AUC: 0.6442;\n",
      "[E 69/2000] T.Loss: 0.0778, T.Acc: 69.14, T.AUC: 0.6366 V.Loss: 0.0838, V.Acc: 65.84, V.AUC: 0.6435;\n",
      "[E 70/2000] T.Loss: 0.0778, T.Acc: 69.27, T.AUC: 0.6367 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6447;\n",
      "[E 71/2000] T.Loss: 0.0777, T.Acc: 69.14, T.AUC: 0.6345 V.Loss: 0.0837, V.Acc: 65.84, V.AUC: 0.6447;\n",
      "[E 72/2000] T.Loss: 0.0776, T.Acc: 69.14, T.AUC: 0.6492 V.Loss: 0.0836, V.Acc: 65.84, V.AUC: 0.6425;\n",
      "[E 73/2000] T.Loss: 0.0776, T.Acc: 69.27, T.AUC: 0.6427 V.Loss: 0.0835, V.Acc: 65.84, V.AUC: 0.6421;\n",
      "[E 74/2000] T.Loss: 0.0775, T.Acc: 69.14, T.AUC: 0.6301 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6425;\n",
      "[E 75/2000] T.Loss: 0.0775, T.Acc: 69.55, T.AUC: 0.6547 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6427;\n",
      "[E 76/2000] T.Loss: 0.0774, T.Acc: 68.59, T.AUC: 0.6274 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6433;\n",
      "[E 77/2000] T.Loss: 0.0774, T.Acc: 69.27, T.AUC: 0.6601 V.Loss: 0.0834, V.Acc: 65.84, V.AUC: 0.6431;\n",
      "[E 78/2000] T.Loss: 0.0773, T.Acc: 68.72, T.AUC: 0.6351 V.Loss: 0.0833, V.Acc: 66.12, V.AUC: 0.6419;\n",
      "[E 79/2000] T.Loss: 0.0772, T.Acc: 68.45, T.AUC: 0.6670 V.Loss: 0.0833, V.Acc: 66.12, V.AUC: 0.6422;\n",
      "[E 80/2000] T.Loss: 0.0772, T.Acc: 68.31, T.AUC: 0.6372 V.Loss: 0.0833, V.Acc: 66.12, V.AUC: 0.6427;\n",
      "[E 81/2000] T.Loss: 0.0772, T.Acc: 69.41, T.AUC: 0.6370 V.Loss: 0.0833, V.Acc: 66.12, V.AUC: 0.6425;\n",
      "[E 82/2000] T.Loss: 0.0771, T.Acc: 68.72, T.AUC: 0.6580 V.Loss: 0.0832, V.Acc: 66.12, V.AUC: 0.6431;\n",
      "[E 83/2000] T.Loss: 0.0770, T.Acc: 68.45, T.AUC: 0.6514 V.Loss: 0.0832, V.Acc: 66.12, V.AUC: 0.6427;\n",
      "[E 84/2000] T.Loss: 0.0770, T.Acc: 69.00, T.AUC: 0.6800 V.Loss: 0.0832, V.Acc: 66.12, V.AUC: 0.6412;\n",
      "[E 85/2000] T.Loss: 0.0769, T.Acc: 68.18, T.AUC: 0.6332 V.Loss: 0.0831, V.Acc: 66.12, V.AUC: 0.6409;\n",
      "[E 86/2000] T.Loss: 0.0769, T.Acc: 68.59, T.AUC: 0.6472 V.Loss: 0.0831, V.Acc: 66.12, V.AUC: 0.6410;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 87/2000] T.Loss: 0.0768, T.Acc: 68.72, T.AUC: 0.6692 V.Loss: 0.0831, V.Acc: 65.84, V.AUC: 0.6417;\n",
      "[E 88/2000] T.Loss: 0.0767, T.Acc: 68.31, T.AUC: 0.6585 V.Loss: 0.0830, V.Acc: 65.56, V.AUC: 0.6404;\n",
      "[E 89/2000] T.Loss: 0.0766, T.Acc: 68.18, T.AUC: 0.6664 V.Loss: 0.0829, V.Acc: 65.56, V.AUC: 0.6415;\n",
      "[E 90/2000] T.Loss: 0.0766, T.Acc: 68.72, T.AUC: 0.6387 V.Loss: 0.0829, V.Acc: 65.56, V.AUC: 0.6407;\n",
      "[E 91/2000] T.Loss: 0.0765, T.Acc: 67.90, T.AUC: 0.6551 V.Loss: 0.0829, V.Acc: 65.56, V.AUC: 0.6414;\n",
      "[E 92/2000] T.Loss: 0.0765, T.Acc: 69.55, T.AUC: 0.6532 V.Loss: 0.0829, V.Acc: 65.56, V.AUC: 0.6410;\n",
      "[E 93/2000] T.Loss: 0.0765, T.Acc: 69.14, T.AUC: 0.6496 V.Loss: 0.0829, V.Acc: 65.56, V.AUC: 0.6409;\n",
      "[E 94/2000] T.Loss: 0.0764, T.Acc: 68.86, T.AUC: 0.6400 V.Loss: 0.0829, V.Acc: 65.56, V.AUC: 0.6410;\n",
      "[E 95/2000] T.Loss: 0.0764, T.Acc: 69.14, T.AUC: 0.6347 V.Loss: 0.0829, V.Acc: 65.56, V.AUC: 0.6393;\n",
      "[E 96/2000] T.Loss: 0.0763, T.Acc: 68.31, T.AUC: 0.6724 V.Loss: 0.0829, V.Acc: 65.56, V.AUC: 0.6385;\n",
      "[E 97/2000] T.Loss: 0.0762, T.Acc: 68.59, T.AUC: 0.6497 V.Loss: 0.0829, V.Acc: 65.29, V.AUC: 0.6378;\n",
      "[E 98/2000] T.Loss: 0.0762, T.Acc: 68.86, T.AUC: 0.6553 V.Loss: 0.0829, V.Acc: 65.29, V.AUC: 0.6377;\n",
      "[E 99/2000] T.Loss: 0.0761, T.Acc: 69.00, T.AUC: 0.6637 V.Loss: 0.0829, V.Acc: 65.56, V.AUC: 0.6389;\n",
      "[E 100/2000] T.Loss: 0.0761, T.Acc: 69.14, T.AUC: 0.6541 V.Loss: 0.0829, V.Acc: 66.12, V.AUC: 0.6407;\n",
      "[E 101/2000] T.Loss: 0.0761, T.Acc: 69.41, T.AUC: 0.6571 V.Loss: 0.0829, V.Acc: 65.84, V.AUC: 0.6399;\n",
      "[E 102/2000] T.Loss: 0.0760, T.Acc: 68.31, T.AUC: 0.6462 V.Loss: 0.0829, V.Acc: 65.84, V.AUC: 0.6404;\n",
      "[E 103/2000] T.Loss: 0.0759, T.Acc: 69.14, T.AUC: 0.6576 V.Loss: 0.0828, V.Acc: 65.29, V.AUC: 0.6402;\n",
      "[E 104/2000] T.Loss: 0.0759, T.Acc: 68.45, T.AUC: 0.6659 V.Loss: 0.0827, V.Acc: 65.29, V.AUC: 0.6396;\n",
      "[E 105/2000] T.Loss: 0.0758, T.Acc: 68.86, T.AUC: 0.6622 V.Loss: 0.0827, V.Acc: 65.29, V.AUC: 0.6380;\n",
      "[E 106/2000] T.Loss: 0.0757, T.Acc: 68.18, T.AUC: 0.6569 V.Loss: 0.0827, V.Acc: 65.29, V.AUC: 0.6393;\n",
      "[E 107/2000] T.Loss: 0.0757, T.Acc: 69.00, T.AUC: 0.6530 V.Loss: 0.0827, V.Acc: 65.29, V.AUC: 0.6393;\n",
      "[E 108/2000] T.Loss: 0.0757, T.Acc: 69.27, T.AUC: 0.6758 V.Loss: 0.0827, V.Acc: 65.29, V.AUC: 0.6390;\n",
      "[E 109/2000] T.Loss: 0.0756, T.Acc: 69.55, T.AUC: 0.6625 V.Loss: 0.0827, V.Acc: 65.56, V.AUC: 0.6398;\n",
      "[E 110/2000] T.Loss: 0.0756, T.Acc: 68.31, T.AUC: 0.6585 V.Loss: 0.0826, V.Acc: 65.84, V.AUC: 0.6396;\n",
      "[E 111/2000] T.Loss: 0.0755, T.Acc: 69.55, T.AUC: 0.6631 V.Loss: 0.0826, V.Acc: 65.56, V.AUC: 0.6389;\n",
      "[E 112/2000] T.Loss: 0.0754, T.Acc: 69.82, T.AUC: 0.6800 V.Loss: 0.0826, V.Acc: 65.56, V.AUC: 0.6390;\n",
      "[E 113/2000] T.Loss: 0.0754, T.Acc: 69.68, T.AUC: 0.6580 V.Loss: 0.0826, V.Acc: 65.29, V.AUC: 0.6379;\n",
      "[E 114/2000] T.Loss: 0.0753, T.Acc: 68.72, T.AUC: 0.6646 V.Loss: 0.0826, V.Acc: 65.29, V.AUC: 0.6389;\n",
      "[E 115/2000] T.Loss: 0.0753, T.Acc: 68.59, T.AUC: 0.6688 V.Loss: 0.0826, V.Acc: 65.01, V.AUC: 0.6394;\n",
      "[E 116/2000] T.Loss: 0.0753, T.Acc: 70.51, T.AUC: 0.6689 V.Loss: 0.0825, V.Acc: 65.84, V.AUC: 0.6405;\n",
      "[E 117/2000] T.Loss: 0.0752, T.Acc: 69.82, T.AUC: 0.6707 V.Loss: 0.0825, V.Acc: 65.84, V.AUC: 0.6394;\n",
      "[E 118/2000] T.Loss: 0.0752, T.Acc: 69.68, T.AUC: 0.6635 V.Loss: 0.0825, V.Acc: 66.39, V.AUC: 0.6393;\n",
      "[E 119/2000] T.Loss: 0.0752, T.Acc: 70.37, T.AUC: 0.6693 V.Loss: 0.0826, V.Acc: 66.39, V.AUC: 0.6398;\n",
      "[E 120/2000] T.Loss: 0.0751, T.Acc: 70.23, T.AUC: 0.6742 V.Loss: 0.0825, V.Acc: 66.39, V.AUC: 0.6394;\n",
      "[E 121/2000] T.Loss: 0.0750, T.Acc: 70.10, T.AUC: 0.6739 V.Loss: 0.0825, V.Acc: 66.94, V.AUC: 0.6390;\n",
      "[E 122/2000] T.Loss: 0.0750, T.Acc: 72.15, T.AUC: 0.6859 V.Loss: 0.0824, V.Acc: 66.67, V.AUC: 0.6402;\n",
      "[E 123/2000] T.Loss: 0.0749, T.Acc: 69.55, T.AUC: 0.6851 V.Loss: 0.0824, V.Acc: 66.67, V.AUC: 0.6401;\n",
      "[E 124/2000] T.Loss: 0.0748, T.Acc: 69.82, T.AUC: 0.6589 V.Loss: 0.0824, V.Acc: 66.12, V.AUC: 0.6411;\n",
      "[E 125/2000] T.Loss: 0.0748, T.Acc: 70.23, T.AUC: 0.6653 V.Loss: 0.0823, V.Acc: 66.12, V.AUC: 0.6425;\n",
      "[E 126/2000] T.Loss: 0.0748, T.Acc: 69.55, T.AUC: 0.6667 V.Loss: 0.0823, V.Acc: 66.94, V.AUC: 0.6431;\n",
      "[E 127/2000] T.Loss: 0.0747, T.Acc: 68.59, T.AUC: 0.6576 V.Loss: 0.0823, V.Acc: 66.67, V.AUC: 0.6430;\n",
      "[E 128/2000] T.Loss: 0.0747, T.Acc: 71.88, T.AUC: 0.6859 V.Loss: 0.0823, V.Acc: 67.22, V.AUC: 0.6427;\n",
      "[E 129/2000] T.Loss: 0.0746, T.Acc: 71.74, T.AUC: 0.6899 V.Loss: 0.0822, V.Acc: 68.04, V.AUC: 0.6424;\n",
      "[E 130/2000] T.Loss: 0.0746, T.Acc: 71.33, T.AUC: 0.6645 V.Loss: 0.0821, V.Acc: 68.04, V.AUC: 0.6438;\n",
      "[E 131/2000] T.Loss: 0.0745, T.Acc: 70.92, T.AUC: 0.6714 V.Loss: 0.0821, V.Acc: 68.32, V.AUC: 0.6463;\n",
      "[E 132/2000] T.Loss: 0.0745, T.Acc: 72.57, T.AUC: 0.6850 V.Loss: 0.0821, V.Acc: 67.49, V.AUC: 0.6479;\n",
      "[E 133/2000] T.Loss: 0.0745, T.Acc: 70.78, T.AUC: 0.6852 V.Loss: 0.0822, V.Acc: 67.77, V.AUC: 0.6444;\n",
      "[E 134/2000] T.Loss: 0.0744, T.Acc: 71.19, T.AUC: 0.6780 V.Loss: 0.0821, V.Acc: 68.32, V.AUC: 0.6440;\n",
      "[E 135/2000] T.Loss: 0.0744, T.Acc: 70.64, T.AUC: 0.6604 V.Loss: 0.0822, V.Acc: 68.60, V.AUC: 0.6457;\n",
      "[E 136/2000] T.Loss: 0.0743, T.Acc: 71.06, T.AUC: 0.6774 V.Loss: 0.0821, V.Acc: 68.87, V.AUC: 0.6452;\n",
      "[E 137/2000] T.Loss: 0.0743, T.Acc: 71.47, T.AUC: 0.6695 V.Loss: 0.0821, V.Acc: 68.60, V.AUC: 0.6460;\n",
      "[E 138/2000] T.Loss: 0.0742, T.Acc: 71.74, T.AUC: 0.6768 V.Loss: 0.0821, V.Acc: 69.15, V.AUC: 0.6441;\n",
      "[E 139/2000] T.Loss: 0.0742, T.Acc: 72.29, T.AUC: 0.6798 V.Loss: 0.0821, V.Acc: 68.87, V.AUC: 0.6443;\n",
      "[E 140/2000] T.Loss: 0.0741, T.Acc: 71.47, T.AUC: 0.6881 V.Loss: 0.0821, V.Acc: 68.32, V.AUC: 0.6440;\n",
      "[E 141/2000] T.Loss: 0.0741, T.Acc: 72.15, T.AUC: 0.6704 V.Loss: 0.0821, V.Acc: 68.04, V.AUC: 0.6423;\n",
      "[E 142/2000] T.Loss: 0.0741, T.Acc: 72.98, T.AUC: 0.6802 V.Loss: 0.0821, V.Acc: 68.32, V.AUC: 0.6431;\n",
      "[E 143/2000] T.Loss: 0.0740, T.Acc: 72.57, T.AUC: 0.6758 V.Loss: 0.0821, V.Acc: 68.04, V.AUC: 0.6433;\n",
      "[E 144/2000] T.Loss: 0.0739, T.Acc: 72.29, T.AUC: 0.6943 V.Loss: 0.0819, V.Acc: 66.12, V.AUC: 0.6427;\n",
      "[E 145/2000] T.Loss: 0.0739, T.Acc: 72.84, T.AUC: 0.6746 V.Loss: 0.0819, V.Acc: 65.56, V.AUC: 0.6463;\n",
      "[E 146/2000] T.Loss: 0.0738, T.Acc: 72.98, T.AUC: 0.6846 V.Loss: 0.0818, V.Acc: 66.39, V.AUC: 0.6484;\n",
      "[E 147/2000] T.Loss: 0.0738, T.Acc: 73.94, T.AUC: 0.6892 V.Loss: 0.0818, V.Acc: 66.12, V.AUC: 0.6496;\n",
      "[E 148/2000] T.Loss: 0.0737, T.Acc: 72.98, T.AUC: 0.6772 V.Loss: 0.0818, V.Acc: 68.04, V.AUC: 0.6492;\n",
      "[E 149/2000] T.Loss: 0.0737, T.Acc: 72.29, T.AUC: 0.6806 V.Loss: 0.0819, V.Acc: 67.77, V.AUC: 0.6503;\n",
      "[E 150/2000] T.Loss: 0.0737, T.Acc: 72.43, T.AUC: 0.6813 V.Loss: 0.0819, V.Acc: 68.32, V.AUC: 0.6498;\n",
      "[E 151/2000] T.Loss: 0.0736, T.Acc: 72.70, T.AUC: 0.6746 V.Loss: 0.0819, V.Acc: 68.32, V.AUC: 0.6498;\n",
      "[E 152/2000] T.Loss: 0.0735, T.Acc: 71.60, T.AUC: 0.6865 V.Loss: 0.0818, V.Acc: 66.94, V.AUC: 0.6493;\n",
      "[E 153/2000] T.Loss: 0.0735, T.Acc: 72.15, T.AUC: 0.6757 V.Loss: 0.0818, V.Acc: 66.67, V.AUC: 0.6497;\n",
      "[E 154/2000] T.Loss: 0.0735, T.Acc: 71.74, T.AUC: 0.6846 V.Loss: 0.0819, V.Acc: 67.49, V.AUC: 0.6514;\n",
      "[E 155/2000] T.Loss: 0.0734, T.Acc: 71.19, T.AUC: 0.6923 V.Loss: 0.0818, V.Acc: 67.49, V.AUC: 0.6525;\n",
      "[E 156/2000] T.Loss: 0.0734, T.Acc: 70.92, T.AUC: 0.6748 V.Loss: 0.0817, V.Acc: 67.77, V.AUC: 0.6519;\n",
      "[E 157/2000] T.Loss: 0.0734, T.Acc: 72.43, T.AUC: 0.6847 V.Loss: 0.0818, V.Acc: 67.49, V.AUC: 0.6518;\n",
      "[E 158/2000] T.Loss: 0.0733, T.Acc: 71.88, T.AUC: 0.6845 V.Loss: 0.0818, V.Acc: 66.39, V.AUC: 0.6493;\n",
      "[E 159/2000] T.Loss: 0.0732, T.Acc: 72.84, T.AUC: 0.6762 V.Loss: 0.0818, V.Acc: 66.94, V.AUC: 0.6475;\n",
      "[E 160/2000] T.Loss: 0.0732, T.Acc: 72.15, T.AUC: 0.6797 V.Loss: 0.0818, V.Acc: 66.67, V.AUC: 0.6480;\n",
      "[E 161/2000] T.Loss: 0.0731, T.Acc: 72.02, T.AUC: 0.6773 V.Loss: 0.0818, V.Acc: 67.22, V.AUC: 0.6481;\n",
      "[E 162/2000] T.Loss: 0.0731, T.Acc: 72.29, T.AUC: 0.6860 V.Loss: 0.0817, V.Acc: 67.49, V.AUC: 0.6508;\n",
      "[E 163/2000] T.Loss: 0.0730, T.Acc: 72.84, T.AUC: 0.6878 V.Loss: 0.0816, V.Acc: 67.49, V.AUC: 0.6526;\n",
      "[E 164/2000] T.Loss: 0.0730, T.Acc: 72.57, T.AUC: 0.6926 V.Loss: 0.0816, V.Acc: 67.22, V.AUC: 0.6517;\n",
      "[E 165/2000] T.Loss: 0.0729, T.Acc: 73.39, T.AUC: 0.6867 V.Loss: 0.0816, V.Acc: 67.22, V.AUC: 0.6519;\n",
      "[E 166/2000] T.Loss: 0.0729, T.Acc: 72.15, T.AUC: 0.6853 V.Loss: 0.0816, V.Acc: 67.22, V.AUC: 0.6533;\n",
      "[E 167/2000] T.Loss: 0.0729, T.Acc: 72.98, T.AUC: 0.6854 V.Loss: 0.0816, V.Acc: 66.94, V.AUC: 0.6526;\n",
      "[E 168/2000] T.Loss: 0.0728, T.Acc: 72.29, T.AUC: 0.6730 V.Loss: 0.0816, V.Acc: 67.22, V.AUC: 0.6530;\n",
      "[E 169/2000] T.Loss: 0.0728, T.Acc: 72.57, T.AUC: 0.6934 V.Loss: 0.0816, V.Acc: 66.94, V.AUC: 0.6536;\n",
      "[E 170/2000] T.Loss: 0.0728, T.Acc: 73.11, T.AUC: 0.6738 V.Loss: 0.0816, V.Acc: 66.94, V.AUC: 0.6536;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 171/2000] T.Loss: 0.0728, T.Acc: 72.84, T.AUC: 0.6788 V.Loss: 0.0815, V.Acc: 66.94, V.AUC: 0.6554;\n",
      "[E 172/2000] T.Loss: 0.0727, T.Acc: 73.66, T.AUC: 0.6755 V.Loss: 0.0814, V.Acc: 66.94, V.AUC: 0.6565;\n",
      "[E 173/2000] T.Loss: 0.0727, T.Acc: 72.15, T.AUC: 0.6788 V.Loss: 0.0814, V.Acc: 66.67, V.AUC: 0.6571;\n",
      "[E 174/2000] T.Loss: 0.0727, T.Acc: 72.43, T.AUC: 0.6707 V.Loss: 0.0814, V.Acc: 66.94, V.AUC: 0.6585;\n",
      "[E 175/2000] T.Loss: 0.0727, T.Acc: 73.94, T.AUC: 0.6782 V.Loss: 0.0814, V.Acc: 66.67, V.AUC: 0.6614;\n",
      "[E 176/2000] T.Loss: 0.0726, T.Acc: 73.66, T.AUC: 0.6975 V.Loss: 0.0815, V.Acc: 66.94, V.AUC: 0.6582;\n",
      "[E 177/2000] T.Loss: 0.0726, T.Acc: 73.94, T.AUC: 0.6751 V.Loss: 0.0815, V.Acc: 66.67, V.AUC: 0.6616;\n",
      "[E 178/2000] T.Loss: 0.0726, T.Acc: 71.88, T.AUC: 0.6699 V.Loss: 0.0814, V.Acc: 66.67, V.AUC: 0.6614;\n",
      "[E 179/2000] T.Loss: 0.0725, T.Acc: 73.94, T.AUC: 0.6963 V.Loss: 0.0815, V.Acc: 66.94, V.AUC: 0.6587;\n",
      "[E 180/2000] T.Loss: 0.0725, T.Acc: 73.66, T.AUC: 0.6945 V.Loss: 0.0815, V.Acc: 66.67, V.AUC: 0.6598;\n",
      "[E 181/2000] T.Loss: 0.0724, T.Acc: 73.94, T.AUC: 0.6909 V.Loss: 0.0815, V.Acc: 66.94, V.AUC: 0.6583;\n",
      "[E 182/2000] T.Loss: 0.0723, T.Acc: 73.66, T.AUC: 0.6714 V.Loss: 0.0813, V.Acc: 67.22, V.AUC: 0.6609;\n",
      "[E 183/2000] T.Loss: 0.0723, T.Acc: 74.62, T.AUC: 0.6838 V.Loss: 0.0813, V.Acc: 66.94, V.AUC: 0.6628;\n",
      "[E 184/2000] T.Loss: 0.0723, T.Acc: 73.25, T.AUC: 0.6852 V.Loss: 0.0814, V.Acc: 66.94, V.AUC: 0.6627;\n",
      "[E 185/2000] T.Loss: 0.0723, T.Acc: 73.39, T.AUC: 0.6823 V.Loss: 0.0813, V.Acc: 66.67, V.AUC: 0.6625;\n",
      "[E 186/2000] T.Loss: 0.0722, T.Acc: 74.90, T.AUC: 0.6945 V.Loss: 0.0813, V.Acc: 66.67, V.AUC: 0.6634;\n",
      "[E 187/2000] T.Loss: 0.0721, T.Acc: 75.31, T.AUC: 0.6751 V.Loss: 0.0814, V.Acc: 66.94, V.AUC: 0.6639;\n",
      "[E 188/2000] T.Loss: 0.0721, T.Acc: 73.39, T.AUC: 0.6792 V.Loss: 0.0813, V.Acc: 66.94, V.AUC: 0.6651;\n",
      "[E 189/2000] T.Loss: 0.0721, T.Acc: 74.35, T.AUC: 0.6783 V.Loss: 0.0813, V.Acc: 66.67, V.AUC: 0.6646;\n",
      "[E 190/2000] T.Loss: 0.0720, T.Acc: 70.92, T.AUC: 0.6946 V.Loss: 0.0813, V.Acc: 66.39, V.AUC: 0.6644;\n",
      "[E 191/2000] T.Loss: 0.0720, T.Acc: 73.80, T.AUC: 0.6877 V.Loss: 0.0812, V.Acc: 66.94, V.AUC: 0.6661;\n",
      "[E 192/2000] T.Loss: 0.0720, T.Acc: 74.21, T.AUC: 0.6926 V.Loss: 0.0813, V.Acc: 66.94, V.AUC: 0.6673;\n",
      "[E 193/2000] T.Loss: 0.0720, T.Acc: 73.25, T.AUC: 0.6796 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6668;\n",
      "[E 194/2000] T.Loss: 0.0719, T.Acc: 73.11, T.AUC: 0.6836 V.Loss: 0.0812, V.Acc: 66.94, V.AUC: 0.6683;\n",
      "[E 195/2000] T.Loss: 0.0719, T.Acc: 73.94, T.AUC: 0.6796 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6678;\n",
      "[E 196/2000] T.Loss: 0.0718, T.Acc: 73.11, T.AUC: 0.6845 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6670;\n",
      "[E 197/2000] T.Loss: 0.0718, T.Acc: 74.07, T.AUC: 0.6979 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6671;\n",
      "[E 198/2000] T.Loss: 0.0717, T.Acc: 74.62, T.AUC: 0.6801 V.Loss: 0.0813, V.Acc: 66.39, V.AUC: 0.6667;\n",
      "[E 199/2000] T.Loss: 0.0716, T.Acc: 73.66, T.AUC: 0.6831 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6664;\n",
      "[E 200/2000] T.Loss: 0.0716, T.Acc: 73.39, T.AUC: 0.6945 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6675;\n",
      "[E 201/2000] T.Loss: 0.0716, T.Acc: 74.07, T.AUC: 0.6929 V.Loss: 0.0811, V.Acc: 66.94, V.AUC: 0.6667;\n",
      "[E 202/2000] T.Loss: 0.0716, T.Acc: 74.07, T.AUC: 0.6772 V.Loss: 0.0810, V.Acc: 66.94, V.AUC: 0.6666;\n",
      "[E 203/2000] T.Loss: 0.0715, T.Acc: 74.35, T.AUC: 0.7006 V.Loss: 0.0811, V.Acc: 65.84, V.AUC: 0.6673;\n",
      "[E 204/2000] T.Loss: 0.0715, T.Acc: 73.53, T.AUC: 0.6807 V.Loss: 0.0811, V.Acc: 66.12, V.AUC: 0.6676;\n",
      "[E 205/2000] T.Loss: 0.0714, T.Acc: 74.35, T.AUC: 0.6913 V.Loss: 0.0811, V.Acc: 66.94, V.AUC: 0.6670;\n",
      "[E 206/2000] T.Loss: 0.0714, T.Acc: 73.39, T.AUC: 0.6867 V.Loss: 0.0810, V.Acc: 66.67, V.AUC: 0.6675;\n",
      "[E 207/2000] T.Loss: 0.0714, T.Acc: 72.70, T.AUC: 0.6941 V.Loss: 0.0810, V.Acc: 66.39, V.AUC: 0.6687;\n",
      "[E 208/2000] T.Loss: 0.0714, T.Acc: 73.66, T.AUC: 0.6863 V.Loss: 0.0809, V.Acc: 66.39, V.AUC: 0.6683;\n",
      "[E 209/2000] T.Loss: 0.0713, T.Acc: 73.53, T.AUC: 0.6819 V.Loss: 0.0810, V.Acc: 66.39, V.AUC: 0.6677;\n",
      "[E 210/2000] T.Loss: 0.0713, T.Acc: 73.66, T.AUC: 0.6952 V.Loss: 0.0810, V.Acc: 66.12, V.AUC: 0.6667;\n",
      "[E 211/2000] T.Loss: 0.0713, T.Acc: 74.21, T.AUC: 0.6867 V.Loss: 0.0811, V.Acc: 65.56, V.AUC: 0.6662;\n",
      "[E 212/2000] T.Loss: 0.0712, T.Acc: 74.90, T.AUC: 0.6971 V.Loss: 0.0811, V.Acc: 65.84, V.AUC: 0.6655;\n",
      "[E 213/2000] T.Loss: 0.0712, T.Acc: 72.98, T.AUC: 0.6805 V.Loss: 0.0811, V.Acc: 66.12, V.AUC: 0.6653;\n",
      "[E 214/2000] T.Loss: 0.0711, T.Acc: 73.80, T.AUC: 0.6759 V.Loss: 0.0810, V.Acc: 66.39, V.AUC: 0.6663;\n",
      "[E 215/2000] T.Loss: 0.0712, T.Acc: 73.80, T.AUC: 0.6904 V.Loss: 0.0810, V.Acc: 65.56, V.AUC: 0.6652;\n",
      "[E 216/2000] T.Loss: 0.0711, T.Acc: 72.15, T.AUC: 0.6973 V.Loss: 0.0809, V.Acc: 65.84, V.AUC: 0.6640;\n",
      "[E 217/2000] T.Loss: 0.0711, T.Acc: 74.07, T.AUC: 0.6922 V.Loss: 0.0808, V.Acc: 66.39, V.AUC: 0.6643;\n",
      "[E 218/2000] T.Loss: 0.0710, T.Acc: 74.21, T.AUC: 0.6866 V.Loss: 0.0808, V.Acc: 66.12, V.AUC: 0.6640;\n",
      "[E 219/2000] T.Loss: 0.0711, T.Acc: 74.49, T.AUC: 0.6929 V.Loss: 0.0809, V.Acc: 65.84, V.AUC: 0.6634;\n",
      "[E 220/2000] T.Loss: 0.0709, T.Acc: 74.35, T.AUC: 0.7028 V.Loss: 0.0808, V.Acc: 66.39, V.AUC: 0.6645;\n",
      "[E 221/2000] T.Loss: 0.0709, T.Acc: 74.76, T.AUC: 0.6845 V.Loss: 0.0808, V.Acc: 66.67, V.AUC: 0.6666;\n",
      "[E 222/2000] T.Loss: 0.0708, T.Acc: 74.21, T.AUC: 0.7076 V.Loss: 0.0807, V.Acc: 66.67, V.AUC: 0.6685;\n",
      "[E 223/2000] T.Loss: 0.0708, T.Acc: 73.80, T.AUC: 0.7078 V.Loss: 0.0808, V.Acc: 66.94, V.AUC: 0.6660;\n",
      "[E 224/2000] T.Loss: 0.0707, T.Acc: 73.80, T.AUC: 0.6886 V.Loss: 0.0809, V.Acc: 65.84, V.AUC: 0.6619;\n",
      "[E 225/2000] T.Loss: 0.0708, T.Acc: 74.49, T.AUC: 0.6961 V.Loss: 0.0809, V.Acc: 66.12, V.AUC: 0.6627;\n",
      "[E 226/2000] T.Loss: 0.0707, T.Acc: 74.90, T.AUC: 0.7089 V.Loss: 0.0809, V.Acc: 66.39, V.AUC: 0.6619;\n",
      "[E 227/2000] T.Loss: 0.0706, T.Acc: 74.62, T.AUC: 0.6930 V.Loss: 0.0808, V.Acc: 66.67, V.AUC: 0.6634;\n",
      "[E 228/2000] T.Loss: 0.0706, T.Acc: 73.25, T.AUC: 0.6809 V.Loss: 0.0809, V.Acc: 65.84, V.AUC: 0.6606;\n",
      "[E 229/2000] T.Loss: 0.0706, T.Acc: 73.66, T.AUC: 0.6970 V.Loss: 0.0810, V.Acc: 65.56, V.AUC: 0.6588;\n",
      "[E 230/2000] T.Loss: 0.0705, T.Acc: 74.90, T.AUC: 0.6935 V.Loss: 0.0809, V.Acc: 66.39, V.AUC: 0.6598;\n",
      "[E 231/2000] T.Loss: 0.0705, T.Acc: 75.03, T.AUC: 0.6929 V.Loss: 0.0808, V.Acc: 66.12, V.AUC: 0.6604;\n",
      "[E 232/2000] T.Loss: 0.0704, T.Acc: 73.80, T.AUC: 0.6882 V.Loss: 0.0808, V.Acc: 66.39, V.AUC: 0.6600;\n",
      "[E 233/2000] T.Loss: 0.0705, T.Acc: 73.25, T.AUC: 0.6934 V.Loss: 0.0809, V.Acc: 65.84, V.AUC: 0.6587;\n",
      "[E 234/2000] T.Loss: 0.0704, T.Acc: 75.17, T.AUC: 0.6909 V.Loss: 0.0808, V.Acc: 66.94, V.AUC: 0.6596;\n",
      "[E 235/2000] T.Loss: 0.0704, T.Acc: 73.53, T.AUC: 0.7022 V.Loss: 0.0808, V.Acc: 66.12, V.AUC: 0.6602;\n",
      "[E 236/2000] T.Loss: 0.0703, T.Acc: 74.62, T.AUC: 0.6964 V.Loss: 0.0808, V.Acc: 65.84, V.AUC: 0.6607;\n",
      "[E 237/2000] T.Loss: 0.0703, T.Acc: 74.35, T.AUC: 0.7049 V.Loss: 0.0808, V.Acc: 66.94, V.AUC: 0.6609;\n",
      "[E 238/2000] T.Loss: 0.0703, T.Acc: 74.76, T.AUC: 0.6986 V.Loss: 0.0809, V.Acc: 66.12, V.AUC: 0.6601;\n",
      "[E 239/2000] T.Loss: 0.0702, T.Acc: 73.39, T.AUC: 0.6887 V.Loss: 0.0808, V.Acc: 66.39, V.AUC: 0.6606;\n",
      "[E 240/2000] T.Loss: 0.0703, T.Acc: 74.76, T.AUC: 0.7024 V.Loss: 0.0807, V.Acc: 66.12, V.AUC: 0.6626;\n",
      "[E 241/2000] T.Loss: 0.0702, T.Acc: 73.66, T.AUC: 0.6978 V.Loss: 0.0809, V.Acc: 66.39, V.AUC: 0.6603;\n",
      "[E 242/2000] T.Loss: 0.0701, T.Acc: 73.11, T.AUC: 0.6939 V.Loss: 0.0807, V.Acc: 65.84, V.AUC: 0.6600;\n",
      "[E 243/2000] T.Loss: 0.0701, T.Acc: 74.62, T.AUC: 0.7089 V.Loss: 0.0807, V.Acc: 66.12, V.AUC: 0.6608;\n",
      "[E 244/2000] T.Loss: 0.0700, T.Acc: 74.07, T.AUC: 0.6913 V.Loss: 0.0807, V.Acc: 66.94, V.AUC: 0.6606;\n",
      "[E 245/2000] T.Loss: 0.0700, T.Acc: 75.45, T.AUC: 0.6918 V.Loss: 0.0807, V.Acc: 66.94, V.AUC: 0.6601;\n",
      "[E 246/2000] T.Loss: 0.0699, T.Acc: 73.11, T.AUC: 0.6857 V.Loss: 0.0808, V.Acc: 66.12, V.AUC: 0.6576;\n",
      "[E 247/2000] T.Loss: 0.0699, T.Acc: 74.35, T.AUC: 0.7147 V.Loss: 0.0809, V.Acc: 66.39, V.AUC: 0.6579;\n",
      "[E 248/2000] T.Loss: 0.0698, T.Acc: 75.72, T.AUC: 0.7059 V.Loss: 0.0809, V.Acc: 65.84, V.AUC: 0.6568;\n",
      "[E 249/2000] T.Loss: 0.0698, T.Acc: 74.35, T.AUC: 0.7052 V.Loss: 0.0809, V.Acc: 65.56, V.AUC: 0.6567;\n",
      "[E 250/2000] T.Loss: 0.0698, T.Acc: 73.94, T.AUC: 0.7051 V.Loss: 0.0809, V.Acc: 66.12, V.AUC: 0.6585;\n",
      "[E 251/2000] T.Loss: 0.0698, T.Acc: 73.80, T.AUC: 0.7010 V.Loss: 0.0809, V.Acc: 66.39, V.AUC: 0.6573;\n",
      "[E 252/2000] T.Loss: 0.0697, T.Acc: 73.80, T.AUC: 0.6857 V.Loss: 0.0808, V.Acc: 66.12, V.AUC: 0.6572;\n",
      "[E 253/2000] T.Loss: 0.0697, T.Acc: 74.76, T.AUC: 0.6908 V.Loss: 0.0808, V.Acc: 66.12, V.AUC: 0.6576;\n",
      "[E 254/2000] T.Loss: 0.0697, T.Acc: 75.31, T.AUC: 0.7044 V.Loss: 0.0809, V.Acc: 66.94, V.AUC: 0.6562;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 255/2000] T.Loss: 0.0696, T.Acc: 73.66, T.AUC: 0.7062 V.Loss: 0.0809, V.Acc: 66.12, V.AUC: 0.6555;\n",
      "[E 256/2000] T.Loss: 0.0696, T.Acc: 74.07, T.AUC: 0.6953 V.Loss: 0.0809, V.Acc: 65.84, V.AUC: 0.6551;\n",
      "[E 257/2000] T.Loss: 0.0695, T.Acc: 74.49, T.AUC: 0.6995 V.Loss: 0.0810, V.Acc: 66.12, V.AUC: 0.6536;\n",
      "[E 258/2000] T.Loss: 0.0695, T.Acc: 74.90, T.AUC: 0.7159 V.Loss: 0.0810, V.Acc: 65.84, V.AUC: 0.6518;\n",
      "[E 259/2000] T.Loss: 0.0695, T.Acc: 74.49, T.AUC: 0.7193 V.Loss: 0.0810, V.Acc: 65.84, V.AUC: 0.6537;\n",
      "[E 260/2000] T.Loss: 0.0695, T.Acc: 74.49, T.AUC: 0.7051 V.Loss: 0.0811, V.Acc: 65.84, V.AUC: 0.6544;\n",
      "[E 261/2000] T.Loss: 0.0694, T.Acc: 74.35, T.AUC: 0.6955 V.Loss: 0.0810, V.Acc: 65.56, V.AUC: 0.6529;\n",
      "[E 262/2000] T.Loss: 0.0694, T.Acc: 73.53, T.AUC: 0.6903 V.Loss: 0.0811, V.Acc: 65.84, V.AUC: 0.6514;\n",
      "[E 263/2000] T.Loss: 0.0694, T.Acc: 74.90, T.AUC: 0.6936 V.Loss: 0.0811, V.Acc: 65.84, V.AUC: 0.6515;\n",
      "[E 264/2000] T.Loss: 0.0694, T.Acc: 76.13, T.AUC: 0.7139 V.Loss: 0.0811, V.Acc: 66.39, V.AUC: 0.6521;\n",
      "[E 265/2000] T.Loss: 0.0693, T.Acc: 75.45, T.AUC: 0.7016 V.Loss: 0.0810, V.Acc: 66.12, V.AUC: 0.6505;\n",
      "[E 266/2000] T.Loss: 0.0692, T.Acc: 74.90, T.AUC: 0.7139 V.Loss: 0.0811, V.Acc: 65.84, V.AUC: 0.6495;\n",
      "[E 267/2000] T.Loss: 0.0692, T.Acc: 74.49, T.AUC: 0.6914 V.Loss: 0.0810, V.Acc: 65.84, V.AUC: 0.6502;\n",
      "[E 268/2000] T.Loss: 0.0692, T.Acc: 74.07, T.AUC: 0.7081 V.Loss: 0.0811, V.Acc: 66.39, V.AUC: 0.6498;\n",
      "[E 269/2000] T.Loss: 0.0692, T.Acc: 73.66, T.AUC: 0.7191 V.Loss: 0.0811, V.Acc: 65.84, V.AUC: 0.6499;\n",
      "[E 270/2000] T.Loss: 0.0692, T.Acc: 74.62, T.AUC: 0.6981 V.Loss: 0.0811, V.Acc: 66.12, V.AUC: 0.6502;\n",
      "[E 271/2000] T.Loss: 0.0691, T.Acc: 74.62, T.AUC: 0.7018 V.Loss: 0.0811, V.Acc: 66.12, V.AUC: 0.6489;\n",
      "[E 272/2000] T.Loss: 0.0692, T.Acc: 75.31, T.AUC: 0.7068 V.Loss: 0.0811, V.Acc: 66.39, V.AUC: 0.6484;\n",
      "[E 273/2000] T.Loss: 0.0691, T.Acc: 75.99, T.AUC: 0.6891 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6457;\n",
      "[E 274/2000] T.Loss: 0.0691, T.Acc: 74.07, T.AUC: 0.6957 V.Loss: 0.0812, V.Acc: 66.12, V.AUC: 0.6434;\n",
      "[E 275/2000] T.Loss: 0.0691, T.Acc: 74.90, T.AUC: 0.6926 V.Loss: 0.0813, V.Acc: 66.39, V.AUC: 0.6429;\n",
      "[E 276/2000] T.Loss: 0.0690, T.Acc: 74.90, T.AUC: 0.7025 V.Loss: 0.0813, V.Acc: 66.39, V.AUC: 0.6443;\n",
      "[E 277/2000] T.Loss: 0.0691, T.Acc: 74.07, T.AUC: 0.7056 V.Loss: 0.0814, V.Acc: 66.39, V.AUC: 0.6450;\n",
      "[E 278/2000] T.Loss: 0.0690, T.Acc: 75.58, T.AUC: 0.7105 V.Loss: 0.0813, V.Acc: 65.84, V.AUC: 0.6457;\n",
      "[E 279/2000] T.Loss: 0.0690, T.Acc: 74.21, T.AUC: 0.7082 V.Loss: 0.0812, V.Acc: 65.56, V.AUC: 0.6458;\n",
      "[E 280/2000] T.Loss: 0.0688, T.Acc: 73.11, T.AUC: 0.7008 V.Loss: 0.0812, V.Acc: 65.84, V.AUC: 0.6437;\n",
      "[E 281/2000] T.Loss: 0.0690, T.Acc: 72.84, T.AUC: 0.6875 V.Loss: 0.0813, V.Acc: 65.84, V.AUC: 0.6431;\n",
      "[E 282/2000] T.Loss: 0.0689, T.Acc: 75.17, T.AUC: 0.6907 V.Loss: 0.0814, V.Acc: 65.84, V.AUC: 0.6419;\n",
      "[E 283/2000] T.Loss: 0.0688, T.Acc: 75.72, T.AUC: 0.7243 V.Loss: 0.0812, V.Acc: 65.84, V.AUC: 0.6419;\n",
      "[E 284/2000] T.Loss: 0.0688, T.Acc: 73.94, T.AUC: 0.7121 V.Loss: 0.0812, V.Acc: 65.84, V.AUC: 0.6416;\n",
      "[E 285/2000] T.Loss: 0.0688, T.Acc: 74.21, T.AUC: 0.6942 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6420;\n",
      "[E 286/2000] T.Loss: 0.0689, T.Acc: 75.03, T.AUC: 0.7012 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6429;\n",
      "[E 287/2000] T.Loss: 0.0687, T.Acc: 74.07, T.AUC: 0.6893 V.Loss: 0.0812, V.Acc: 66.12, V.AUC: 0.6419;\n",
      "[E 288/2000] T.Loss: 0.0687, T.Acc: 75.86, T.AUC: 0.7083 V.Loss: 0.0813, V.Acc: 66.12, V.AUC: 0.6412;\n",
      "[E 289/2000] T.Loss: 0.0686, T.Acc: 75.03, T.AUC: 0.6919 V.Loss: 0.0813, V.Acc: 66.12, V.AUC: 0.6394;\n",
      "[E 290/2000] T.Loss: 0.0686, T.Acc: 75.72, T.AUC: 0.7036 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6422;\n",
      "[E 291/2000] T.Loss: 0.0685, T.Acc: 74.07, T.AUC: 0.6952 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6417;\n",
      "[E 292/2000] T.Loss: 0.0685, T.Acc: 74.35, T.AUC: 0.7164 V.Loss: 0.0813, V.Acc: 66.12, V.AUC: 0.6394;\n",
      "[E 293/2000] T.Loss: 0.0685, T.Acc: 74.35, T.AUC: 0.7023 V.Loss: 0.0813, V.Acc: 66.39, V.AUC: 0.6391;\n",
      "[E 294/2000] T.Loss: 0.0685, T.Acc: 75.58, T.AUC: 0.7085 V.Loss: 0.0813, V.Acc: 66.12, V.AUC: 0.6392;\n",
      "[E 295/2000] T.Loss: 0.0686, T.Acc: 74.49, T.AUC: 0.7075 V.Loss: 0.0813, V.Acc: 66.67, V.AUC: 0.6405;\n",
      "[E 296/2000] T.Loss: 0.0685, T.Acc: 74.07, T.AUC: 0.7040 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6429;\n",
      "[E 297/2000] T.Loss: 0.0683, T.Acc: 74.21, T.AUC: 0.7288 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6414;\n",
      "[E 298/2000] T.Loss: 0.0683, T.Acc: 75.17, T.AUC: 0.6948 V.Loss: 0.0813, V.Acc: 66.39, V.AUC: 0.6394;\n",
      "[E 299/2000] T.Loss: 0.0683, T.Acc: 73.94, T.AUC: 0.7204 V.Loss: 0.0813, V.Acc: 66.94, V.AUC: 0.6391;\n",
      "[E 300/2000] T.Loss: 0.0682, T.Acc: 75.17, T.AUC: 0.7030 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6372;\n",
      "[E 301/2000] T.Loss: 0.0682, T.Acc: 74.90, T.AUC: 0.7155 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6384;\n",
      "[E 302/2000] T.Loss: 0.0682, T.Acc: 75.72, T.AUC: 0.7156 V.Loss: 0.0813, V.Acc: 66.39, V.AUC: 0.6397;\n",
      "[E 303/2000] T.Loss: 0.0681, T.Acc: 75.72, T.AUC: 0.7117 V.Loss: 0.0812, V.Acc: 66.94, V.AUC: 0.6427;\n",
      "[E 304/2000] T.Loss: 0.0682, T.Acc: 74.07, T.AUC: 0.6913 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6412;\n",
      "[E 305/2000] T.Loss: 0.0681, T.Acc: 75.99, T.AUC: 0.7164 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6399;\n",
      "[E 306/2000] T.Loss: 0.0681, T.Acc: 74.90, T.AUC: 0.7201 V.Loss: 0.0811, V.Acc: 66.67, V.AUC: 0.6387;\n",
      "[E 307/2000] T.Loss: 0.0681, T.Acc: 74.35, T.AUC: 0.7071 V.Loss: 0.0813, V.Acc: 66.39, V.AUC: 0.6384;\n",
      "[E 308/2000] T.Loss: 0.0681, T.Acc: 74.35, T.AUC: 0.7024 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6366;\n",
      "[E 309/2000] T.Loss: 0.0680, T.Acc: 75.58, T.AUC: 0.7091 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6378;\n",
      "[E 310/2000] T.Loss: 0.0680, T.Acc: 74.35, T.AUC: 0.7139 V.Loss: 0.0813, V.Acc: 66.94, V.AUC: 0.6385;\n",
      "[E 311/2000] T.Loss: 0.0680, T.Acc: 74.90, T.AUC: 0.7094 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6362;\n",
      "[E 312/2000] T.Loss: 0.0679, T.Acc: 76.13, T.AUC: 0.6899 V.Loss: 0.0813, V.Acc: 66.39, V.AUC: 0.6356;\n",
      "[E 313/2000] T.Loss: 0.0679, T.Acc: 74.07, T.AUC: 0.7129 V.Loss: 0.0812, V.Acc: 66.67, V.AUC: 0.6363;\n",
      "[E 314/2000] T.Loss: 0.0678, T.Acc: 74.35, T.AUC: 0.7083 V.Loss: 0.0812, V.Acc: 66.39, V.AUC: 0.6372;\n",
      "[E 315/2000] T.Loss: 0.0678, T.Acc: 75.03, T.AUC: 0.7172 V.Loss: 0.0811, V.Acc: 66.67, V.AUC: 0.6370;\n",
      "[E 316/2000] T.Loss: 0.0678, T.Acc: 75.45, T.AUC: 0.7126 V.Loss: 0.0811, V.Acc: 67.22, V.AUC: 0.6377;\n",
      "[E 317/2000] T.Loss: 0.0678, T.Acc: 74.90, T.AUC: 0.7113 V.Loss: 0.0811, V.Acc: 66.67, V.AUC: 0.6357;\n",
      "[E 318/2000] T.Loss: 0.0677, T.Acc: 75.31, T.AUC: 0.7375 V.Loss: 0.0811, V.Acc: 66.67, V.AUC: 0.6367;\n",
      "[E 319/2000] T.Loss: 0.0677, T.Acc: 74.90, T.AUC: 0.7298 V.Loss: 0.0810, V.Acc: 66.94, V.AUC: 0.6383;\n",
      "[E 320/2000] T.Loss: 0.0676, T.Acc: 75.58, T.AUC: 0.7244 V.Loss: 0.0810, V.Acc: 66.94, V.AUC: 0.6380;\n",
      "[E 321/2000] T.Loss: 0.0676, T.Acc: 75.86, T.AUC: 0.7288 V.Loss: 0.0811, V.Acc: 66.94, V.AUC: 0.6375;\n",
      "[E 322/2000] T.Loss: 0.0675, T.Acc: 75.31, T.AUC: 0.7149 V.Loss: 0.0811, V.Acc: 67.77, V.AUC: 0.6356;\n",
      "[E 323/2000] T.Loss: 0.0675, T.Acc: 74.90, T.AUC: 0.7149 V.Loss: 0.0812, V.Acc: 66.94, V.AUC: 0.6357;\n",
      "[E 324/2000] T.Loss: 0.0675, T.Acc: 75.99, T.AUC: 0.6991 V.Loss: 0.0813, V.Acc: 66.67, V.AUC: 0.6369;\n",
      "[E 325/2000] T.Loss: 0.0675, T.Acc: 75.17, T.AUC: 0.7308 V.Loss: 0.0813, V.Acc: 66.67, V.AUC: 0.6359;\n",
      "[E 326/2000] T.Loss: 0.0675, T.Acc: 75.45, T.AUC: 0.7144 V.Loss: 0.0814, V.Acc: 66.67, V.AUC: 0.6341;\n",
      "[E 327/2000] T.Loss: 0.0675, T.Acc: 76.41, T.AUC: 0.7289 V.Loss: 0.0816, V.Acc: 66.94, V.AUC: 0.6336;\n",
      "[E 328/2000] T.Loss: 0.0675, T.Acc: 75.45, T.AUC: 0.7243 V.Loss: 0.0813, V.Acc: 67.49, V.AUC: 0.6358;\n",
      "[E 329/2000] T.Loss: 0.0674, T.Acc: 74.35, T.AUC: 0.7170 V.Loss: 0.0815, V.Acc: 66.94, V.AUC: 0.6338;\n",
      "[E 330/2000] T.Loss: 0.0675, T.Acc: 73.39, T.AUC: 0.7155 V.Loss: 0.0816, V.Acc: 66.67, V.AUC: 0.6297;\n",
      "[E 331/2000] T.Loss: 0.0674, T.Acc: 74.90, T.AUC: 0.7135 V.Loss: 0.0817, V.Acc: 66.94, V.AUC: 0.6298;\n",
      "[E 332/2000] T.Loss: 0.0673, T.Acc: 74.90, T.AUC: 0.7256 V.Loss: 0.0815, V.Acc: 67.22, V.AUC: 0.6340;\n",
      "[E 333/2000] T.Loss: 0.0673, T.Acc: 76.27, T.AUC: 0.7183 V.Loss: 0.0815, V.Acc: 67.22, V.AUC: 0.6334;\n",
      "[E 334/2000] T.Loss: 0.0673, T.Acc: 75.58, T.AUC: 0.7209 V.Loss: 0.0816, V.Acc: 67.22, V.AUC: 0.6322;\n",
      "[E 335/2000] T.Loss: 0.0672, T.Acc: 75.17, T.AUC: 0.7337 V.Loss: 0.0816, V.Acc: 67.22, V.AUC: 0.6354;\n",
      "[E 336/2000] T.Loss: 0.0673, T.Acc: 74.62, T.AUC: 0.7127 V.Loss: 0.0817, V.Acc: 67.49, V.AUC: 0.6357;\n",
      "[E 337/2000] T.Loss: 0.0673, T.Acc: 77.37, T.AUC: 0.7150 V.Loss: 0.0816, V.Acc: 66.94, V.AUC: 0.6330;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E 338/2000] T.Loss: 0.0672, T.Acc: 75.17, T.AUC: 0.7275 V.Loss: 0.0817, V.Acc: 67.49, V.AUC: 0.6286;\n",
      "[E 339/2000] T.Loss: 0.0671, T.Acc: 75.86, T.AUC: 0.7313 V.Loss: 0.0818, V.Acc: 67.22, V.AUC: 0.6281;\n",
      "[E 340/2000] T.Loss: 0.0671, T.Acc: 75.17, T.AUC: 0.7183 V.Loss: 0.0817, V.Acc: 67.22, V.AUC: 0.6273;\n",
      "[E 341/2000] T.Loss: 0.0672, T.Acc: 74.49, T.AUC: 0.7082 V.Loss: 0.0817, V.Acc: 67.22, V.AUC: 0.6282;\n",
      "[E 342/2000] T.Loss: 0.0671, T.Acc: 75.72, T.AUC: 0.7215 V.Loss: 0.0816, V.Acc: 67.22, V.AUC: 0.6305;\n",
      "[E 343/2000] T.Loss: 0.0671, T.Acc: 75.72, T.AUC: 0.7171 V.Loss: 0.0816, V.Acc: 67.77, V.AUC: 0.6323;\n",
      "[E 344/2000] T.Loss: 0.0671, T.Acc: 75.72, T.AUC: 0.7182 V.Loss: 0.0818, V.Acc: 67.49, V.AUC: 0.6301;\n",
      "[E 345/2000] T.Loss: 0.0670, T.Acc: 75.31, T.AUC: 0.7169 V.Loss: 0.0818, V.Acc: 67.49, V.AUC: 0.6294;\n",
      "[E 346/2000] T.Loss: 0.0670, T.Acc: 75.31, T.AUC: 0.7175 V.Loss: 0.0819, V.Acc: 67.49, V.AUC: 0.6290;\n",
      "[E 347/2000] T.Loss: 0.0671, T.Acc: 75.03, T.AUC: 0.7211 V.Loss: 0.0819, V.Acc: 67.49, V.AUC: 0.6279;\n",
      "[E 348/2000] T.Loss: 0.0670, T.Acc: 75.72, T.AUC: 0.7162 V.Loss: 0.0819, V.Acc: 67.49, V.AUC: 0.6296;\n",
      "[E 349/2000] T.Loss: 0.0669, T.Acc: 75.45, T.AUC: 0.7216 V.Loss: 0.0819, V.Acc: 67.49, V.AUC: 0.6291;\n",
      "[E 350/2000] T.Loss: 0.0669, T.Acc: 74.76, T.AUC: 0.7115 V.Loss: 0.0818, V.Acc: 67.49, V.AUC: 0.6297;\n",
      "[E 351/2000] T.Loss: 0.0670, T.Acc: 74.62, T.AUC: 0.7126 V.Loss: 0.0819, V.Acc: 67.22, V.AUC: 0.6320;\n",
      "[E 352/2000] T.Loss: 0.0670, T.Acc: 74.90, T.AUC: 0.7163 V.Loss: 0.0818, V.Acc: 67.77, V.AUC: 0.6337;\n",
      "[E 353/2000] T.Loss: 0.0669, T.Acc: 75.31, T.AUC: 0.7351 V.Loss: 0.0818, V.Acc: 67.77, V.AUC: 0.6338;\n",
      "[E 354/2000] T.Loss: 0.0669, T.Acc: 74.35, T.AUC: 0.7104 V.Loss: 0.0820, V.Acc: 67.49, V.AUC: 0.6301;\n",
      "[E 355/2000] T.Loss: 0.0668, T.Acc: 74.90, T.AUC: 0.7242 V.Loss: 0.0821, V.Acc: 67.22, V.AUC: 0.6306;\n",
      "[E 356/2000] T.Loss: 0.0667, T.Acc: 75.86, T.AUC: 0.7152 V.Loss: 0.0821, V.Acc: 67.22, V.AUC: 0.6289;\n",
      "[E 357/2000] T.Loss: 0.0667, T.Acc: 76.27, T.AUC: 0.7260 V.Loss: 0.0821, V.Acc: 67.49, V.AUC: 0.6263;\n",
      "[E 358/2000] T.Loss: 0.0667, T.Acc: 75.58, T.AUC: 0.7242 V.Loss: 0.0822, V.Acc: 67.77, V.AUC: 0.6262;\n",
      "[E 359/2000] T.Loss: 0.0667, T.Acc: 75.17, T.AUC: 0.7074 V.Loss: 0.0823, V.Acc: 67.49, V.AUC: 0.6253;\n",
      "[E 360/2000] T.Loss: 0.0667, T.Acc: 75.17, T.AUC: 0.7334 V.Loss: 0.0821, V.Acc: 67.22, V.AUC: 0.6277;\n",
      "[E 361/2000] T.Loss: 0.0667, T.Acc: 74.90, T.AUC: 0.7161 V.Loss: 0.0821, V.Acc: 67.22, V.AUC: 0.6297;\n",
      "[E 362/2000] T.Loss: 0.0666, T.Acc: 75.31, T.AUC: 0.7262 V.Loss: 0.0821, V.Acc: 67.49, V.AUC: 0.6289;\n",
      "[E 363/2000] T.Loss: 0.0666, T.Acc: 74.21, T.AUC: 0.6982 V.Loss: 0.0821, V.Acc: 67.49, V.AUC: 0.6265;\n",
      "[E 364/2000] T.Loss: 0.0666, T.Acc: 75.58, T.AUC: 0.7243 V.Loss: 0.0820, V.Acc: 67.77, V.AUC: 0.6285;\n",
      "[E 365/2000] T.Loss: 0.0665, T.Acc: 75.17, T.AUC: 0.7253 V.Loss: 0.0820, V.Acc: 67.77, V.AUC: 0.6293;\n",
      "[E 366/2000] T.Loss: 0.0665, T.Acc: 75.03, T.AUC: 0.7145 V.Loss: 0.0821, V.Acc: 67.77, V.AUC: 0.6300;\n",
      "[E 367/2000] T.Loss: 0.0665, T.Acc: 75.58, T.AUC: 0.7228 V.Loss: 0.0821, V.Acc: 68.04, V.AUC: 0.6280;\n",
      "[E 368/2000] T.Loss: 0.0665, T.Acc: 74.90, T.AUC: 0.7066 V.Loss: 0.0823, V.Acc: 67.49, V.AUC: 0.6253;\n",
      "[E 369/2000] T.Loss: 0.0664, T.Acc: 75.72, T.AUC: 0.7303 V.Loss: 0.0824, V.Acc: 68.04, V.AUC: 0.6237;\n",
      "[E 370/2000] T.Loss: 0.0664, T.Acc: 75.31, T.AUC: 0.7264 V.Loss: 0.0824, V.Acc: 67.49, V.AUC: 0.6255;\n",
      "[E 371/2000] T.Loss: 0.0663, T.Acc: 76.68, T.AUC: 0.7327 V.Loss: 0.0824, V.Acc: 67.49, V.AUC: 0.6255;\n",
      "[E 372/2000] T.Loss: 0.0663, T.Acc: 76.54, T.AUC: 0.7307 V.Loss: 0.0823, V.Acc: 67.77, V.AUC: 0.6274;\n",
      "[E 373/2000] T.Loss: 0.0663, T.Acc: 74.49, T.AUC: 0.7279 V.Loss: 0.0824, V.Acc: 67.49, V.AUC: 0.6262;\n",
      "[E 374/2000] T.Loss: 0.0663, T.Acc: 74.76, T.AUC: 0.7284 V.Loss: 0.0822, V.Acc: 68.04, V.AUC: 0.6279;\n",
      "[E 375/2000] T.Loss: 0.0663, T.Acc: 75.86, T.AUC: 0.7240 V.Loss: 0.0824, V.Acc: 67.22, V.AUC: 0.6248;\n",
      "[E 376/2000] T.Loss: 0.0663, T.Acc: 75.31, T.AUC: 0.7368 V.Loss: 0.0822, V.Acc: 67.77, V.AUC: 0.6278;\n",
      "[E 377/2000] T.Loss: 0.0662, T.Acc: 76.41, T.AUC: 0.7365 V.Loss: 0.0823, V.Acc: 67.49, V.AUC: 0.6273;\n",
      "[E 378/2000] T.Loss: 0.0663, T.Acc: 75.31, T.AUC: 0.7180 V.Loss: 0.0823, V.Acc: 67.77, V.AUC: 0.6294;\n",
      "[E 379/2000] T.Loss: 0.0662, T.Acc: 76.13, T.AUC: 0.7213 V.Loss: 0.0822, V.Acc: 67.77, V.AUC: 0.6322;\n",
      "[E 380/2000] T.Loss: 0.0662, T.Acc: 75.17, T.AUC: 0.7174 V.Loss: 0.0823, V.Acc: 67.77, V.AUC: 0.6298;\n",
      "[E 381/2000] T.Loss: 0.0662, T.Acc: 75.72, T.AUC: 0.7224 V.Loss: 0.0822, V.Acc: 67.77, V.AUC: 0.6326;\n",
      "[E 382/2000] T.Loss: 0.0662, T.Acc: 77.91, T.AUC: 0.7209 V.Loss: 0.0824, V.Acc: 67.77, V.AUC: 0.6299;\n",
      "[E 383/2000] T.Loss: 0.0661, T.Acc: 75.72, T.AUC: 0.7300 V.Loss: 0.0826, V.Acc: 67.77, V.AUC: 0.6281;\n",
      "[E 384/2000] T.Loss: 0.0661, T.Acc: 76.41, T.AUC: 0.7406 V.Loss: 0.0823, V.Acc: 68.04, V.AUC: 0.6300;\n",
      "[E 385/2000] T.Loss: 0.0660, T.Acc: 76.27, T.AUC: 0.7246 V.Loss: 0.0824, V.Acc: 67.77, V.AUC: 0.6297;\n",
      "[E 386/2000] T.Loss: 0.0660, T.Acc: 75.17, T.AUC: 0.7306 V.Loss: 0.0825, V.Acc: 67.49, V.AUC: 0.6266;\n",
      "[E 387/2000] T.Loss: 0.0660, T.Acc: 76.82, T.AUC: 0.7298 V.Loss: 0.0826, V.Acc: 67.49, V.AUC: 0.6258;\n",
      "Stopping here!\n"
     ]
    }
   ],
   "source": [
    "mytrain_input  = metatrain_input\n",
    "mytrain_output = mytrain_output_0\n",
    "\n",
    "myvalid_input  = metavalid_input\n",
    "myvalid_output = myvalid_output_0\n",
    "\n",
    "input_size  = mytrain_input.shape[2]    # number of features\n",
    "output_size = mytrain_output.shape[2]*4 # depend on the model\n",
    "seq_len     = 4\n",
    "hidden_dim  = 8\n",
    "n_layers    = 1\n",
    "fc_size     = [16]\n",
    "\n",
    "dropoutrate = 0.15\n",
    "lr          = 0.001\n",
    "max_epochs  = 2000\n",
    "batch_size  = 50\n",
    "\n",
    "# Automatically determine the device that PyTorch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_meta = Model_meta(input_size=input_size, output_size=output_size, hidden_dim=hidden_dim, seq_len=seq_len, n_layers=n_layers, fc_size=fc_size, dropoutrate=dropoutrate)\n",
    "criterion = nn.MSELoss() # mean-squared error multiple values (not 0 or 1)\n",
    "optimizer = torch.optim.Adam(model_meta.parameters(), lr=lr) \n",
    "\n",
    "TrainHistory, stop_epoch = RNNtrain_meta(model_meta, device, criterion, optimizer, mytrain_input, mytrain_output, myvalid_input, myvalid_output, max_epochs, batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbadd48",
   "metadata": {},
   "source": [
    "## 2.7. krd distance KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8664d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myKNN(mytrain, mytrainlabel, mytest, myK):\n",
    "    myprob = []\n",
    "    neighbors = []\n",
    "    \n",
    "    for i in range(mytest.shape[0]):\n",
    "        x_sorted = [x for x, _ in sorted(zip(mytrain.iloc[i], mytrainlabel))]\n",
    "        y_sorted = [y for _, y in sorted(zip(mytrain.iloc[i], mytrainlabel))]\n",
    "        \n",
    "        tmpmyprob = np.exp(x_sorted[:myK])/sum(np.exp(x_sorted[:myK]))\n",
    "        tmppred = y_sorted[:myK]\n",
    "        myprob.append(sum(tmpmyprob*tmppred))\n",
    "        neighbors.append(tmppred)\n",
    "        \n",
    "    mypred = list(map(most_common, neighbors))\n",
    "    \n",
    "    return mypred, myprob\n",
    "\n",
    "# Returns the most common element in a list\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4cad002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "krd_data_train1 = krd_data_train.drop(['participant_id', 'collect_period'], axis=1)\n",
    "\n",
    "krd_err = []\n",
    "kclc = range(1,100)\n",
    "\n",
    "for k in kclc:\n",
    "    krd_pred, _ = myKNN(mytrain=krd_data_train1, mytrainlabel=meta_data_train['was_preterm'], mytest=krd_data_valid, myK=k)\n",
    "    krd_err.append(np.mean(krd_pred != meta_data_valid['was_preterm']))\n",
    "    \n",
    "# mytrain_K = np.argmin(krd_err)\n",
    "mytrain_K = np.argmin(krd_err) + 1\n",
    "print(mytrain_K)\n",
    "\n",
    "# Visualize accuracy vs. k\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(kclc, krd_err)\n",
    "ax.set(xlabel=\"k\",\n",
    "       ylabel=\"Error rate\",\n",
    "       title=\"Performance of knn\")\n",
    "plt.axvline(x=mytrain_K, color = 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d1acc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mytrain_K = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183ce21",
   "metadata": {},
   "source": [
    "## 2.8. Second-stage logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6996cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, myinput, myoutput, cutoff=0.5):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # predicted labels\n",
    "    myinput  = torch.from_numpy(myinput).float().to(device)\n",
    "    myoutput_nn, hidden = model(myinput)\n",
    "    myoutput_nn = myoutput_nn.reshape((myoutput.shape))\n",
    "    output_prob = nn.functional.softmax(myoutput_nn, dim=2)\n",
    "    mypredprob = output_prob[:,3,0].cpu().detach().numpy()\n",
    "    mypred = 1*(mypredprob > cutoff)\n",
    "\n",
    "    # observed labels\n",
    "    myobs  = myoutput[:,3,0]\n",
    "    \n",
    "    return myobs, mypred, mypredprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c88557fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "phylotype_obs, phylotype_pred, phylotype_prob = evaluate(model=model_phylotype, myinput=phylotypetrain_input, myoutput=mytrain_output_0, cutoff=0.5)\n",
    "taxonomy_obs, taxonomy_pred, taxonomy_prob = evaluate(model=model_taxonomy, myinput=taxonomytrain_input, myoutput=mytrain_output_0, cutoff=0.5)\n",
    "meta_obs, meta_pred, meta_prob = evaluate(model=model_meta, myinput=metatrain_input, myoutput=mytrain_output_0, cutoff=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df254d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, krd_prob = myKNN(mytrain=krd_data_train1, mytrainlabel=meta_data_train['was_preterm'], mytest=krd_data_train1, myK=mytrain_K)\n",
    "\n",
    "krd_prob = pd.DataFrame({\n",
    "    'participant_id': meta_data_train['participant_id'],\n",
    "    'collect_period': meta_data_train['collect_period'],\n",
    "    'krd_prob': krd_prob\n",
    "})\n",
    "\n",
    "krd_prob_wide = krd_prob.pivot_table(index=['participant_id'], columns='collect_period', values='krd_prob')\n",
    "# last period probability as final probability\n",
    "krd_prob = np. array(krd_prob_wide.fillna(method='ffill', axis=1).iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4f97055",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(np.transpose([phylotype_prob, taxonomy_prob, krd_prob, meta_prob])).reshape(-1, 4)\n",
    "\n",
    "Logistic_model = LogisticRegression(solver='liblinear').fit(x, phylotype_obs)\n",
    "# Logistic_model.predict_proba(x)\n",
    "# Logistic_model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce30fbf",
   "metadata": {},
   "source": [
    "# 3. Test model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c292b",
   "metadata": {},
   "source": [
    "Test set prior probability of class \"preterm\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37c003d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30727023319615915\n"
     ]
    }
   ],
   "source": [
    "testprior = sum(phylotype_obs)/len(phylotype_obs)\n",
    "print(testprior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc8dec",
   "metadata": {},
   "source": [
    "## 3.1 First stage model AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394fda8",
   "metadata": {},
   "source": [
    "#### phylotype data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f43d973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7156529793146033\n",
      "[[79  0]\n",
      " [41  0]]\n"
     ]
    }
   ],
   "source": [
    "phylotype_obs, phylotype_pred, phylotype_prob = evaluate(model=model_phylotype, myinput=phylotypetest_input, myoutput=mytest_output_0, cutoff=0.5)\n",
    "print(metrics.roc_auc_score(phylotype_obs, phylotype_prob))\n",
    "confusion = metrics.confusion_matrix(phylotype_obs, phylotype_pred)\n",
    "print(confusion)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1723f741",
   "metadata": {},
   "source": [
    "#### taxonomy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0461559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7675208397653597\n",
      "[[70  9]\n",
      " [22 19]]\n"
     ]
    }
   ],
   "source": [
    "taxonomy_obs, taxonomy_pred, taxonomy_prob = evaluate(model=model_taxonomy, myinput=taxonomytest_input, myoutput=mytest_output_0, cutoff=0.5)\n",
    "print(metrics.roc_auc_score(taxonomy_obs, taxonomy_prob))\n",
    "confusion = metrics.confusion_matrix(taxonomy_obs, taxonomy_pred)\n",
    "print(confusion)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae0fe3",
   "metadata": {},
   "source": [
    "#### meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44fc780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6900277863538129\n",
      "[[72  7]\n",
      " [27 14]]\n"
     ]
    }
   ],
   "source": [
    "meta_obs, meta_pred, meta_prob = evaluate(model=model_meta, myinput=metatest_input, myoutput=mytest_output_0, cutoff=0.5)\n",
    "print(metrics.roc_auc_score(meta_obs, meta_prob))\n",
    "confusion = metrics.confusion_matrix(meta_obs, meta_pred)\n",
    "print(confusion)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda9aec6",
   "metadata": {},
   "source": [
    "#### krd distance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4983d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "krd_pred, krd_prob = myKNN(mytrain=krd_data_train1, mytrainlabel=meta_data_train['was_preterm'], mytest=krd_data_test, myK=mytrain_K)\n",
    "krd_pred_data = pd.DataFrame({\n",
    "    'participant_id': meta_data_test['participant_id'],\n",
    "    'collect_period': meta_data_test['collect_period'],\n",
    "    'krd_obs': meta_data_test['was_preterm'],\n",
    "    'krd_prob': krd_prob,\n",
    "    'krd_pred': krd_pred\n",
    "})\n",
    "\n",
    "krd_obs = krd_pred_data.pivot_table(index=['participant_id'], columns='collect_period', values='krd_obs')\n",
    "# last period probability as final probability\n",
    "krd_obs = np. array(krd_obs.fillna(method='ffill', axis=1).iloc[:, -1])\n",
    "\n",
    "krd_prob = krd_pred_data.pivot_table(index=['participant_id'], columns='collect_period', values='krd_prob')\n",
    "# last period probability as final probability\n",
    "krd_prob = np. array(krd_prob.fillna(method='ffill', axis=1).iloc[:, -1])\n",
    "\n",
    "krd_pred = krd_pred_data.pivot_table(index=['participant_id'], columns='collect_period', values='krd_pred')\n",
    "# last period probability as final probability\n",
    "krd_pred = np. array(krd_pred.fillna(method='ffill', axis=1).iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "514ee531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4016671812287743\n",
      "[[77  2]\n",
      " [40  1]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.roc_auc_score(krd_obs, krd_prob))\n",
    "confusion = metrics.confusion_matrix(krd_obs, krd_pred)\n",
    "print(confusion)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302cca2",
   "metadata": {},
   "source": [
    "## 3.2. Second stage model AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "651f4082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7585674590923125\n",
      "[[68 11]\n",
      " [23 18]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(np.transpose([phylotype_prob, taxonomy_prob, krd_prob, meta_prob])).reshape(-1, 4)\n",
    "final_obs = meta_obs\n",
    "final_prob = Logistic_model.predict_proba(x)[:,1]\n",
    "final_pred = Logistic_model.predict(x)\n",
    "\n",
    "print(metrics.roc_auc_score(final_obs, final_prob))\n",
    "\n",
    "confusion = metrics.confusion_matrix(final_obs, final_pred)\n",
    "print(confusion)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ed586",
   "metadata": {},
   "source": [
    "# 4. Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83c145",
   "metadata": {},
   "source": [
    "- Re-train the obtained proability from the first stage model did improve the performance in terms of AUC value;\n",
    "- Every LSTM same structure or preform parameter tunning;\n",
    "- Need to split training/validation/testing set project-wise;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb51ec",
   "metadata": {},
   "source": [
    "# 5. Study materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ed6d0",
   "metadata": {},
   "source": [
    "### How to use Learning Curves to Diagnose Machine Learning Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45b3b7",
   "metadata": {},
   "source": [
    "Study materials:\n",
    "https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34eb72",
   "metadata": {},
   "source": [
    "A plot of learning curves shows **Underfitting** if:\n",
    "- The training loss remains flat regardless of training;\n",
    "- The training loss continues to decrease untill the end of training;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e03c67",
   "metadata": {},
   "source": [
    "A plot of learning curves shows **Overfitting** if:\n",
    "- The training loss continues to decrease with experience;\n",
    "- The validation loss decreases to a point and begins increasing again;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38a3bd",
   "metadata": {},
   "source": [
    "A plot of learning curves shows **Goodfitting** if:\n",
    "- The plot of training loss decreases to a point of stability;\n",
    "- The plot of validation loss decreases to a point of stability and has a small gap with the training loss;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88be39",
   "metadata": {},
   "source": [
    "A plot of learning curves shows **Unrepresentative training set** if:\n",
    "- The plot of training loss shows improvement;\n",
    "- The plot of validation loss shows improvement, but a large gap between two curves;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7df6b5",
   "metadata": {},
   "source": [
    "A plot of learning curves shows **Unrepresentative validation set** if:\n",
    "- The plot of training loss shows goodfit;\n",
    "- The plot of validation loss shows noisy movement around the training loss;\n",
    "- The validation loss is lower than the training loss;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870f42e",
   "metadata": {},
   "source": [
    "### Why the training loss is above the validation loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5edf71b",
   "metadata": {},
   "source": [
    "- Regularization is applied during training, but not during validation. If add in the regularization loss during validation, your loss values and curves will look more similar;\n",
    "- Training loss is measured during each epoch while validation loss is measured after each epoch. On average, the training loss is measured 1/2 an epoch earlier. If you shift your training loss curve a half epoch to the left, your losses will align a bit better;\n",
    "- Your validation set may be easier than your training set or there is a leak in your data/bug in your code. Make sure your validation set is reasonably large and is sampled from the same distribution (and difficulty) as your training set;\n",
    "- You may be over-regularizing your model. Try reducing your regularization constraints, including increasing your model capacity (i.e., making it deeper with more parameters), reducing dropout, reducing L2 weight decay strength, etc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d52b6f",
   "metadata": {},
   "source": [
    "**Reason**: \n",
    "\n",
    "The training loss is calculated within each batch and adding together, which is different from the calculation method of validation set. Align the calculation method for each epoch will make the loss function values comparable and reasonable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
